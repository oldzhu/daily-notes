now we are in build LLM from scratch project, now I would like to build myself build llm from scratch using c/c++ which doing full stack gpt/transformer like training/inference in desktop/lapton/edge devie cpu or gpu(support rocm,cuda,musa and other popular public api using gpu,even some part can write gpu instructions directly, dont use any high level libraries  ), the goal of the project is to practice  and learm llm, master all areas of the LLM from bottom(hardware) to top(use llm to chat, coding, debugging). How to start and pls share what ur think and the plan to go. By the way, I have two boxes can be used to as initial hardware to start, 1 is AMD_RYZEN_AI_MAX+_395_w/_Radeon_8060S and another is laptop with amd cpu with 64g ram + nvida 3070 16g. In addition, is there any other similar oss project already?
=========
You can absolutely do this, but the scope is huge if you mean “training + inference + CPU SIMD + multi‑GPU backends (CUDA/ROCm/MUSA) + kernels from scratch”. The best way to succeed is to build a tiny, correct reference first, then optimize and add backends one at a time.

Also: I can’t enable “Claude Haiku 4.5 for all clients” from here (I don’t control your org/client feature flags). I’m GPT-5.2 in this VS Code session. If you tell me what product you’re using (VS Code extension? internal gateway? OpenAI-compatible proxy?), I can point you to the exact place that setting usually lives.

How To Start (practical path that actually finishes)

Phase 0 — Decide your “from scratch” boundary (1 day)

You said “no high-level libraries.” Realistically you’ll still need some low-level platform APIs:
CPU: C/C++ + OS threads + file IO.
NVIDIA: CUDA runtime + NVRTC or offline nvcc builds (this is not “high level”; it’s the vendor API).
AMD: HIP/ROCm runtime (again vendor API).
MUSA: MUSA runtime (vendor API).
Explicitly avoid BLAS/cuBLAS/rocBLAS, cuDNN/MIOpen, PyTorch, etc. (fits your rule).
Pick a first target model: GPT-2 small-ish is already heavy; I’d start with something like:
10–30M params (fast iteration, still “real Transformer”)
context 256–1024, vocab 32k–50k
Phase 1 — Minimal CPU inference (correctness first)
Implement only what’s needed to load weights and generate tokens:

Tokenizer: BPE (GPT-2 style) or a simpler unigram/BPE variant you implement.
Tensor core: a tiny Tensor struct (shape, strides, dtype, pointer) + allocator.
Ops (FP32 first):
embedding gather
matmul (naive)
layernorm
attention (QKV, causal mask, softmax)
MLP (GELU/SwiGLU depending on model)
logits + sampling (greedy/top-k)
Verification strategy:
Use your existing Python notebooks in this repo as the “oracle”: dump intermediate activations from Python and compare with your C implementation on the same prompt/weights.
Phase 2 — CPU training (still no GPU yet)
Training forces you to master the full stack (autograd/backprop, optimizer, numerics, checkpointing).

Add backward pass. Two workable approaches:
Manual backward per op (fastest runtime, most work)
A tiny autograd tape (cleaner, slower, still educational)
Add optimizer: AdamW (FP32), then add gradient clipping.
Add checkpoint format: your own simple binary + JSON header is fine.
Add dataloader: memmapped dataset + shuffling + batching.
Train on something tiny first (e.g., a few MB text) so you can verify loss decreases.
Phase 3 — Make CPU fast enough (SIMD + threading)
Before GPU, squeeze CPU so you learn hardware realities:

Threading: parallelize matmul and attention over batch/heads/rows.
SIMD: implement packed kernels for matmul and layernorm:
AVX2/AVX-512 on x86, or whatever your CPU supports
Add FP16/BF16 storage (compute maybe FP32 at first), then mixed precision.
Phase 4 — First GPU backend: CUDA on the 3070 box
This is your “get a real GPU working” milestone. NVIDIA first is the smoothest path.

Write CUDA kernels for:
matmul (start naive, then tiled shared-memory)
softmax (block reductions)
layernorm
fused attention eventually (but don’t start there)
Build a backend abstraction:
DeviceBuffer, launch_kernel(...), copy_host_to_device, etc.
Keep the model code backend-agnostic.
Phase 5 — Second GPU backend: ROCm/HIP
Do this after CUDA works, because you’ll reuse architecture and learn what differs.

Prefer HIP-style kernels that can compile for both CUDA+ROCm if you design for it, but in practice you’ll still hit differences.
Note: on Windows, ROCm support is historically limited; you’ll likely want Linux on the AMD box for serious ROCm work.
Phase 6 — Add MUSA / “other public GPU APIs”
Do it only once your kernel interface is stable.

Treat each vendor as a plugin backend.
Expect kernel-language/tooling differences and missing features.
Hardware Choice (your two boxes)
Laptop (AMD CPU + 64GB + NVIDIA 3070 16GB): start here for GPU.
CUDA documentation/tooling is mature; easiest path to “I wrote kernels and it trains”.
AMD_RYZEN_AI_MAX+_395 w/ Radeon 8060S: great for CPU work + ROCm later (likely Linux).
Use it for CPU SIMD experiments and then ROCm once your CUDA backend proves the design.
What I Think (so you don’t get stuck)
To “master from hardware to chat UI”, you need tight milestones and constant correctness checks:

First goal is not a big model—it’s a tiny model that matches a reference exactly.
Only then optimize and add GPU backends.
Multi-backend too early is the #1 way these projects die; do CPU → CUDA → ROCm → others.
Similar OSS projects (worth studying, even if you re-implement)
These are useful as references for architecture, numerics, kernels, and checkpoint formats:

Karpathy llm.c (C, minimal GPT training/inference; great “from-scratch” vibe)
llama.cpp / ggml (C/C++, low-level tensor engine + many backends; great for inference design)
gpt-fast (often PyTorch-based but has very good kernel/fusion ideas; concept reference)
tinygrad (Python, but excellent for backend abstraction thinking)
If you want, I can curate a “study list” mapping each project to what to steal conceptually (kernel patterns, memory planning, quantization, etc.) while still writing your own code.
A concrete first milestone I recommend
“CPU-only GPT inference that can load a small checkpoint and generate text”

Model: 10–30M params
Dtype: FP32
Single-thread first
Verified against Python outputs on 1–2 prompts
If you want, I can scaffold a new cpp/ directory in your workspace with:

CMake project
minimal tensor + allocator
tokenizer stub
a CPU-only forward pass skeleton
a test harness that compares against Python-exported tensors
Tell me: do you want to start with (A) inference-first or (B) training-first, and are you OK using CUDA runtime / HIP runtime as the “lowest-level allowed” GPU interface?


