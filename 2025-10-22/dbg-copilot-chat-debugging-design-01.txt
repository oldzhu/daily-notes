I would like to change the copilot chat experience as the below through prompts, prompt templates or code:
1. for single command or multiple commands llm model decide to run, lldm will list command or commands to ask the user for confirmation. only reply <cmd>...<cmd> to ask for real execution after get the user confirmation.
2. will send single command or muiltple commands ouptut after executed back to llm model, let llm modle decide how to move forward -  need to run another command/commands to finish the task or ask if need other help if the task done
3. pretend gdb> before the outut of the executed command so it looks like we run the command in gdb shell directly.
Build after change done and I will test to how new experience is.
==============
I’ve implemented the confirmation-first chat flow, added gdb> echoes, and built/reinstalled locally (no commit) so you can test right away.

What changed

Confirmation-first orchestration
Propose-only first: When the model suggests commands, it now lists them and asks for confirmation. It does not execute.
On “yes/ok”: It executes, prefixes the echo with gdb> cmd, captures output, then sends that fresh output back to the model for a follow-up decision (more commands or done).
On “no”: It asks the model for a safer alternative and collects a new proposal for confirmation.
Pending command memory: Proposed commands are stored in state.pending_cmd until you respond.
gdb> echoes
All command echoes in both REPL /exec and orchestrator execution are now prefixed with gdb>.
Colorized in cyan/bold when colors are enabled; raw GDB output still uncolored.
Prompts updated
Protocol is now human-in-the-loop: propose (no <cmd>), confirm, then a single <cmd> line on execution turns.
JSON in prompts.json and fallback in defaults.py are aligned.
Files touched

orchestrator.py
ask() now manages proposal/confirmation flow and post-exec follow-up.
Added helpers: _extract_commands_list, _format_confirmation_message, _execute_and_follow_up, etc.
Echoes use “gdb> ” prefix.
repl.py
/exec echoes with “gdb> ”.
prompts.json and defaults.py
Switched to confirmation-first protocol text, examples, and rules.
Build and tests

Built and reinstalled the wheel locally for testing.
Tests: PASS (pytest -q).
Build: PASS.
Lint/typecheck: not configured in repo; skipped.
How to test interactively (no commit yet)

In GDB:
python import dbgcopilot.plugins.gdb.copilot_cmd
copilot
Optional: /prompts reload (ensures the new prompts are active from configs/prompts.json)
Try a natural request like:
“Load and run examples/crash_demo/crash, then show backtrace.”
Expected: Assistant proposes:
gdb> file examples/crash_demo/crash
gdb> run
confirm? (It might add bt as a follow-up after run output.)
Reply yes or ok to execute. You’ll see:
gdb> file …
Reading symbols… / Starting program…
Then it sends the output to the LLM for the next step (e.g., propose gdb> bt, confirm again).
You can decline a proposal (no), and it will ask the model for a safer alternative.
Use /colors on|off to toggle colorized echoes.
Notes and assumptions

The LLM should follow the new protocol now that prompts are updated; if you want to iterate faster, tweak prompts.json and run /prompts reload.
You can also override with DBGCOPILOT_PROMPTS=<path/to/your.json> in the environment.
If this matches the experience you want, I’ll leave it uncommitted while you test. When you’re happy, I can commit the changes in one descriptive changeset.
