Excellent. This is a comprehensive and very practical topic. Based on the search results, I have synthesized the landscape of LLM optimization across CPU and GPU platforms. Below is a categorized framework covering **Training** and **Inference** for both hardware types, mapping each technique to the specific part of the algorithm or computation it targets.

è¿™æ˜¯ä¸€ä¸ªéå¸¸å…¨é¢ä¸”å®ç”¨çš„ä¸»é¢˜ã€‚æ ¹æ®æœç´¢ç»“æœï¼Œæˆ‘æ•´åˆäº†CPUå’ŒGPUå¹³å°ä¸ŠLLMä¼˜åŒ–çš„å…¨æ™¯å›¾ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªåˆ†ç±»æ¡†æ¶ï¼Œæ¶µç›–ä¸¤ç§ç¡¬ä»¶ç±»å‹çš„**è®­ç»ƒ**å’Œ**æ¨ç†**ï¼Œå°†æ¯ç§æŠ€æœ¯æ˜ å°„åˆ°å…¶é’ˆå¯¹çš„ç‰¹å®šç®—æ³•æˆ–è®¡ç®—ç¯èŠ‚ã€‚

---

# ğŸ“‹ Optimization Categories / ä¼˜åŒ–åˆ†ç±»ç›®å½•
**English & Chinese / è‹±æ–‡ä¸ä¸­æ–‡**

| Hardware / ç¡¬ä»¶ | Stage / é˜¶æ®µ | Optimization Technique / ä¼˜åŒ–æŠ€æœ¯ | Target Part of Algorithm / ç®—æ³•ç›®æ ‡ç¯èŠ‚ | Simple Summary / ç®€è¿° |
| :--- | :--- | :--- | :--- | :--- |
| **GPU** | **Training** | **Activation CPU Offloading**  | Backward Pass (Memory Footprint) / åå‘ä¼ æ’­ï¼ˆå†…å­˜å ç”¨ï¼‰ | Moves intermediate activations from GPU RAM to CPU RAM to enable larger batch sizes; trades speed for memory. / å°†ä¸­é—´æ¿€æ´»å¼ é‡ä»GPUæ˜¾å­˜ç§»è‡³CPUå†…å­˜ï¼Œä»¥æ”¯æŒæ›´å¤§æ‰¹é‡ï¼›ä»¥é€Ÿåº¦ä¸ºä»£ä»·æ¢å–å†…å­˜ç©ºé—´ã€‚ |
| **GPU** | **Training** | **Unified Memory (UM)**  | Data Transfer Management / æ•°æ®ä¼ è¾“ç®¡ç† | Simplifies programming by automatically migrating data between CPU and GPU; performance depends on access patterns (good for LoRA, bad for full tuning). / é€šè¿‡CPUä¸GPUé—´è‡ªåŠ¨æ•°æ®è¿ç§»ç®€åŒ–ç¼–ç¨‹ï¼›æ€§èƒ½å–å†³äºè®¿é—®æ¨¡å¼ï¼ˆå¯¹LoRAå‹å¥½ï¼Œå¯¹å…¨é‡å¾®è°ƒä¸å‹å¥½ï¼‰ã€‚ |
| **GPU** | **Training** | **Automatic Mixed Precision (AMP)**  | Matrix Multiplications (GEMM) / çŸ©é˜µä¹˜æ³• | Uses FP16/BF16 for compute, FP32 for master weights; leverages Tensor Cores for 2-4x speedup. / ä½¿ç”¨FP16/BF16è¿›è¡Œè®¡ç®—ï¼ŒFP32ä¿å­˜ä¸»æƒé‡ï¼›åˆ©ç”¨Tensor Coreå®ç°2-4å€åŠ é€Ÿã€‚ |
| **GPU** | **Training** | **FP8 Training**  | Matrix Multiplications (GEMM) / çŸ©é˜µä¹˜æ³• | Extreme low-precision (8-bit) training on Hopper/Blackwell; requires Transformer Engine, significantly reduces memory and increases TFLOPs. / åœ¨Hopper/Blackwellæ¶æ„ä¸Šçš„æç«¯ä½ç²¾åº¦ï¼ˆ8ä½ï¼‰è®­ç»ƒï¼›éœ€Transformer Engineï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å¹¶æå‡ç®—åŠ›ã€‚ |
| **GPU** | **Training** | **QLoRA / 4-bit Quantization**  | Weight Storage (Fine-tuning) / æƒé‡å­˜å‚¨ï¼ˆå¾®è°ƒï¼‰ | Loads model in 4-bit (NF4) and adds adapters; enables 40B+ fine-tuning on single consumer GPU (70% VRAM reduction). / ä»¥4ä½ç²¾åº¦åŠ è½½æ¨¡å‹å¹¶æ·»åŠ é€‚é…å™¨ï¼›å®ç°å•å¼ æ¶ˆè´¹çº§GPUå¾®è°ƒ400äº¿å‚æ•°æ¨¡å‹ï¼ˆæ˜¾å­˜å‡å°‘70%ï¼‰ã€‚ |
| **GPU** | **Training** | **Custom Triton Kernels**  | Attention & Projection Layers / æ³¨æ„åŠ›å±‚ä¸æŠ•å½±å±‚ | Hand-written kernels (Unsloth) to reduce backward pass computation and memory writes. / æ‰‹å†™å†…æ ¸å‡å°‘åå‘ä¼ æ’­è®¡ç®—é‡ä¸å†…å­˜å†™å…¥ï¼ˆå¦‚Unslothï¼‰ã€‚ |
| **GPU** | **Inference** | **FlashInfer/Fused Kernels**  | Attention (KV Cache) & MoE / æ³¨æ„åŠ›æœºåˆ¶ä¸æ··åˆä¸“å®¶ | Fuses multi-step operations (e.g., RoPE+Q+Cache) to reduce memory round-trips and launch overhead. / èåˆå¤šæ­¥æ“ä½œä»¥å‡å°‘å†…å­˜å¾€è¿”ä¸å†…æ ¸å¯åŠ¨å¼€é”€ã€‚ |
| **GPU** | **Inference** | **FP8 KV Cache**  | Key-Value Cache Storage / é”®å€¼ç¼“å­˜å­˜å‚¨ | Stores KV cache in 8-bit precision; increases concurrent request capacity without heavy accuracy loss. / ä»¥8ä½ç²¾åº¦å­˜å‚¨KVç¼“å­˜ï¼›åœ¨ä¸ä¸¥é‡æŸå¤±ç²¾åº¦å‰æä¸‹æå‡å¹¶å‘è¯·æ±‚å®¹é‡ã€‚ |
| **GPU** | **Inference** | **torch.compile Graph Fusion**  | Compute Graph / è®¡ç®—å›¾ | Automatically fuses operations (e.g., AllReduce + RMSNorm); reduces kernel launch frequency. / è‡ªåŠ¨èåˆæ“ä½œï¼ˆå¦‚å…¨è§„çº¦+RMSNormï¼‰ï¼›å‡å°‘å†…æ ¸å¯åŠ¨é¢‘ç‡ã€‚ |
| **GPU** | **Inference** | **Async Scheduling**  | Host-side Batching / ä¸»æœºç«¯æ‰¹å¤„ç† | Decouples CPU request scheduling from GPU execution; hides host overhead (critical for fast GPUs like Blackwell). / å°†CPUè¯·æ±‚è°ƒåº¦ä¸GPUæ‰§è¡Œè§£è€¦ï¼›éšè—ä¸»æœºå¼€é”€ï¼ˆå¯¹Blackwellè¿™ç±»é«˜é€ŸGPUè‡³å…³é‡è¦ï¼‰ã€‚ |
| **GPU** | **Inference** | **Stream Interval**  | Network I/O / ç½‘ç»œè¾“å…¥è¾“å‡º | Buffers tokens before sending; reduces CPU serialization/HTTP overhead (up to 57% gain). / ç¼“å†²tokenåå†å‘é€ï¼›é™ä½CPUåºåˆ—åŒ–/HTTPå¼€é”€ï¼ˆæœ€é«˜æå‡57%æ•ˆç‡ï¼‰ã€‚ |
| **GPU** | **Inference** | **MoE Kernel Selection**  | Mixture-of-Experts Routing / æ··åˆä¸“å®¶è·¯ç”± | Specifically offloads "expert" layers to CPU while keeping attention layers on GPU; maximizes VRAM utility. / ç‰¹æŒ‡å°†â€œä¸“å®¶â€å±‚å¸è½½è‡³CPUï¼ŒåŒæ—¶ä¿ç•™æ³¨æ„åŠ›å±‚åœ¨GPUï¼›æœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨ç‡ã€‚ |
| **GPU** | **Inference** | **Tensor Parallelism (TP)**  | Layer Partitioning / å±‚åˆ‡åˆ† | Splits weights across GPUs; communication via AllReduce. / å°†æƒé‡åˆ‡åˆ†è‡³å¤šGPUï¼›é€šè¿‡å…¨è§„çº¦é€šä¿¡ã€‚ |
| **GPU** | **Inference** | **Disaggregation**  | Prefill/Decode Separation / é¢„å¡«å……ä¸è§£ç åˆ†ç¦» | Runs prefill and decode on separate GPUs; optimizes compute/memory ratio per stage. / å°†é¢„å¡«å……ä¸è§£ç é˜¶æ®µåˆ†ç¦»è‡³ä¸åŒGPUï¼›ä¼˜åŒ–å„é˜¶æ®µè®¡ç®—/å†…å­˜é…æ¯”ã€‚ |
| **CPU** | **Training** | **LoRA (Parameter-Efficient)**  | Weight Updates / æƒé‡æ›´æ–° | Freezes base weights; trains only small adapters; avoids swap thrashing on RAM-constrained systems. / å†»ç»“åŸºåº§æƒé‡ï¼›ä»…è®­ç»ƒå°å‹é€‚é…å™¨ï¼›é¿å…å†…å­˜å—é™ç³»ç»Ÿä¸‹çš„é¢‘ç¹äº¤æ¢ã€‚ |
| **CPU** | **Training** | **Full Model CPU Offloading**  | Entire Model State / å®Œæ•´æ¨¡å‹çŠ¶æ€ | When GPU VRAM is insufficient, entire model resides on CPU; execution happens via CPU or page-migration. / å½“GPUæ˜¾å­˜ä¸è¶³æ—¶ï¼Œå®Œæ•´æ¨¡å‹é©»ç•™CPUï¼›é€šè¿‡CPUæ‰§è¡Œæˆ–é¡µè¿ç§»è¿è¡Œã€‚ |
| **CPU** | **Inference** | **SmoothQuant (8-bit)**  | Activation Outliers / æ¿€æ´»å¼‚å¸¸å€¼ | Smooths outliers from activations to weights; enables INT8 without accuracy loss on Xeon. / å°†æ¿€æ´»ä¸­çš„å¼‚å¸¸å€¼è¿ç§»è‡³æƒé‡ï¼›ä½¿INT8é‡åŒ–åœ¨è‡³å¼ºCPUä¸Šæ— ç²¾åº¦æŸå¤±ã€‚ |
| **CPU** | **Inference** | **Ultra-low-bit (1/1.58/2-bit)**  | GEMM Microkernels / é€šç”¨çŸ©é˜µä¹˜å¾®å†…æ ¸ | Custom AVX2/AMX microkernels for binary/ternary weights; up to 7x speedup vs FP16. / é’ˆå¯¹äºŒå€¼/ä¸‰å€¼æƒé‡çš„è‡ªå®šä¹‰AVX2/AMXå¾®å†…æ ¸ï¼›ç›¸æ¯”FP16æœ€é«˜7å€åŠ é€Ÿã€‚ |
| **CPU** | **Inference** | **Selective MoE Offloading**  | Expert Layers (MoE) / ä¸“å®¶å±‚ | Only offloads MoE expert weights to RAM; critical attention layers stay on GPU (or iGPU). / ä»…å°†MoEä¸“å®¶æƒé‡å¸è½½è‡³å†…å­˜ï¼›å…³é”®æ³¨æ„åŠ›å±‚ä¿ç•™åœ¨GPUã€‚ |

---

# ğŸ§  Next Steps: Deep Dive Roadmap / ä¸‹ä¸€æ­¥ï¼šæ·±åº¦æ¢è®¨è·¯çº¿å›¾

We now have the map. Based on the strength of the search results, I suggest we proceed in the following logical order. This sequence moves from **memory-centric** solutions to **compute-centric** and finally **system-architecture** solutions.

æˆ‘ä»¬ç°åœ¨æœ‰äº†å®Œæ•´çš„åœ°å›¾ã€‚æ ¹æ®æœç´¢ç»“æœçš„è¯¦å®ç¨‹åº¦ï¼Œæˆ‘å»ºè®®æŒ‰ç…§ä»¥ä¸‹é€»è¾‘é¡ºåºé€ä¸€æ·±å…¥ã€‚è¿™ä¸ªé¡ºåºæ˜¯ä»**å†…å­˜ä¸ºä¸­å¿ƒ**çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ°**è®¡ç®—ä¸ºä¸­å¿ƒ**ï¼Œæœ€åæ˜¯**ç³»ç»Ÿæ¶æ„**çº§æ–¹æ¡ˆã€‚

## ğŸ”¹ Track 1: Memory Wall & Capacity (The VRAM Crisis) / å†…å­˜å¢™ä¸å®¹é‡ï¼ˆæ˜¾å­˜å±æœºï¼‰
*Focus: How to fit 100B models into 24GB GPUs or DRAM.*
*é‡ç‚¹ï¼šå¦‚ä½•å°†åƒäº¿å‚æ•°æ¨¡å‹å¡è¿›24GBæ˜¾å­˜æˆ–å†…å­˜ä¸­ã€‚*

1.  **Quantization Spectrum** (GPU/CPU) / é‡åŒ–å…¨è°±
    - *From SmoothQuant 8-bit on Xeons to MXFP4/NVFP4 on Blackwell, down to 1-bit BitNet on AI PCs.*
    - *ä»è‡³å¼ºçš„SmoothQuant 8-bitï¼Œåˆ°Blackwellçš„MXFP4/NVFP4ï¼Œå†åˆ°AI PCçš„1ä½BitNetã€‚*
    - **Key Sources**: 
2.  **CPU Offloading Strategies** / CPUå¸è½½ç­–ç•¥
    - *Generic layer-splitting vs. Selective MoE offloading. Why MoE is special?*
    - *é€šç”¨å±‚åˆ‡åˆ† vs é€‰æ‹©æ€§MoEå¸è½½ã€‚ä¸ºä»€ä¹ˆMoEç‰¹æ®Šï¼Ÿ*
    - **Key Sources**: 
3.  **Parameter-Efficient Fine-Tuning (PEFT)** / å‚æ•°é«˜æ•ˆå¾®è°ƒ
    - *Why LoRA succeeds on CPU-only training while Full Tuning fails (swap thrashing).*
    - *ä¸ºä»€ä¹ˆLoRAåœ¨çº¯CPUè®­ç»ƒä¸­æˆåŠŸï¼Œè€Œå…¨é‡å¾®è°ƒå¤±è´¥ï¼ˆäº¤æ¢æ­»é”ï¼‰ã€‚*
    - **Key Sources**: 

## ğŸ”¹ Track 2: The Math Engine (Kernels & Precision) / æ•°å­¦å¼•æ“ï¼ˆå†…æ ¸ä¸ç²¾åº¦ï¼‰
*Focus: Making GEMM and Attention faster.*
*é‡ç‚¹ï¼šè®©çŸ©é˜µä¹˜æ³•ä¸æ³¨æ„åŠ›è®¡ç®—æ›´å¿«ã€‚*

1.  **Low Precision Training** / ä½ç²¾åº¦è®­ç»ƒ
    - *AMP (FP16) -> FP8 -> FP4. The role of Tensor Cores and Transformer Engine.*
    - *ä»æ··åˆç²¾åº¦åˆ°FP8åˆ°FP4ã€‚Tensor Coreä¸Transformer Engineçš„è§’è‰²ã€‚*
    - **Key Sources**: 
2.  **Attention & MoE Kernel Wars** / æ³¨æ„åŠ›ä¸MoEå†…æ ¸ä¹‹äº‰
    - *FlashInfer, Cutlass, trtllm-gen. Why "Fusion" reduces memory bound.*
    - *FlashInfer, Cutlass, trtllm-genã€‚ä¸ºä»€ä¹ˆâ€œèåˆâ€èƒ½å‡å°‘å†…å­˜ç“¶é¢ˆã€‚*
    - **Key Sources**: 

## ğŸ”¹ Track 3: System & Overhead (Hiding the CPU) / ç³»ç»Ÿä¸å¼€é”€ï¼ˆéšè—CPUï¼‰
*Focus: When the GPU is too fast, the CPU becomes the bottleneck.*
*é‡ç‚¹ï¼šå½“GPUå¤ªå¿«æ—¶ï¼ŒCPUæˆä¸ºç“¶é¢ˆã€‚*

1.  **Host Overhead Hiding** / ä¸»æœºå¼€é”€éšè—
    - *Async Scheduling, Stream Interval, Continuous Batching evolution.*
    - *å¼‚æ­¥è°ƒåº¦ã€æµé—´éš”ã€è¿ç»­æ‰¹å¤„ç†æ¼”è¿›ã€‚*
    - **Key Sources**: 
2.  **Graph Capture & Compilation** / å›¾æ•è·ä¸ç¼–è¯‘
    - *torch.compile fusions vs. manual kernel writing.*
    - *torch.compileèåˆ vs æ‰‹åŠ¨å†™å†…æ ¸ã€‚*
    - **Key Sources**: 

## ğŸ”¹ Track 4: Architecture Shift / æ¶æ„å˜é©
*Focus: Breaking the single-GPU, single-stage paradigm.*
*é‡ç‚¹ï¼šæ‰“ç ´å•GPUã€å•é˜¶æ®µèŒƒå¼ã€‚*

1.  **Disaggregated Inference** / åˆ†ç¦»å¼æ¨ç†
    - *Separating Prefill (compute-bound) from Decode (memory-bound).*
    - *åˆ†ç¦»é¢„å¡«å……ï¼ˆè®¡ç®—å¯†é›†å‹ï¼‰ä¸è§£ç ï¼ˆå†…å­˜å¯†é›†å‹ï¼‰ã€‚*
    - **Key Sources**: 
2.  **Data+Expert Parallelism (DEP)** / æ•°æ®+ä¸“å®¶å¹¶è¡Œ
    - *The next frontier for MoE scaling.*
    - *MoEæ‰©å±•çš„ä¸‹ä¸€ä¸ªå‰æ²¿ã€‚*
    - **Key Sources**: 

---

**Please choose which Track or specific technique you would like to start with.** The search results provide particularly rich, actionable data on **CPU Ultra-low-bit inference** (Track 1) and **vLLM Blackwell Optimizations** (Track 3/4), but I am ready to dive into any of them with architectural diagrams and kernel-level explanations.

**è¯·é€‰æ‹©æ‚¨æƒ³ä»å“ªä¸ªä¸“é¢˜æˆ–å…·ä½“æŠ€æœ¯å¼€å§‹ã€‚** æœç´¢ç»“æœä¸­å…³äº**CPUè¶…ä½ä½æ¨ç†**ï¼ˆä¸“é¢˜ä¸€ï¼‰å’Œ**vLLM Blackwellä¼˜åŒ–**ï¼ˆä¸“é¢˜ä¸‰/å››ï¼‰æä¾›äº†éå¸¸ä¸°å¯Œä¸”å¯æ“ä½œçš„å®è·µæ•°æ®ï¼Œæˆ‘å¯ä»¥éšæ—¶ç»“åˆæ¶æ„å›¾å’Œå†…æ ¸çº§è§£é‡Šè¿›è¡Œæ·±å…¥è®²è§£ã€‚
