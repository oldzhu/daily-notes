# å®Œæ•´LLMè®­ç»ƒç”Ÿå‘½å‘¨æœŸè¯¦è§£

æˆ‘å°†è¯¦ç»†è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å„é˜¶æ®µç›®æ ‡ã€æ•°æ®ã€æŠ€æœ¯å’Œå®é™…åº”ç”¨ã€‚

## ğŸ“Š å®Œæ•´LLMè®­ç»ƒç”Ÿå‘½å‘¨æœŸï¼ˆ7ä¸ªé˜¶æ®µï¼‰

```mermaid
graph TD
    A[æ•°æ®æ”¶é›†] --> B[é¢„è®­ç»ƒ Pre-training]
    B --> C[æœ‰ç›‘ç£å¾®è°ƒ SFT]
    C --> D[å¥–åŠ±å»ºæ¨¡ RM]
    D --> E[å¼ºåŒ–å­¦ä¹  RLHF/RL]
    E --> F[å¯¹é½è°ƒä¼˜ Alignment Tuning]
    F --> G[æŒç»­é¢„è®­ç»ƒ CPT]
    G --> H[éƒ¨ç½²ä¸ç›‘æ§]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
    style G fill:#e0f2f1
    style H fill:#fce4ec
```

## 1. é¢„è®­ç»ƒ (Pre-training)

### æ ¸å¿ƒç›®æ ‡ï¼šå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
- **ä¸­æ–‡**ï¼šåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šå­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹å’Œä¸–ç•ŒçŸ¥è¯†
- **è‹±æ–‡**ï¼šLearn general language representations from massive unlabeled text

### æŠ€æœ¯ç»†èŠ‚ï¼š
```python
# é¢„è®­ç»ƒæŸå¤±å‡½æ•°ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼‰
def pretraining_loss(model_output, targets):
    """
    è‡ªå›å½’è¯­è¨€å»ºæ¨¡æŸå¤±
    Auto-regressive Language Modeling Loss
    
    è¾“å…¥å½¢çŠ¶: [batch_size, seq_len, vocab_size]
    Input shape: [batch_size, seq_len, vocab_size]
    """
    logits = model_output.logits  # æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ / Model predictions
    shift_logits = logits[..., :-1, :].contiguous()  # é¢„æµ‹tokens / Predict tokens
    shift_labels = targets[..., 1:].contiguous()     # ç›®æ ‡tokens / Target tokens
    
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=-100
    )
    return loss

# è®­ç»ƒæ•°æ®ç¤ºä¾‹
pretrain_corpus = """
äº’è”ç½‘æ•°æ®ï¼ˆç½‘é¡µã€æ–°é—»ã€ç™¾ç§‘ã€è®ºå›ï¼‰
Books, Wikipedia, Reddit, News articles
ä»£ç ä»“åº“ï¼ˆGitHubï¼‰
Common Crawl, GitHub repositories
å­¦æœ¯è®ºæ–‡
Scientific papers, arXiv
å¤šè¯­è¨€æ–‡æœ¬
Multi-lingual texts
"""
```

### å…³é”®æŒ‘æˆ˜ï¼š
- **è®¡ç®—æˆæœ¬**ï¼šæ•°åƒGPU/TPUæœˆï¼Œç™¾ä¸‡ç¾å…ƒçº§åˆ«
- **æ•°æ®è´¨é‡**ï¼šéœ€è¦é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ–‡æœ¬
- **è®­ç»ƒç¨³å®šæ€§**ï¼šéœ€è¦ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒç­–ç•¥ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªç­‰ï¼‰

## 2. æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT)

### æ ¸å¿ƒç›®æ ‡ï¼šå­¦ä¹ éµå¾ªæŒ‡ä»¤
- **ä¸­æ–‡**ï¼šæ•™æ¨¡å‹ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤
- **è‹±æ–‡**ï¼šTeach model to understand and follow human instructions

### SFTæµç¨‹ï¼š
```python
class SFTDataset:
    """æœ‰ç›‘ç£å¾®è°ƒæ•°æ®é›†æ ¼å¼ / Supervised Fine-Tuning Dataset Format"""
    
    def __init__(self):
        # å…¸å‹SFTæ•°æ®æ ¼å¼
        self.examples = [
            {
                "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "input": "",  # æœ‰æ—¶ä¸ºç©º
                "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...",
                "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"
            },
            {
                "instruction": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
                "input": "",
                "output": "æ˜¥é£æ‹‚é¢èŠ±é¦™æº¢ï¼Œä¸‡ç‰©å¤è‹ç”Ÿæœºå‹ƒ...",
                "system": "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„AI"
            }
        ]
    
    def format_prompt(self, example):
        """æ ¼å¼åŒ–å¯¹è¯æç¤º / Format conversation prompt"""
        # å¸¸ç”¨æ ¼å¼: System + Human + Assistant
        prompt = f"""<|system|>
{example['system']}</s>
<|user|>
{example['instruction']}
{example['input']}</s>
<|assistant|>
{example['output']}</s>"""
        return prompt

# SFTè®­ç»ƒä¼ªä»£ç 
def sft_training_loop(model, sft_data, num_epochs=3):
    """
    SFTè®­ç»ƒå¾ªç¯
    SFT Training Loop
    """
    optimizer = AdamW(model.parameters(), lr=2e-5)
    
    for epoch in range(num_epochs):
        for batch in sft_data:
            # åªè®¡ç®—assistantéƒ¨åˆ†çš„æŸå¤±
            # Only compute loss on assistant responses
            outputs = model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                labels=batch['labels']
            )
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
```

### SFTæ•°æ®æ¥æºï¼š
1. **äººå·¥ç¼–å†™**ï¼šé›‡ä½£æ ‡æ³¨å›¢é˜Ÿåˆ›å»ºé«˜è´¨é‡å¯¹è¯
2. **ç°æœ‰æ•°æ®é›†**ï¼š
   - Alpaca (52KæŒ‡ä»¤)
   - Dolly (15KæŒ‡ä»¤)
   - OpenAssistant (161Kå¤šè¯­è¨€å¯¹è¯)
   - ShareGPT (ç”¨æˆ·ä¸ChatGPTçš„å¯¹è¯)
3. **åˆæˆæ•°æ®**ï¼šä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®

## 3. å¥–åŠ±å»ºæ¨¡ (Reward Modeling, RM)

### æ ¸å¿ƒç›®æ ‡ï¼šå­¦ä¹ äººç±»åå¥½
- **ä¸­æ–‡**ï¼šè®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥è¯„ä¼°å›å¤çš„è´¨é‡
- **è‹±æ–‡**ï¼šTrain a model to evaluate response quality

### RMè®­ç»ƒæµç¨‹ï¼š
```python
class RewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹ç»“æ„ / Reward Model Architecture"""
    
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model  # é€šå¸¸æ˜¯SFTæ¨¡å‹ / Usually SFT model
        self.reward_head = nn.Linear(
            base_model.config.hidden_size, 1
        )  # æ ‡é‡å¥–åŠ±è¾“å‡º / Scalar reward output
    
    def forward(self, input_ids, attention_mask):
        # è·å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        # Get last token hidden state
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        last_hidden = outputs.hidden_states[-1]
        last_token_hidden = last_hidden[:, -1, :]  # [batch, hidden_size]
        
        # è®¡ç®—å¥–åŠ±åˆ†æ•°
        # Compute reward score
        reward = self.reward_head(last_token_hidden)
        return reward

# åå¥½æ•°æ®æ ¼å¼
preference_data = [
    {
        "prompt": "è§£é‡Šé‡å­è®¡ç®—",
        "chosen": "é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†...",  # æ›´å¥½çš„å›ç­” / Better response
        "rejected": "é‡å­è®¡ç®—å°±æ˜¯å¾ˆå¿«çš„è®¡ç®—...",    # æ›´å·®çš„å›ç­” / Worse response
        "chosen_score": 0.9,  # äººå·¥è¯„åˆ†ï¼ˆå¯é€‰ï¼‰
        "rejected_score": 0.2
    }
]

# æŸå¤±å‡½æ•° - æˆå¯¹æ’åæŸå¤±
def preference_loss(chosen_rewards, rejected_rewards):
    """
    æˆå¯¹æ’åæŸå¤±
    Pairwise Ranking Loss
    
    ç›®æ ‡ï¼šè®©chosençš„å¥–åŠ± > rejectedçš„å¥–åŠ±
    Goal: Make chosen reward > rejected reward
    """
    # Bradley-Terryæ¨¡å‹
    # Bradley-Terry model
    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()
    return loss

# æˆ–è€…ä½¿ç”¨InfoNCEæŸå¤±
def info_nce_loss(rewards, temperature=0.1):
    """
    InfoNCEå¯¹æ¯”æŸå¤±
    InfoNCE Contrastive Loss
    """
    # rewards shape: [batch_size]
    # å‡è®¾æ¯ä¸ªbatchä¸­ç¬¬ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬
    # Assume first in each batch is positive
    pos_rewards = rewards[::2]
    neg_rewards = rewards[1::2]
    
    logits = torch.stack([pos_rewards, neg_rewards], dim=1) / temperature
    labels = torch.zeros(len(pos_rewards), dtype=torch.long)
    
    loss = F.cross_entropy(logits, labels)
    return loss
```

## 4. å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ (RLHF/RL)

### æ ¸å¿ƒç›®æ ‡ï¼šæ ¹æ®äººç±»åå¥½ä¼˜åŒ–æ¨¡å‹
- **ä¸­æ–‡**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ åŸºäºå¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç”Ÿæˆç­–ç•¥
- **è‹±æ–‡**ï¼šUse RL to optimize generation policy based on reward model

### PPOç®—æ³•å®ç°ï¼š
```python
class RLHFTrainer:
    """RLHFè®­ç»ƒå™¨ / RLHF Trainer using PPO"""
    
    def __init__(self, policy_model, reward_model, ref_model):
        """
        policy_model: è¦ä¼˜åŒ–çš„æ¨¡å‹ï¼ˆSFTåçš„æ¨¡å‹ï¼‰
        reward_model: å¥–åŠ±æ¨¡å‹
        ref_model: å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸ä¸policy_modelåˆå§‹ç›¸åŒï¼‰
        
        policy_model: Model to optimize (after SFT)
        reward_model: Reward model
        ref_model: Reference model (usually initial policy_model)
        """
        self.policy_model = policy_model
        self.reward_model = reward_model
        self.ref_model = ref_model  # ç”¨äºKLæ•£åº¦æƒ©ç½š / For KL divergence penalty
        
    def compute_advantages(self, rewards, values, gamma=0.99, lam=0.95):
        """
        è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°
        Compute GAE advantages
        
        GAE: Generalized Advantage Estimation
        é€šç”¨ä¼˜åŠ¿ä¼°è®¡
        """
        advantages = []
        gae = 0
        next_value = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + gamma * next_value - values[t]
            gae = delta + gamma * lam * gae
            advantages.insert(0, gae)
            next_value = values[t]
        
        return torch.tensor(advantages)
    
    def ppo_loss(self, old_logprobs, new_logprobs, advantages, 
                 epsilon=0.2, beta=0.01):
        """
        PPOè£å‰ªç›®æ ‡å‡½æ•°
        PPO Clipped Objective
        
        åŒ…å«ç­–ç•¥æŸå¤±å’Œä»·å€¼æŸå¤±
        Includes policy loss and value loss
        """
        # ç­–ç•¥æ¯”ç‡
        # Policy ratio
        ratio = torch.exp(new_logprobs - old_logprobs)
        
        # è£å‰ªçš„PPOç›®æ ‡
        # Clipped PPO objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # KLæ•£åº¦æƒ©ç½šï¼ˆé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹å¤ªå¤šï¼‰
        # KL divergence penalty (prevent drifting too far)
        kl_penalty = beta * (new_logprobs - old_logprobs).mean()
        
        return policy_loss + kl_penalty
    
    def train_step(self, prompts):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤
        Single training step
        """
        # 1. ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆå›å¤
        # 1. Generate responses with current policy
        with torch.no_grad():
            ref_logits = self.ref_model(prompts).logits
            policy_logits = self.policy_model(prompts).logits
        
        # 2. é‡‡æ ·åŠ¨ä½œï¼ˆtokensï¼‰
        # 2. Sample actions (tokens)
        policy_dist = Categorical(logits=policy_logits)
        actions = policy_dist.sample()
        new_logprobs = policy_dist.log_prob(actions)
        
        # 3. è®¡ç®—å¥–åŠ±
        # 3. Compute rewards
        with torch.no_grad():
            # å¥–åŠ±æ¨¡å‹åˆ†æ•°
            # Reward model score
            rm_rewards = self.reward_model(
                input_ids=torch.cat([prompts, actions], dim=1)
            )
            
            # KLæƒ©ç½šï¼ˆé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹ï¼‰
            # KL penalty (prevent divergence from reference)
            ref_dist = Categorical(logits=ref_logits)
            ref_logprobs = ref_dist.log_prob(actions)
            kl_penalty = beta * (new_logprobs - ref_logprobs).mean()
            
            # æ€»å¥–åŠ±
            # Total reward
            total_rewards = rm_rewards - kl_penalty
        
        # 4. PPOæ›´æ–°
        # 4. PPO update
        loss = self.ppo_loss(
            old_logprobs=ref_logprobs,
            new_logprobs=new_logprobs,
            advantages=self.compute_advantages(total_rewards),
            epsilon=0.2,
            beta=0.01
        )
        
        return loss
```

## 5. å¯¹é½è°ƒä¼˜ (Alignment Tuning)

### æ ¸å¿ƒç›®æ ‡ï¼šç¡®ä¿æ¨¡å‹ç¬¦åˆäººç±»ä»·å€¼è§‚
- **ä¸­æ–‡**ï¼šè¿›ä¸€æ­¥å¾®è°ƒä½¿æ¨¡å‹æ›´å®‰å…¨ã€æ›´æœ‰å¸®åŠ©ã€æ›´è¯šå®
- **è‹±æ–‡**ï¼šFurther fine-tune to make model safer, more helpful, more honest

### å¯¹é½æŠ€æœ¯ï¼š
```python
class AlignmentTechniques:
    """å¯¹é½æŠ€æœ¯é›†åˆ / Alignment Techniques Collection"""
    
    @staticmethod
    def constitutional_ai(model, constitution):
        """
        å®ªæ³•AIï¼šä½¿ç”¨åŸåˆ™åˆ—è¡¨æŒ‡å¯¼æ¨¡å‹
        Constitutional AI: Use list of principles to guide model
        
        Anthropicçš„å®ªæ³•AIæ–¹æ³•
        Anthropic's Constitutional AI approach
        """
        principles = [
            "è¯·æä¾›æœ‰å¸®åŠ©ã€æ— å®³ã€è¯šå®çš„å›ç­”",
            "è¯·å°Šé‡æ‰€æœ‰æ–‡åŒ–å’Œä¸ªä½“",
            "é¿å…æä¾›å±é™©æˆ–éæ³•çš„å»ºè®®",
            "æ‰¿è®¤çŸ¥è¯†çš„å±€é™æ€§",
            # "Please provide helpful, harmless, honest responses",
            # "Respect all cultures and individuals",
            # "Avoid dangerous or illegal advice",
            # "Acknowledge limitations of knowledge"
        ]
        
        # ä½¿ç”¨åŸåˆ™è¿›è¡Œå¼ºåŒ–å­¦ä¹ 
        # Use principles for reinforcement learning
        return model
    
    @staticmethod
    def dpo_training(policy_model, ref_model, preference_data):
        """
        ç›´æ¥åå¥½ä¼˜åŒ–
        Direct Preference Optimization
        
        æ›¿ä»£RLHFçš„æ›´ç®€å•æ–¹æ³•
        Simpler alternative to RLHF
        """
        # DPOæŸå¤±å‡½æ•°
        # DPO loss function
        def dpo_loss(policy_logps, ref_logps, beta=0.1):
            """
            policy_logps: ç­–ç•¥æ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
            ref_logps: å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
            beta: æ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹çš„ç¨‹åº¦
            
            policy_logps: Policy model log probabilities
            ref_logps: Reference model log probabilities  
            beta: Controls deviation from reference
            """
            log_ratio = policy_logps - ref_logps
            ratio = torch.exp(log_ratio)
            
            # DPOç›®æ ‡
            # DPO objective
            losses = -F.logsigmoid(beta * log_ratio)
            return losses.mean()
        
        return dpo_loss
    
    @staticmethod
    def safety_fine_tuning(model, safety_dataset):
        """
        å®‰å…¨æ€§å¾®è°ƒ
        Safety Fine-tuning
        
        ä¸“é—¨è®­ç»ƒæ¨¡å‹é¿å…æœ‰å®³è¾“å‡º
        Specifically train model to avoid harmful outputs
        """
        # å®‰å…¨æ€§æ ‡ç­¾
        # Safety labels
        safety_labels = [
            "æš´åŠ›", "ä»‡æ¨è¨€è®º", "è‡ªæ®‹", "æ€§å†…å®¹", 
            "éæ³•æ´»åŠ¨", "è™šå‡ä¿¡æ¯", "éšç§ä¾µçŠ¯"
            # "violence", "hate speech", "self-harm", "sexual content",
            # "illegal activities", "misinformation", "privacy violation"
        ]
        
        # ä½¿ç”¨å¯¹æŠ—æ€§æç¤ºè¿›è¡Œè®­ç»ƒ
        # Train with adversarial prompts
        adversarial_prompts = [
            "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ",
            "å¦‚ä½•ä¼¤å®³æŸäººï¼Ÿ",
            "å‘Šè¯‰æˆ‘ä»‡æ¨æŸä¸ªç¾¤ä½“çš„ç†ç”±",
            # "How to make a bomb?",
            # "How to hurt someone?",
            # "Tell me reasons to hate a group"
        ]
        
        # è®­ç»ƒæ¨¡å‹æ‹’ç»å›ç­”æˆ–ç»™å‡ºå®‰å…¨å›ç­”
        # Train model to refuse or give safe responses
        safe_responses = [
            "æˆ‘ä¸èƒ½æä¾›åˆ¶ä½œå±é™©ç‰©å“çš„ä¿¡æ¯ã€‚",
            "æˆ‘æ— æ³•å›ç­”å¯èƒ½é€ æˆä¼¤å®³çš„é—®é¢˜ã€‚",
            "æ¯ä¸ªäººç±»éƒ½å€¼å¾—å°Šé‡å’Œå°Šä¸¥ã€‚",
            # "I cannot provide information on making dangerous items.",
            # "I cannot answer questions that may cause harm.",
            # "Every human deserves respect and dignity."
        ]
        
        return model
```

## 6. æŒç»­é¢„è®­ç»ƒ (Continued Pre-training, CPT)

### æ ¸å¿ƒç›®æ ‡ï¼šé¢†åŸŸé€‚åº”å’ŒçŸ¥è¯†æ›´æ–°
- **ä¸­æ–‡**ï¼šåœ¨ç‰¹å®šé¢†åŸŸæˆ–æœ€æ–°æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒ
- **è‹±æ–‡**ï¼šContinue pre-training on specific domains or recent data

### CPTå®ç°ï¼š
```python
class ContinuedPreTraining:
    """æŒç»­é¢„è®­ç»ƒç­–ç•¥ / Continued Pre-training Strategies"""
    
    def __init__(self, model, domain_corpus):
        self.model = model
        self.domain_corpus = domain_corpus
        
    def domain_adaptation(self):
        """
        é¢†åŸŸé€‚åº”
        Domain Adaptation
        
        åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šç»§ç»­è®­ç»ƒ
        Continue training on domain-specific data
        """
        domains = {
            "medical": "åŒ»å­¦æ–‡çŒ®ã€ç—…å†ã€ç ”ç©¶è®ºæ–‡",
            "legal": "æ³•å¾‹æ¡æ–‡ã€æ¡ˆä¾‹ã€åˆåŒ",
            "code": "GitHubä»“åº“ã€æŠ€æœ¯æ–‡æ¡£",
            "multilingual": "å¤šè¯­è¨€æ–‡æœ¬",
            # "medical": "Medical literature, records, research papers",
            # "legal": "Legal texts, cases, contracts", 
            # "code": "GitHub repos, technical docs",
            # "multilingual": "Multi-lingual texts"
        }
        
        # è®­ç»ƒç­–ç•¥
        # Training strategies
        strategies = {
            "gradual_unfreezing": "é€æ¸è§£å†»å±‚",
            "layerwise_lr": "ä¸åŒå±‚ä¸åŒå­¦ä¹ ç‡",
            "lora_adaptation": "ä½¿ç”¨LoRAé€‚é…",
            # "gradual_unfreezing": "Gradually unfreeze layers",
            # "layerwise_lr": "Different LR per layer",
            # "lora_adaptation": "Use LoRA adaptation"
        }
        
        return self.model
    
    def knowledge_update(self, recent_data):
        """
        çŸ¥è¯†æ›´æ–°
        Knowledge Update
        
        ç”¨æœ€æ–°æ•°æ®æ›´æ–°æ¨¡å‹çŸ¥è¯†
        Update model knowledge with recent data
        """
        # å¤„ç†æ—¶é—´æ•æ„Ÿä¿¡æ¯
        # Handle time-sensitive information
        recent_topics = [
            "2024å¹´å¤§é€‰", "æœ€æ–°ç§‘æŠ€çªç ´", "å½“å‰ç»æµçŠ¶å†µ",
            "è¿‘æœŸè‡ªç„¶ç¾å®³", "æ–°å† ç–«æƒ…æœ€æ–°å‘å±•"
            # "2024 elections", "Latest tech breakthroughs", 
            # "Current economic situation", "Recent natural disasters",
            # "Latest COVID-19 developments"
        ]
        
        # æŒ‘æˆ˜ï¼šé¿å…ç¾éš¾æ€§é—å¿˜
        # Challenge: Avoid catastrophic forgetting
        techniques = [
            "å›æ”¾ç¼“å†²ï¼ˆä¿ç•™æ—§æ•°æ®ï¼‰",
            "å¼¹æ€§æƒé‡åˆå¹¶",
            "çŸ¥è¯†è’¸é¦",
            # "Replay buffer (keep old data)",
            # "Elastic Weight Consolidation",
            # "Knowledge Distillation"
        ]
        
        return self.model
```

## 7. è¯„ä¼°å’Œéƒ¨ç½² (Evaluation & Deployment)

### æ ¸å¿ƒç›®æ ‡ï¼šå…¨é¢è¯„ä¼°å’Œå¯é éƒ¨ç½²
```python
class LLMEvaluation:
    """LLMç»¼åˆè¯„ä¼° / Comprehensive LLM Evaluation"""
    
    @staticmethod
    def automated_metrics():
        """è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ / Automated Evaluation Metrics"""
        return {
            "è¯­è¨€ç†è§£": ["MMLU", "HellaSwag", "ARC", "BoolQ"],
            "ä»£ç èƒ½åŠ›": ["HumanEval", "MBPP", "APPS"],
            "æ•°å­¦æ¨ç†": ["GSM8K", "MATH", "AMC"],
            "å¤šè¯­è¨€": ["XNLI", "XQuAD", "TyDiQA"],
            "å®‰å…¨æ€§": ["ToxiGen", "RealToxicityPrompts"],
            "æŒ‡ä»¤éµå¾ª": ["AlpacaEval", "MT-Bench"],
            # "Language Understanding": ["MMLU", "HellaSwag", "ARC", "BoolQ"],
            # "Coding": ["HumanEval", "MBPP", "APPS"],
            # "Math Reasoning": ["GSM8K", "MATH", "AMC"],
            # "Multilingual": ["XNLI", "XQuAD", "TyDiQA"],
            # "Safety": ["ToxiGen", "RealToxicityPrompts"],
            # "Instruction Following": ["AlpacaEval", "MT-Bench"]
        }
    
    @staticmethod
    def human_evaluation():
        """äººå·¥è¯„ä¼°ç»´åº¦ / Human Evaluation Dimensions"""
        dimensions = {
            "æœ‰å¸®åŠ©æ€§": "å›ç­”æ˜¯å¦è§£å†³äº†ç”¨æˆ·é—®é¢˜",
            "çœŸå®æ€§": "å›ç­”æ˜¯å¦å‡†ç¡®ã€æ— å¹»è§‰",
            "å®‰å…¨æ€§": "å›ç­”æ˜¯å¦æ— å®³ã€æ— åè§",
            "æµç•…æ€§": "è¯­è¨€æ˜¯å¦è‡ªç„¶ã€è¿è´¯",
            "ç›¸å…³æ€§": "å›ç­”æ˜¯å¦ç›¸å…³ã€ä¸è·‘é¢˜",
            # "Helpfulness": "Does answer solve user's problem",
            # "Truthfulness": "Is answer accurate, no hallucinations",
            # "Safety": "Is answer harmless, unbiased",
            # "Fluency": "Is language natural, coherent",
            # "Relevance": "Is answer relevant, on-topic"
        }
        return dimensions
    
    @staticmethod
    def red_teaming():
        """çº¢é˜Ÿæµ‹è¯• / Red Teaming"""
        attack_vectors = [
            "è¶Šç‹±æç¤ºï¼ˆç»•è¿‡å®‰å…¨é™åˆ¶ï¼‰",
            "å¯¹æŠ—æ€§è¾“å…¥ï¼ˆè§¦å‘é”™è¯¯è¡Œä¸ºï¼‰",
            "ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆé€šè¿‡é•¿ä¸Šä¸‹æ–‡æ”»å‡»ï¼‰",
            "å¤šè½®å¯¹è¯æ”»å‡»ï¼ˆé€æ¸å¼•å¯¼ï¼‰",
            # "Jailbreak prompts (bypass safety)",
            # "Adversarial inputs (trigger misbehavior)",
            # "Context injection (attack via long context)",
            # "Multi-turn attacks (gradual manipulation)"
        ]
        
        defenses = [
            "è¾“å…¥è¿‡æ»¤å’Œæ¸…ç†",
            "è¾“å‡ºå†…å®¹å®¡æ ¸",
            "ä¸ç¡®å®šæ€§æ ¡å‡†",
            "äººç±»å®¡æ ¸å¾ªç¯",
            # "Input filtering and sanitization",
            # "Output content moderation",
            # "Uncertainty calibration",
            # "Human-in-the-loop review"
        ]
        
        return attack_vectors, defenses

class DeploymentStrategies:
    """éƒ¨ç½²ç­–ç•¥ / Deployment Strategies"""
    
    def __init__(self):
        self.strategies = {
            "æ¸è¿›å¼å‘å¸ƒ": "é€æ¸å¢åŠ ç”¨æˆ·è®¿é—®é‡",
            "A/Bæµ‹è¯•": "æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„æ•ˆæœ",
            "å½±å­éƒ¨ç½²": "åœ¨ä¸å½±å“ç”¨æˆ·çš„æƒ…å†µä¸‹æµ‹è¯•",
            "é‡‘ä¸é›€å‘å¸ƒ": "å…ˆå‘å°éƒ¨åˆ†ç”¨æˆ·å‘å¸ƒ",
            "å›æ»šè®¡åˆ’": "å‡†å¤‡å¥½å¿«é€Ÿå›æ»šåˆ°æ—§ç‰ˆæœ¬",
            # "Progressive rollouts": "Gradually increase user access",
            # "A/B testing": "Compare different versions",
            # "Shadow deployment": "Test without affecting users",
            # "Canary releases": "Release to small subset first",
            # "Rollback plans": "Be ready to roll back quickly"
        }
    
    def monitoring_metrics(self):
        """ç›‘æ§æŒ‡æ ‡ / Monitoring Metrics"""
        return {
            "æ€§èƒ½": ["å»¶è¿Ÿ", "ååé‡", "é”™è¯¯ç‡"],
            "è´¨é‡": ["ç”¨æˆ·æ»¡æ„åº¦", "ä»»åŠ¡å®Œæˆç‡", "åé¦ˆè¯„åˆ†"],
            "æˆæœ¬": ["è®¡ç®—æˆæœ¬", "å­˜å‚¨æˆæœ¬", "APIæˆæœ¬"],
            "å®‰å…¨æ€§": ["æ»¥ç”¨æ£€æµ‹", "å†…å®¹è¿è§„", "éšç§æ³„éœ²"],
            # "Performance": ["Latency", "Throughput", "Error rate"],
            # "Quality": ["User satisfaction", "Task completion", "Feedback scores"],
            # "Cost": ["Compute cost", "Storage cost", "API cost"],
            # "Safety": ["Abuse detection", "Content violations", "Privacy leaks"]
        }
```

## ğŸ“ˆ å®Œæ•´è®­ç»ƒæµç¨‹ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm

class CompleteLLMTrainingPipeline:
    """
    å®Œæ•´LLMè®­ç»ƒæµç¨‹
    Complete LLM Training Pipeline
    """
    
    def __init__(self, model_name="meta-llama/Llama-2-7b"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        # 1. Load base model
        print("æ­¥éª¤1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        print("Step 1: Loading pre-trained model")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        ).to(self.device)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è®­ç»ƒçŠ¶æ€
        # Training state
        self.training_stage = "pre-training"
        self.checkpoint_paths = {}
    
    def pretrain(self, corpus_path, epochs=1):
        """é¢„è®­ç»ƒé˜¶æ®µ / Pre-training stage"""
        print(f"\n{'='*50}")
        print("é˜¶æ®µ1: é¢„è®­ç»ƒ (Pre-training)")
        print(f"{'='*50}")
        
        # åŠ è½½é¢„è®­ç»ƒæ•°æ®
        # Load pre-training data
        dataset = load_dataset("text", data_files=corpus_path, split="train")
        
        # ç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…ä¸­éœ€è¦æ›´å¤æ‚çš„æ•°æ®å¤„ç†
        # Simplified example, real implementation needs more complex processing
        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)
        
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
                # åˆ†è¯
                # Tokenize
                texts = batch["text"]
                inputs = self.tokenizer(
                    texts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                # å‰å‘ä¼ æ’­
                # Forward pass
                outputs = self.model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataloader)
            print(f"é¢„è®­ç»ƒ Epoch {epoch+1}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"Pre-train Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        # Save checkpoint
        self.checkpoint_paths["pretrained"] = "./checkpoints/pretrained"
        self.model.save_pretrained(self.checkpoint_paths["pretrained"])
        self.tokenizer.save_pretrained(self.checkpoint_paths["pretrained"])
        
        return self.model
    
    def supervised_finetune(self, sft_dataset, epochs=3):
        """æœ‰ç›‘ç£å¾®è°ƒ / Supervised Fine-tuning"""
        print(f"\n{'='*50}")
        print("é˜¶æ®µ2: æœ‰ç›‘ç£å¾®è°ƒ (SFT)")
        print(f"{'='*50}")
        
        # åŠ è½½SFTæ•°æ®é›†
        # Load SFT dataset
        dataset = load_dataset("json", data_files=sft_dataset, split="train")
        
        def format_sft_example(example):
            """æ ¼å¼åŒ–SFTç¤ºä¾‹ / Format SFT example"""
            # å®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ ¼å¼å¤„ç†
            # May need more complex formatting in practice
            prompt = f"Instruction: {example['instruction']}\n\nResponse: {example['output']}"
            return prompt
        
        processed_data = dataset.map(
            lambda x: {"text": format_sft_example(x)},
            remove_columns=dataset.column_names
        )
        
        dataloader = DataLoader(processed_data, batch_size=4, shuffle=True)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
        
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in tqdm(dataloader, desc=f"SFT Epoch {epoch+1}"):
                texts = batch["text"]
                inputs = self.tokenizer(
                    texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                # è®­ç»ƒæ•´ä¸ªåºåˆ—
                # Train on entire sequence
                outputs = self.model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataloader)
            print(f"SFT Epoch {epoch+1}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"SFT Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}")
        
        # ä¿å­˜SFTæ¨¡å‹
        # Save SFT model
        self.checkpoint_paths["sft"] = "./checkpoints/sft"
        self.model.save_pretrained(self.checkpoint_paths["sft"])
        
        return self.model
    
    def train_reward_model(self, preference_data):
        """è®­ç»ƒå¥–åŠ±æ¨¡å‹ / Train Reward Model"""
        print(f"\n{'='*50}")
        print("é˜¶æ®µ3: è®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Modeling)")
        print(f"{'='*50}")
        
        # åˆ›å»ºå¥–åŠ±æ¨¡å‹ï¼ˆåœ¨SFTæ¨¡å‹åŸºç¡€ä¸Šï¼‰
        # Create reward model (based on SFT model)
        reward_model = AutoModelForCausalLM.from_pretrained(
            self.checkpoint_paths["sft"]
        )
        
        # æ·»åŠ å¥–åŠ±å¤´
        # Add reward head
        class RewardModelWrapper(torch.nn.Module):
            def __init__(self, base_model):
                super().__init__()
                self.base_model = base_model
                self.reward_head = torch.nn.Linear(
                    base_model.config.hidden_size, 1
                )
            
            def forward(self, input_ids, attention_mask):
                outputs = self.base_model.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=True
                )
                last_hidden = outputs.hidden_states[-1]
                last_token_hidden = last_hidden[:, -1, :]
                reward = self.reward_head(last_token_hidden)
                return reward
        
        reward_model = RewardModelWrapper(reward_model).to(self.device)
        
        # åŠ è½½åå¥½æ•°æ®
        # Load preference data
        dataset = load_dataset("json", data_files=preference_data, split="train")
        
        # è®­ç»ƒå¥–åŠ±æ¨¡å‹
        # Train reward model
        optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)
        
        reward_model.train()
        for epoch in range(3):
            total_loss = 0
            for batch in tqdm(dataset, desc=f"RM Epoch {epoch+1}"):
                # å¤„ç†chosenå’Œrejectedå›å¤
                # Process chosen and rejected responses
                chosen_inputs = self.tokenizer(
                    batch["chosen"],
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                rejected_inputs = self.tokenizer(
                    batch["rejected"],
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                # è®¡ç®—å¥–åŠ±
                # Compute rewards
                chosen_rewards = reward_model(
                    chosen_inputs["input_ids"],
                    chosen_inputs["attention_mask"]
                )
                
                rejected_rewards = reward_model(
                    rejected_inputs["input_ids"],
                    rejected_inputs["attention_mask"]
                )
                
                # æˆå¯¹æ’åæŸå¤±
                # Pairwise ranking loss
                loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataset)
            print(f"å¥–åŠ±æ¨¡å‹ Epoch {epoch+1}, æŸå¤±: {avg_loss:.4f}")
            print(f"Reward Model Epoch {epoch+1}, Loss: {avg_loss:.4f}")
        
        # ä¿å­˜å¥–åŠ±æ¨¡å‹
        # Save reward model
        torch.save(reward_model.state_dict(), "./checkpoints/reward_model.pt")
        
        return reward_model
    
    def rlhf_finetune(self, reward_model, prompts_dataset, epochs=1):
        """RLHFå¾®è°ƒ / RLHF Fine-tuning"""
        print(f"\n{'='*50}")
        print("é˜¶æ®µ4: RLHFå¾®è°ƒ (RLHF)")
        print(f"{'='*50}")
        
        # ç®€åŒ–ç‰ˆçš„PPOå®ç°
        # Simplified PPO implementation
        print("æ³¨æ„: å®Œæ•´PPOå®ç°éå¸¸å¤æ‚ï¼Œè¿™é‡Œä»…ä¸ºç¤ºæ„")
        print("Note: Full PPO is complex, this is justç¤ºæ„")
        
        # å®é™…å®ç°éœ€è¦ä½¿ç”¨ä¸“é—¨çš„RLåº“
        # Real implementation needs dedicated RL libraries
        print("å»ºè®®ä½¿ç”¨trlåº“: https://github.com/huggingface/trl")
        print("Recommended to use trl library")
        
        return self.model
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹ / Run complete training pipeline"""
        print("å¼€å§‹å®Œæ•´LLMè®­ç»ƒæµç¨‹")
        print("Starting complete LLM training pipeline")
        print("-" * 50)
        
        # 1. é¢„è®­ç»ƒ
        # 1. Pre-training
        self.pretrain("corpus.txt", epochs=1)
        
        # 2. SFT
        self.supervised_finetune("sft_data.json", epochs=2)
        
        # 3. å¥–åŠ±å»ºæ¨¡
        # 3. Reward Modeling
        reward_model = self.train_reward_model("preference_data.json")
        
        # 4. RLHF
        self.rlhf_finetune(reward_model, "prompts.json", epochs=1)
        
        print("\n" + "="*50)
        print("è®­ç»ƒæµç¨‹å®Œæˆ!")
        print("Training pipeline complete!")
        print("="*50)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        # Save final model
        final_path = "./final_model"
        self.model.save_pretrained(final_path)
        self.tokenizer.save_pretrained(final_path)
        
        print(f"æ¨¡å‹ä¿å­˜åˆ°: {final_path}")
        print(f"Model saved to: {final_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    pipeline = CompleteLLMTrainingPipeline("gpt2")  # ä½¿ç”¨å°æ¨¡å‹æ¼”ç¤º
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    # Run complete pipeline
    # pipeline.run_complete_pipeline()
    
    # æˆ–è€…è¿è¡Œå•ä¸ªé˜¶æ®µ
    # Or run individual stages
    print("é€‰æ‹©è®­ç»ƒé˜¶æ®µ:")
    print("1. é¢„è®­ç»ƒ (Pre-training)")
    print("2. æœ‰ç›‘ç£å¾®è°ƒ (SFT)")
    print("3. å¥–åŠ±å»ºæ¨¡ (Reward Modeling)")
    print("4. RLHFå¾®è°ƒ")
    print("5. å®Œæ•´æµç¨‹")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-5): ")
    
    if choice == "1":
        pipeline.pretrain("data/corpus.txt")
    elif choice == "2":
        pipeline.supervised_finetune("data/sft_data.json")
    elif choice == "5":
        pipeline.run_complete_pipeline()
```

## ğŸ¯ å…³é”®æ¦‚å¿µæ€»ç»“

| é˜¶æ®µ | ä¸­æ–‡ | è‹±æ–‡ | ç›®æ ‡ | æ•°æ® | æŠ€æœ¯ |
|------|------|------|------|------|------|
| **é¢„è®­ç»ƒ** | é¢„è®­ç»ƒ | Pre-training | å­¦ä¹ è¯­è¨€åŸºç¡€ | å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ | è‡ªå›å½’è¯­è¨€å»ºæ¨¡ |
| **SFT** | æœ‰ç›‘ç£å¾®è°ƒ | Supervised Fine-tuning | å­¦ä¹ éµå¾ªæŒ‡ä»¤ | æŒ‡ä»¤-å›å¤å¯¹ | æœ‰ç›‘ç£å­¦ä¹  |
| **RM** | å¥–åŠ±å»ºæ¨¡ | Reward Modeling | å­¦ä¹ äººç±»åå¥½ | åå¥½å¯¹æ¯”æ•°æ® | æˆå¯¹æ’åå­¦ä¹  |
| **RLHF** | å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ | RLHF | ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ | åŠ¨æ€ç”Ÿæˆæ•°æ® | PPOç®—æ³• |
| **å¯¹é½** | å¯¹é½è°ƒä¼˜ | Alignment Tuning | ç¬¦åˆä»·å€¼è§‚ | å®‰å…¨/ä¼¦ç†æ•°æ® | å®ªæ³•AI, DPO |
| **CPT** | æŒç»­é¢„è®­ç»ƒ | Continued Pre-training | é¢†åŸŸé€‚åº” | é¢†åŸŸç‰¹å®šæ•°æ® | é¢†åŸŸé€‚åº”æŠ€æœ¯ |
| **è¯„ä¼°** | è¯„ä¼°éƒ¨ç½² | Evaluation & Deployment | ç¡®ä¿è´¨é‡å®‰å…¨ | è¯„ä¼°æ•°æ®é›† | çº¢é˜Ÿæµ‹è¯•, A/Bæµ‹è¯• |

## ğŸ”§ å®é™…å·¥å…·æ¨è

1. **é¢„è®­ç»ƒ**: Megatron-LM, DeepSpeed, Hugging Face Transformers
2. **SFT**: TRL (Transformer Reinforcement Learning), Axolotl
3. **RLHF**: TRL, RL4LMs, Colossal-AI
4. **è¯„ä¼°**: LM Evaluation Harness, HELM, AlpacaEval
5. **éƒ¨ç½²**: vLLM, TGI (Text Generation Inference), Triton

## ğŸ“š å­¦ä¹ èµ„æº

1. **è®ºæ–‡**:
   - InstructGPT (SFT+RLHF)
   - LLaMA (é¢„è®­ç»ƒ)
   - Constitutional AI (å¯¹é½)
   - DPO (ç›´æ¥åå¥½ä¼˜åŒ–)

2. **ä»£ç åº“**:
   - Hugging Face Transformers
   - TRL (Transformers Reinforcement Learning)
   - Axolotl (LLMè®­ç»ƒæ¡†æ¶)

3. **è¯¾ç¨‹**:
   - Stanford CS324 (å¤§è¯­è¨€æ¨¡å‹)
   - Hugging Face LLMè¯¾ç¨‹
   - DeepLearning.AI LLMè¯¾ç¨‹

è¿™ä¸ªå®Œæ•´çš„è®­ç»ƒæµç¨‹å±•ç¤ºäº†LLMä»åŸºç¡€é¢„è®­ç»ƒåˆ°æœ€ç»ˆéƒ¨ç½²çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å…¶ç‰¹å®šçš„ç›®æ ‡å’ŒæŠ€æœ¯æŒ‘æˆ˜ã€‚å®é™…è®­ç»ƒä¸­ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´æµç¨‹æˆ–è·³è¿‡æŸäº›é˜¶æ®µã€‚
