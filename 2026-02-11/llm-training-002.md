# PyTorchä¸C++åº•å±‚å®ç°å¯¹ç…§è¡¨

æˆ‘å°†æä¾›ä¸€ä¸ªè¯¦ç»†çš„æ˜ å°„è¡¨ï¼Œå±•ç¤ºPyTorché«˜çº§APIä¸åº•å±‚C++å®ç°çš„å¯¹åº”å…³ç³»ï¼Œå¸®åŠ©ä½ ç†è§£ä»é«˜å±‚åˆ°åº•å±‚çš„å®Œæ•´æ ˆã€‚

## ğŸ“Š å®Œæ•´æ˜ å°„å…³ç³»è¡¨

```mermaid
graph TB
    subgraph "PyTorch (High Level)"
        P1[nn.Module]
        P2[torch.Tensor]
        P3[torch.nn.functional]
        P4[torch.optim]
        P5[DataLoader]
        P6[torch.autograd]
    end
    
    subgraph "C++ Implementation (Low Level)"
        C1[C++ Class Hierarchy]
        C2[Tensor Data Structure]
        C3[Manual Math Operations]
        C4[Optimizer Algorithms]
        C5[Custom Data Pipeline]
        C6[Manual Gradients]
    end
    
    subgraph "LLM Components Mapping"
        M1[GPT Model]
        M2[Transformer Block]
        M3[Attention Mechanism]
        M4[Training Loop]
        M5[Loss Function]
        M6[Backward Pass]
    end
    
    P1 --> C1
    P2 --> C2
    P3 --> C3
    P4 --> C4
    P5 --> C5
    P6 --> C6
    
    M1 --> P1
    M2 --> P2
    M3 --> P3
    M4 --> P4
    M5 --> P5
    M6 --> P6
    
    M1 --> C1
    M2 --> C2
    M3 --> C3
    M4 --> C4
    M5 --> C5
    M6 --> C6
```

## 1. åŸºç¡€æ•°æ®ç»“æ„æ˜ å°„

### 1.1 Tensorç±»ï¼šPyTorch vs C++

```python
# Python/PyTorch é«˜çº§API
import torch

# åˆ›å»ºå¼ é‡
x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
y = torch.randn(2, 3, requires_grad=True)

# å¼ é‡æ“ä½œ
z = x @ y.T  # çŸ©é˜µä¹˜æ³•
z.backward()  # è‡ªåŠ¨å¾®åˆ†
```

```cpp
// C++åº•å±‚å®ç°
class Tensor {
public:
    // æ•°æ®å­˜å‚¨
    std::vector<float> data;
    std::vector<size_t> shape;
    
    // æ¢¯åº¦ä¿¡æ¯
    std::vector<float> grad;
    bool requires_grad;
    
    // æ„é€ å‡½æ•°
    Tensor(const std::vector<size_t>& shape_vec, bool requires_grad = false) {
        shape = shape_vec;
        requires_grad = requires_grad;
        
        // åˆ†é…å†…å­˜
        size_t total_size = 1;
        for (size_t dim : shape) total_size *= dim;
        data.resize(total_size, 0.0f);
        
        if (requires_grad) {
            grad.resize(total_size, 0.0f);
        }
    }
    
    // çŸ©é˜µä¹˜æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
    Tensor matmul(const Tensor& other) {
        // å®ç°çŸ©é˜µä¹˜æ³•
        Tensor result({shape[0], other.shape[1]});
        
        for (size_t i = 0; i < shape[0]; i++) {
            for (size_t j = 0; j < other.shape[1]; j++) {
                float sum = 0.0f;
                for (size_t k = 0; k < shape[1]; k++) {
                    size_t idx1 = i * shape[1] + k;
                    size_t idx2 = k * other.shape[1] + j;
                    sum += data[idx1] * other.data[idx2];
                }
                result.data[i * result.shape[1] + j] = sum;
            }
        }
        
        return result;
    }
};
```

### 1.2 PyTorchå¼ é‡æ“ä½œä¸åº•å±‚C++çš„æ˜ å°„

```python
# PyTorchå¼ é‡æ“ä½œ
class PyTorchOperations:
    """PyTorché«˜çº§APIæ“ä½œç¤ºä¾‹"""
    
    @staticmethod
    def tensor_operations():
        # 1. åˆ›å»ºå’Œåˆå§‹åŒ–
        x = torch.zeros(2, 3)          # å¯¹åº”C++: Tensor({2, 3}, dataå…¨0)
        y = torch.ones(2, 3)           # å¯¹åº”C++: Tensor({2, 3}, dataå…¨1)
        z = torch.randn(2, 3)          # å¯¹åº”C++: Tensor({2, 3}, éšæœºåˆå§‹åŒ–)
        
        # 2. å½¢çŠ¶æ“ä½œ
        x_reshaped = x.view(3, 2)      # å¯¹åº”C++: x.reshape({3, 2})
        x_transposed = x.t()           # å¯¹åº”C++: x.transpose()
        x_flattened = x.flatten()      # å¯¹åº”C++: x.reshape({x.numel()})
        
        # 3. æ•°å­¦è¿ç®—
        add_result = x + y             # å¯¹åº”C++: add_tensors(x, y)
        mul_result = x * y             # å¯¹åº”C++: elementwise_mul(x, y)
        matmul_result = x @ y.T        # å¯¹åº”C++: x.matmul(y.transpose())
        
        # 4. å½’çº¦æ“ä½œ
        sum_all = x.sum()              # å¯¹åº”C++: x.sum_all()
        sum_dim0 = x.sum(dim=0)        # å¯¹åº”C++: x.sum_along_dim(0)
        mean_val = x.mean()            # å¯¹åº”C++: x.mean()
        
        return locals()
```

```cpp
// C++åº•å±‚å®ç°
class TensorOperations {
public:
    // 1. åˆ›å»ºå’Œåˆå§‹åŒ–
    static Tensor zeros(const std::vector<size_t>& shape) {
        Tensor tensor(shape);
        std::fill(tensor.data.begin(), tensor.data.end(), 0.0f);
        return tensor;
    }
    
    static Tensor ones(const std::vector<size_t>& shape) {
        Tensor tensor(shape);
        std::fill(tensor.data.begin(), tensor.data.end(), 1.0f);
        return tensor;
    }
    
    static Tensor randn(const std::vector<size_t>& shape) {
        Tensor tensor(shape);
        std::random_device rd;
        std::mt19937 gen(rd());
        std::normal_distribution<float> dist(0.0f, 1.0f);
        
        for (float& val : tensor.data) {
            val = dist(gen);
        }
        return tensor;
    }
    
    // 2. å½¢çŠ¶æ“ä½œ
    static Tensor reshape(const Tensor& x, const std::vector<size_t>& new_shape) {
        size_t total_elements = 1;
        for (size_t dim : new_shape) total_elements *= dim;
        
        if (total_elements != x.numel()) {
            throw std::runtime_error("Reshape dimensions mismatch");
        }
        
        Tensor result = x;  // æµ…æ‹·è´æ•°æ®
        result.shape = new_shape;
        return result;
    }
    
    static Tensor transpose(const Tensor& x) {
        // 2Dè½¬ç½®
        if (x.shape.size() != 2) {
            throw std::runtime_error("Transpose requires 2D tensor");
        }
        
        Tensor result({x.shape[1], x.shape[0]});
        
        for (size_t i = 0; i < x.shape[0]; i++) {
            for (size_t j = 0; j < x.shape[1]; j++) {
                size_t src_idx = i * x.shape[1] + j;
                size_t dst_idx = j * x.shape[0] + i;
                result.data[dst_idx] = x.data[src_idx];
            }
        }
        
        return result;
    }
    
    // 3. æ•°å­¦è¿ç®—
    static Tensor add(const Tensor& a, const Tensor& b) {
        if (a.shape != b.shape) {
            throw std::runtime_error("Tensor shapes must match for addition");
        }
        
        Tensor result(a.shape);
        for (size_t i = 0; i < a.data.size(); i++) {
            result.data[i] = a.data[i] + b.data[i];
        }
        return result;
    }
    
    static Tensor elementwise_mul(const Tensor& a, const Tensor& b) {
        if (a.shape != b.shape) {
            throw std::runtime_error("Tensor shapes must match for multiplication");
        }
        
        Tensor result(a.shape);
        for (size_t i = 0; i < a.data.size(); i++) {
            result.data[i] = a.data[i] * b.data[i];
        }
        return result;
    }
    
    // 4. å½’çº¦æ“ä½œ
    static float sum_all(const Tensor& x) {
        float total = 0.0f;
        for (float val : x.data) {
            total += val;
        }
        return total;
    }
    
    static Tensor sum_along_dim(const Tensor& x, size_t dim) {
        // æ²¿ç€æŒ‡å®šç»´åº¦æ±‚å’Œ
        std::vector<size_t> new_shape = x.shape;
        new_shape.erase(new_shape.begin() + dim);
        
        Tensor result(new_shape);
        
        // è®¡ç®—ç´¢å¼•æ˜ å°„
        // è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„ç´¢å¼•è®¡ç®—
        return result;
    }
    
    static float mean(const Tensor& x) {
        return sum_all(x) / x.numel();
    }
};
```

## 2. ç¥ç»ç½‘ç»œå±‚æ˜ å°„

### 2.1 ModuleåŸºç±»ï¼šPyTorch vs C++

```python
# PyTorchçš„nn.Module
import torch.nn as nn
import torch.nn.functional as F

class PyTorchLayer(nn.Module):
    """PyTorchä¸­çš„å±‚å®ç°"""
    
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
    def forward(self, x):
        # å‰å‘ä¼ æ’­
        return F.linear(x, self.weight, self.bias)
    
    def parameters(self):
        # è¿”å›æ‰€æœ‰å‚æ•°
        return [self.weight, self.bias]
```

```cpp
// C++ä¸­çš„å±‚åŸºç±»
class Layer {
protected:
    std::vector<Tensor> parameters;
    std::string name;
    bool training_mode;
    
public:
    Layer(const std::string& layer_name = "") 
        : name(layer_name), training_mode(true) {}
    
    virtual ~Layer() = default;
    
    // å‰å‘ä¼ æ’­ - çº¯è™šå‡½æ•°
    virtual Tensor forward(const Tensor& input) = 0;
    
    // åå‘ä¼ æ’­ - ç®€åŒ–ç‰ˆ
    virtual void backward(const Tensor& grad_output) {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè®¡ç®—å‚æ•°çš„æ¢¯åº¦
        for (auto& param : parameters) {
            if (param.requires_grad) {
                // è®¡ç®—å‚æ•°çš„æ¢¯åº¦
                compute_parameter_gradients(param, grad_output);
            }
        }
    }
    
    // è·å–å‚æ•°
    std::vector<Tensor*> get_parameters() {
        std::vector<Tensor*> param_ptrs;
        for (auto& param : parameters) {
            param_ptrs.push_back(&param);
        }
        return param_ptrs;
    }
    
    // è®­ç»ƒ/è¯„ä¼°æ¨¡å¼åˆ‡æ¢
    void train() { training_mode = true; }
    void eval() { training_mode = false; }
    
protected:
    void compute_parameter_gradients(Tensor& param, const Tensor& grad_output) {
        // å‚æ•°æ¢¯åº¦è®¡ç®—çš„ç®€åŒ–å®ç°
        // å®é™…ä¸­éœ€è¦æ ¹æ®å…·ä½“å±‚ç±»å‹å®ç°
    }
};
```

### 2.2 çº¿æ€§å±‚å®ç°å¯¹æ¯”

```python
# PyTorchçº¿æ€§å±‚
class PyTorchLinear(nn.Module):
    """PyTorchçº¿æ€§å±‚å®ç°"""
    
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # å‚æ•°åˆå§‹åŒ–
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        # Kaimingåˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, input):
        # ä½¿ç”¨torch.matmulå®ç°
        output = input.matmul(self.weight.t())
        if self.bias is not None:
            output += self.bias
        return output
    
    def extra_repr(self):
        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'
```

```cpp
// C++çº¿æ€§å±‚å®ç°
class LinearLayer : public Layer {
private:
    size_t in_features;
    size_t out_features;
    Tensor weight;
    Tensor bias;
    bool use_bias;
    
public:
    LinearLayer(size_t in_dim, size_t out_dim, bool has_bias = true)
        : Layer("Linear"), 
          in_features(in_dim), 
          out_features(out_dim),
          use_bias(has_bias) {
        
        // åˆå§‹åŒ–æƒé‡
        weight = Tensor({out_features, in_features}, true);
        initialize_weights(weight);
        parameters.push_back(weight);
        
        // åˆå§‹åŒ–åç½®
        if (use_bias) {
            bias = Tensor({out_features}, true);
            std::fill(bias.data.begin(), bias.data.end(), 0.0f);
            parameters.push_back(bias);
        }
    }
    
    void initialize_weights(Tensor& w) {
        // Kaiming/Heåˆå§‹åŒ–
        float gain = sqrt(2.0f);  // ReLUçš„æ¨èå¢ç›Š
        float std = gain * sqrt(2.0f / (in_features + out_features));
        
        std::normal_distribution<float> dist(0.0f, std);
        std::random_device rd;
        std::mt19937 gen(rd());
        
        for (float& val : w.data) {
            val = dist(gen);
        }
    }
    
    Tensor forward(const Tensor& input) override {
        // input shape: [batch_size, seq_len, in_features] æˆ– [batch_size, in_features]
        // weight shape: [out_features, in_features]
        
        size_t batch_size = input.shape[0];
        size_t seq_len = (input.shape.size() > 2) ? input.shape[1] : 1;
        
        // è®¡ç®—è¾“å‡ºå½¢çŠ¶
        std::vector<size_t> output_shape;
        if (input.shape.size() == 3) {
            output_shape = {batch_size, seq_len, out_features};
        } else {
            output_shape = {batch_size, out_features};
        }
        
        Tensor output(output_shape);
        
        // çŸ©é˜µä¹˜æ³•: input * weight^T
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t o = 0; o < out_features; o++) {
                    float sum = 0.0f;
                    
                    for (size_t i = 0; i < in_features; i++) {
                        // è·å–è¾“å…¥å€¼
                        float input_val;
                        if (input.shape.size() == 3) {
                            input_val = input[{b, s, i}];
                        } else {
                            input_val = input[{b, i}];
                        }
                        
                        // è·å–æƒé‡å€¼
                        float weight_val = weight[{o, i}];
                        
                        sum += input_val * weight_val;
                    }
                    
                    // æ·»åŠ åç½®
                    if (use_bias) {
                        sum += bias.data[o];
                    }
                    
                    // å­˜å‚¨è¾“å‡º
                    if (output_shape.size() == 3) {
                        output[{b, s, o}] = sum;
                    } else {
                        output[{b, o}] = sum;
                    }
                }
            }
        }
        
        return output;
    }
    
    void backward(const Tensor& grad_output) override {
        // è®¡ç®—æƒé‡çš„æ¢¯åº¦
        if (weight.requires_grad) {
            // dL/dW = X^T * dL/dY
            compute_weight_gradients(grad_output);
        }
        
        // è®¡ç®—åç½®çš„æ¢¯åº¦
        if (use_bias && bias.requires_grad) {
            compute_bias_gradients(grad_output);
        }
    }
    
private:
    void compute_weight_gradients(const Tensor& grad_output) {
        // è¿™é‡Œéœ€è¦ä¿å­˜è¾“å…¥ä»¥è®¡ç®—æ¢¯åº¦
        // å®é™…å®ç°ä¸­ï¼Œforwardæ–¹æ³•éœ€è¦ä¿å­˜è¾“å…¥
    }
    
    void compute_bias_gradients(const Tensor& grad_output) {
        // åç½®çš„æ¢¯åº¦æ˜¯grad_outputæ²¿batchç»´åº¦çš„å’Œ
    }
};
```

## 3. Transformerå±‚æ˜ å°„

### 3.1 å¤šå¤´æ³¨æ„åŠ›å®ç°å¯¹æ¯”

```python
# PyTorchå¤šå¤´æ³¨æ„åŠ›
class PyTorchMultiheadAttention(nn.Module):
    """PyTorché£æ ¼çš„å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        # çº¿æ€§å˜æ¢å±‚
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, key_padding_mask=None, 
                need_weights=True, attn_mask=None):
        # è¾“å…¥å½¢çŠ¶: [seq_len, batch_size, embed_dim] æˆ– [batch_size, seq_len, embed_dim]
        
        batch_size = query.size(1) if query.dim() == 3 else query.size(0)
        tgt_len = query.size(0) if query.dim() == 3 else query.size(1)
        src_len = key.size(0) if key.dim() == 3 else key.size(1)
        
        # çº¿æ€§æŠ•å½±
        q = self.q_proj(query)
        k = self.k_proj(key)
        v = self.v_proj(value)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = self._reshape_to_heads(q, batch_size, tgt_len)
        k = self._reshape_to_heads(k, batch_size, src_len)
        v = self._reshape_to_heads(v, batch_size, src_len)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_output_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨æ©ç 
        if attn_mask is not None:
            attn_output_weights += attn_mask
            
        if key_padding_mask is not None:
            attn_output_weights = attn_output_weights.masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(2),
                float('-inf')
            )
        
        # Softmax
        attn_output_weights = F.softmax(attn_output_weights, dim=-1)
        attn_output_weights = self.dropout(attn_output_weights)
        
        # æ³¨æ„åŠ›åŠ æƒ
        attn_output = torch.matmul(attn_output_weights, v)
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        attn_output = self._reshape_from_heads(attn_output, batch_size, tgt_len)
        
        # è¾“å‡ºæŠ•å½±
        attn_output = self.out_proj(attn_output)
        
        if need_weights:
            return attn_output, attn_output_weights
        else:
            return attn_output
    
    def _reshape_to_heads(self, x, batch_size, seq_len):
        # é‡å¡‘ä¸º [batch_size, num_heads, seq_len, head_dim]
        if x.dim() == 3:  # [seq_len, batch_size, embed_dim]
            x = x.transpose(0, 1)  # [batch_size, seq_len, embed_dim]
        
        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)
        x = x.transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]
        return x
    
    def _reshape_from_heads(self, x, batch_size, seq_len):
        # ä»å¤šå¤´æ ¼å¼é‡å¡‘å›æ¥
        x = x.transpose(1, 2)  # [batch_size, seq_len, num_heads, head_dim]
        x = x.contiguous().view(batch_size, seq_len, self.embed_dim)
        
        # å¦‚æœéœ€è¦ï¼Œè½¬ç½®å› [seq_len, batch_size, embed_dim]
        # è¿™é‡Œæˆ‘ä»¬ä¿æŒ [batch_size, seq_len, embed_dim]
        return x
```

```cpp
// C++å¤šå¤´æ³¨æ„åŠ›å®ç°
class MultiHeadAttention : public Layer {
private:
    size_t d_model;
    size_t num_heads;
    size_t d_k;
    size_t d_v;
    
    LinearLayer W_q;
    LinearLayer W_k;
    LinearLayer W_v;
    LinearLayer W_o;
    
    float dropout_rate;
    Tensor dropout_mask;
    
public:
    MultiHeadAttention(size_t model_dim, size_t n_heads, float dropout = 0.1f)
        : Layer("MultiHeadAttention"),
          d_model(model_dim),
          num_heads(n_heads),
          d_k(model_dim / n_heads),
          d_v(model_dim / n_heads),
          W_q(model_dim, model_dim, true),
          W_k(model_dim, model_dim, true),
          W_v(model_dim, model_dim, true),
          W_o(model_dim, model_dim, true),
          dropout_rate(dropout) {
        
        // éªŒè¯ç»´åº¦
        if (d_k * num_heads != model_dim) {
            throw std::runtime_error("model_dim must be divisible by num_heads");
        }
        
        // æ”¶é›†æ‰€æœ‰å‚æ•°
        auto q_params = W_q.get_parameters();
        auto k_params = W_k.get_parameters();
        auto v_params = W_v.get_parameters();
        auto o_params = W_o.get_parameters();
        
        // æ·»åŠ åˆ°å‚æ•°åˆ—è¡¨
        parameters.insert(parameters.end(), q_params.begin(), q_params.end());
        parameters.insert(parameters.end(), k_params.begin(), k_params.end());
        parameters.insert(parameters.end(), v_params.begin(), v_params.end());
        parameters.insert(parameters.end(), o_params.begin(), o_params.end());
    }
    
    Tensor forward(const Tensor& x, const Tensor& mask = Tensor()) override {
        // x shape: [batch_size, seq_len, d_model]
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        
        // 1. çº¿æ€§æŠ•å½±
        Tensor Q = W_q.forward(x);  // [batch, seq, d_model]
        Tensor K = W_k.forward(x);
        Tensor V = W_v.forward(x);
        
        // 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Tensor Q_heads = reshape_to_heads(Q, batch_size, seq_len);  // [batch, heads, seq, d_k]
        Tensor K_heads = reshape_to_heads(K, batch_size, seq_len);
        Tensor V_heads = reshape_to_heads(V, batch_size, seq_len);
        
        // 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        Tensor attn_scores = compute_attention_scores(Q_heads, K_heads);
        
        // 4. åº”ç”¨æ©ç 
        if (mask.numel() > 0) {
            apply_attention_mask(attn_scores, mask);
        }
        
        // 5. Softmax
        Tensor attn_weights = softmax_attention(attn_scores);
        
        // 6. Dropoutï¼ˆè®­ç»ƒæ—¶ï¼‰
        if (training_mode && dropout_rate > 0.0f) {
            attn_weights = apply_dropout(attn_weights, dropout_rate);
        }
        
        // 7. æ³¨æ„åŠ›åŠ æƒ
        Tensor attn_output = apply_attention(attn_weights, V_heads);
        
        // 8. é‡å¡‘å›åŸå§‹å½¢çŠ¶
        Tensor output = reshape_from_heads(attn_output, batch_size, seq_len);
        
        // 9. è¾“å‡ºæŠ•å½±
        output = W_o.forward(output);
        
        return output;
    }
    
private:
    Tensor reshape_to_heads(const Tensor& x, size_t batch_size, size_t seq_len) {
        // ä» [batch, seq, d_model] é‡å¡‘ä¸º [batch, heads, seq, d_k]
        Tensor heads({batch_size, num_heads, seq_len, d_k});
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t s = 0; s < seq_len; s++) {
                    for (size_t d = 0; d < d_k; d++) {
                        size_t src_idx = h * d_k + d;
                        heads[{b, h, s, d}] = x[{b, s, src_idx}];
                    }
                }
            }
        }
        
        return heads;
    }
    
    Tensor compute_attention_scores(const Tensor& Q, const Tensor& K) {
        // Q, K shape: [batch, heads, seq_q, d_k]
        size_t batch_size = Q.shape[0];
        size_t num_heads = Q.shape[1];
        size_t seq_len_q = Q.shape[2];
        size_t seq_len_k = K.shape[2];
        
        Tensor scores({batch_size, num_heads, seq_len_q, seq_len_k});
        
        float scale_factor = 1.0f / sqrt(static_cast<float>(d_k));
        
        // è®¡ç®—ç¼©æ”¾ç‚¹ç§¯
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t j = 0; j < seq_len_k; j++) {
                        float score = 0.0f;
                        
                        for (size_t k = 0; k < d_k; k++) {
                            score += Q[{b, h, i, k}] * K[{b, h, j, k}];
                        }
                        
                        scores[{b, h, i, j}] = score * scale_factor;
                    }
                }
            }
        }
        
        return scores;
    }
    
    void apply_attention_mask(Tensor& scores, const Tensor& mask) {
        // åº”ç”¨å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ï¼‰æˆ–å…¶ä»–æ©ç 
        size_t batch_size = scores.shape[0];
        size_t num_heads = scores.shape[1];
        size_t seq_len_q = scores.shape[2];
        size_t seq_len_k = scores.shape[3];
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t j = 0; j < seq_len_k; j++) {
                        // å› æœæ©ç ï¼šj > i çš„ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·
                        if (j > i) {
                            scores[{b, h, i, j}] = -1e9f;
                        }
                        
                        // åº”ç”¨è‡ªå®šä¹‰æ©ç 
                        if (mask.numel() > 0) {
                            if (mask.shape.size() == 2) {
                                if (mask[{i, j}] == 0.0f) {
                                    scores[{b, h, i, j}] = -1e9f;
                                }
                            } else if (mask.shape.size() == 3) {
                                if (mask[{b, i, j}] == 0.0f) {
                                    scores[{b, h, i, j}] = -1e9f;
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    Tensor softmax_attention(const Tensor& scores) {
        // æ²¿æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œsoftmax
        size_t batch_size = scores.shape[0];
        size_t num_heads = scores.shape[1];
        size_t seq_len_q = scores.shape[2];
        size_t seq_len_k = scores.shape[3];
        
        Tensor weights(scores.shape);
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    // æ‰¾åˆ°æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
                    float max_val = -std::numeric_limits<float>::max();
                    for (size_t j = 0; j < seq_len_k; j++) {
                        max_val = std::max(max_val, scores[{b, h, i, j}]);
                    }
                    
                    // è®¡ç®—æŒ‡æ•°å’Œ
                    float sum_exp = 0.0f;
                    for (size_t j = 0; j < seq_len_k; j++) {
                        sum_exp += exp(scores[{b, h, i, j}] - max_val);
                    }
                    
                    // è®¡ç®—softmax
                    for (size_t j = 0; j < seq_len_k; j++) {
                        float exp_val = exp(scores[{b, h, i, j}] - max_val);
                        weights[{b, h, i, j}] = exp_val / sum_exp;
                    }
                }
            }
        }
        
        return weights;
    }
    
    Tensor apply_dropout(const Tensor& x, float dropout_rate) {
        if (!training_mode || dropout_rate <= 0.0f) {
            return x;
        }
        
        Tensor result = x;
        dropout_mask = Tensor(x.shape);
        
        // ç”Ÿæˆdropoutæ©ç 
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_real_distribution<float> dist(0.0f, 1.0f);
        
        float scale = 1.0f / (1.0f - dropout_rate);
        
        for (size_t i = 0; i < x.numel(); i++) {
            if (dist(gen) > dropout_rate) {
                dropout_mask.data[i] = scale;
                result.data[i] *= scale;
            } else {
                dropout_mask.data[i] = 0.0f;
                result.data[i] = 0.0f;
            }
        }
        
        return result;
    }
    
    Tensor apply_attention(const Tensor& weights, const Tensor& V) {
        // weights: [batch, heads, seq_q, seq_k]
        // V: [batch, heads, seq_k, d_v]
        
        size_t batch_size = weights.shape[0];
        size_t num_heads = weights.shape[1];
        size_t seq_len_q = weights.shape[2];
        size_t seq_len_k = weights.shape[3];
        size_t d_v = V.shape[3];
        
        Tensor output({batch_size, num_heads, seq_len_q, d_v});
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t d = 0; d < d_v; d++) {
                        float sum = 0.0f;
                        
                        for (size_t j = 0; j < seq_len_k; j++) {
                            sum += weights[{b, h, i, j}] * V[{b, h, j, d}];
                        }
                        
                        output[{b, h, i, d}] = sum;
                    }
                }
            }
        }
        
        return output;
    }
    
    Tensor reshape_from_heads(const Tensor& x, size_t batch_size, size_t seq_len) {
        // ä» [batch, heads, seq, d_k] é‡å¡‘ä¸º [batch, seq, d_model]
        Tensor output({batch_size, seq_len, d_model});
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t h = 0; h < num_heads; h++) {
                    for (size_t d = 0; d < d_k; d++) {
                        size_t dst_idx = h * d_k + d;
                        output[{b, s, dst_idx}] = x[{b, h, s, d}];
                    }
                }
            }
        }
        
        return output;
    }
};
```

## 4. è®­ç»ƒæµç¨‹æ˜ å°„

### 4.1 è®­ç»ƒå¾ªç¯å¯¹æ¯”

```python
# PyTorchè®­ç»ƒå¾ªç¯
class PyTorchTrainer:
    """PyTorché£æ ¼è®­ç»ƒå™¨"""
    
    def __init__(self, model, train_loader, val_loader, optimizer, 
                 scheduler=None, device='cuda'):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        
        self.model.to(device)
        self.criterion = nn.CrossEntropyLoss()
        
    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        total_samples = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(pbar):
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # æ¢¯åº¦æ¸…é›¶
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            logits = outputs.logits
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.optimizer.step()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•æŸå¤±
            batch_size = input_ids.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': loss.item(),
                'lr': self.optimizer.param_groups[0]['lr']
            })
        
        avg_loss = total_loss / total_samples
        return avg_loss
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                batch_size = input_ids.size(0)
                
                total_loss += loss.item() * batch_size
                total_samples += batch_size
        
        avg_loss = total_loss / total_samples
        return avg_loss
    
    def train(self, num_epochs):
        for epoch in range(1, num_epochs + 1):
            train_loss = self.train_epoch(epoch)
            val_loss = self.validate()
            
            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, val_loss)
```

```cpp
// C++è®­ç»ƒå¾ªç¯å®ç°
class CPUTrainer {
private:
    GPTModel& model;
    DataPipeline& train_data;
    DataPipeline& val_data;
    AdamOptimizer optimizer;
    CrossEntropyLoss criterion;
    GradientClipper clipper;
    LearningRateScheduler scheduler;
    
    size_t num_epochs;
    size_t current_epoch;
    
public:
    CPUTrainer(GPTModel& m, DataPipeline& train, DataPipeline& val,
               size_t epochs = 10)
        : model(m), train_data(train), val_data(val),
          optimizer(1e-4, 0.9, 0.999, 1e-8),
          clipper(1.0f),
          scheduler(LearningRateScheduler::COSINE_ANNEALING, 1e-4, 1000),
          num_epochs(epochs), current_epoch(0) {
        
        // æ³¨å†Œæ¨¡å‹å‚æ•°åˆ°ä¼˜åŒ–å™¨
        register_parameters();
    }
    
    void register_parameters() {
        // è·å–æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        auto params = model.get_parameters();
        
        // æ³¨å†Œåˆ°ä¼˜åŒ–å™¨
        for (auto& param : params) {
            if (param->requires_grad) {
                optimizer.add_parameter(param->data.data(), param->grad.data(), param->numel());
            }
        }
    }
    
    float train_epoch() {
        model.train();  // è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        float total_loss = 0.0f;
        size_t total_batches = 0;
        
        auto start_time = std::chrono::high_resolution_clock::now();
        
        while (true) {
            try {
                // è·å–è®­ç»ƒæ‰¹æ¬¡
                TrainingBatch batch = train_data.get_batch();
                
                // å‰å‘ä¼ æ’­
                auto [logits, loss] = model.forward_with_loss(
                    batch.input_ids, 
                    batch.attention_mask, 
                    batch.labels
                );
                
                // è®°å½•æŸå¤±
                total_loss += loss;
                total_batches++;
                
                // æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                compute_gradients(logits, batch.labels);
                
                // æ¢¯åº¦è£å‰ª
                clipper.clip(model.get_parameters());
                
                // æ›´æ–°å­¦ä¹ ç‡
                float lr = scheduler.get_lr();
                optimizer.set_learning_rate(lr);
                
                // ä¼˜åŒ–å™¨æ­¥éª¤
                optimizer.step();
                optimizer.zero_grad();
                
                // æ‰“å°è¿›åº¦
                if (total_batches % 100 == 0) {
                    auto current_time = std::chrono::high_resolution_clock::now();
                    auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(
                        current_time - start_time).count();
                    
                    float avg_loss = total_loss / total_batches;
                    
                    std::cout << "Batch " << total_batches 
                              << ", Loss: " << avg_loss
                              << ", LR: " << lr
                              << ", Time: " << elapsed << "ms"
                              << std::endl;
                }
                
            } catch (const std::exception& e) {
                // æ•°æ®è€—å°½æˆ–é”™è¯¯
                break;
            }
        }
        
        float avg_loss = total_loss / total_batches;
        return avg_loss;
    }
    
    float validate() {
        model.eval();  // è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        float total_loss = 0.0f;
        size_t total_batches = 0;
        
        for (size_t i = 0; i < 100; i++) {  // åªéªŒè¯å°‘é‡æ‰¹æ¬¡
            try {
                TrainingBatch batch = val_data.get_batch();
                
                // å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
                auto [logits, loss] = model.forward_with_loss(
                    batch.input_ids,
                    batch.attention_mask,
                    batch.labels
                );
                
                total_loss += loss;
                total_batches++;
                
            } catch (const std::exception& e) {
                break;
            }
        }
        
        float avg_loss = total_loss / total_batches;
        return avg_loss;
    }
    
    void train() {
        std::cout << "å¼€å§‹è®­ç»ƒ..." << std::endl;
        std::cout << "æ€»è½®æ•°: " << num_epochs << std::endl;
        
        for (current_epoch = 1; current_epoch <= num_epochs; current_epoch++) {
            std::cout << "\nEpoch " << current_epoch << "/" << num_epochs << std::endl;
            std::cout << "----------------------------------------" << std::endl;
            
            // è®­ç»ƒä¸€ä¸ªepoch
            float train_loss = train_epoch();
            
            // éªŒè¯
            float val_loss = validate();
            
            std::cout << "\nEpoch " << current_epoch << " å®Œæˆ" << std::endl;
            std::cout << "è®­ç»ƒæŸå¤±: " << train_loss << std::endl;
            std::cout << "éªŒè¯æŸå¤±: " << val_loss << std::endl;
            
            // ä¿å­˜æ£€æŸ¥ç‚¹
            save_checkpoint(current_epoch, val_loss);
        }
        
        std::cout << "\nè®­ç»ƒå®Œæˆ!" << std::endl;
    }
    
private:
    void compute_gradients(const Tensor& logits, const Tensor& targets) {
        // ç®€åŒ–ç‰ˆçš„æ¢¯åº¦è®¡ç®—
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™éœ€è¦å®Œæ•´çš„åå‘ä¼ æ’­
        
        // å¯¹äºäº¤å‰ç†µæŸå¤±ï¼Œæ¢¯åº¦æ˜¯é¢„æµ‹æ¦‚ç‡å‡å»çœŸå®æ ‡ç­¾çš„one-hot
        size_t batch_size = logits.shape[0];
        size_t seq_len = logits.shape[1];
        size_t vocab_size = logits.shape[2];
        
        // è·å–æ¨¡å‹å‚æ•°çš„æ¢¯åº¦å¼ é‡
        auto params = model.get_parameters();
        
        // è¿™é‡Œæˆ‘ä»¬åªæ˜¯æ¼”ç¤ºï¼Œå®é™…å®ç°éœ€è¦è®¡ç®—æ¯ä¸ªå‚æ•°çš„ç²¾ç¡®æ¢¯åº¦
        for (auto& param : params) {
            if (param->requires_grad) {
                // ç®€åŒ–ï¼šéšæœºæ¢¯åº¦ï¼ˆå®é™…ä¸­éœ€è¦ç²¾ç¡®è®¡ç®—ï¼‰
                for (size_t i = 0; i < param->grad.size(); i++) {
                    param->grad[i] = (rand() / (float)RAND_MAX - 0.5f) * 0.01f;
                }
            }
        }
    }
    
    void save_checkpoint(size_t epoch, float val_loss) {
        std::string filename = "checkpoint_epoch_" + std::to_string(epoch) + ".bin";
        
        // ä¿å­˜æ¨¡å‹
        model.save(filename);
        
        // ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer.save("optimizer_state.bin");
        
        // ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
        std::ofstream meta("checkpoint_meta.json");
        meta << "{"
             << "\"epoch\": " << epoch << ", "
             << "\"val_loss\": " << val_loss << ", "
             << "\"learning_rate\": " << optimizer.get_learning_rate() << ", "
             << "\"timestamp\": \"" << get_current_time() << "\""
             << "}" << std::endl;
        meta.close();
        
        std::cout << "æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: " << filename << std::endl;
    }
    
    std::string get_current_time() {
        auto now = std::chrono::system_clock::now();
        auto in_time_t = std::chrono::system_clock::to_time_t(now);
        
        std::stringstream ss;
        ss << std::put_time(std::localtime(&in_time_t), "%Y-%m-%d %X");
        return ss.str();
    }
};
```

## 5. è‡ªåŠ¨å¾®åˆ†æ˜ å°„

### 5.1 è®¡ç®—å›¾å¯¹æ¯”

```python
# PyTorchè‡ªåŠ¨å¾®åˆ†
class PyTorchAutograd:
    """PyTorchè‡ªåŠ¨å¾®åˆ†ç¤ºä¾‹"""
    
    @staticmethod
    def autograd_example():
        import torch
        
        # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
        x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
        y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)
        
        # è¿›è¡Œè®¡ç®—ï¼ˆPyTorchä¼šè®°å½•è®¡ç®—å›¾ï¼‰
        z = x * y  # element-wise multiplication
        w = z.sum()  # æ ‡é‡
        
        # åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼‰
        w.backward()
        
        # æŸ¥çœ‹æ¢¯åº¦
        print(f"x.grad: {x.grad}")  # åº”è¯¥æ˜¯ y = [4, 5, 6]
        print(f"y.grad: {y.grad}")  # åº”è¯¥æ˜¯ x = [1, 2, 3]
        
        # è®¡ç®—å›¾ç»†èŠ‚
        print(f"w.grad_fn: {w.grad_fn}")
        print(f"z.grad_fn: {z.grad_fn}")
        
        return locals()
    
    @staticmethod
    def custom_autograd():
        """è‡ªå®šä¹‰è‡ªåŠ¨å¾®åˆ†å‡½æ•°"""
        class CustomFunction(torch.autograd.Function):
            @staticmethod
            def forward(ctx, x):
                # ä¿å­˜å‰å‘ä¼ æ’­ä¸­éœ€è¦çš„ä¿¡æ¯
                ctx.save_for_backward(x)
                return x * x
            
            @staticmethod
            def backward(ctx, grad_output):
                # è®¡ç®—æ¢¯åº¦
                x, = ctx.saved_tensors
                return 2 * x * grad_output
        
        x = torch.tensor(3.0, requires_grad=True)
        y = CustomFunction.apply(x)
        
        y.backward()
        print(f"x = {x}, y = x^2 = {y}, dy/dx = {x.grad}")
        
        return locals()
```

```cpp
// C++æ‰‹åŠ¨åå‘ä¼ æ’­
class ManualAutograd {
public:
    // ç®€å•çš„è®¡ç®—èŠ‚ç‚¹
    struct ComputeNode {
        std::vector<Tensor*> inputs;
        Tensor* output;
        std::function<void()> backward_func;
        
        ComputeNode(const std::vector<Tensor*>& in, Tensor* out, 
                   std::function<void()> backward)
            : inputs(in), output(out), backward_func(backward) {}
        
        void backward() {
            if (backward_func) {
                backward_func();
            }
        }
    };
    
    // è®¡ç®—å›¾
    class ComputationGraph {
    private:
        std::vector<ComputeNode> nodes;
        Tensor* loss_tensor;
        
    public:
        void add_node(const ComputeNode& node) {
            nodes.push_back(node);
        }
        
        void set_loss(Tensor* loss) {
            loss_tensor = loss;
        }
        
        void backward() {
            // åˆå§‹åŒ–æŸå¤±æ¢¯åº¦ä¸º1
            if (loss_tensor && loss_tensor->requires_grad) {
                std::fill(loss_tensor->grad.begin(), loss_tensor->grad.end(), 1.0f);
            }
            
            // åå‘éå†è®¡ç®—å›¾
            for (auto it = nodes.rbegin(); it != nodes.rend(); ++it) {
                it->backward();
            }
        }
    };
    
    // è‡ªå®šä¹‰æ“ä½œï¼šå…ƒç´ ä¹˜æ³•
    static Tensor* elementwise_multiply(Tensor* a, Tensor* b, ComputationGraph& graph) {
        // å‰å‘ä¼ æ’­
        Tensor* result = new Tensor(a->shape, true);
        
        for (size_t i = 0; i < a->data.size(); i++) {
            result->data[i] = a->data[i] * b->data[i];
        }
        
        // åˆ›å»ºè®¡ç®—èŠ‚ç‚¹
        ComputeNode node(
            {a, b}, result,
            [a, b, result]() {
                // åå‘ä¼ æ’­ï¼šda = grad_output * b
                //          db = grad_output * a
                for (size_t i = 0; i < a->grad.size(); i++) {
                    a->grad[i] += result->grad[i] * b->data[i];
                    b->grad[i] += result->grad[i] * a->data[i];
                }
            }
        );
        
        graph.add_node(node);
        return result;
    }
    
    // è‡ªå®šä¹‰æ“ä½œï¼šæ±‚å’Œ
    static Tensor* sum_all(Tensor* x, ComputationGraph& graph) {
        // å‰å‘ä¼ æ’­
        Tensor* result = new Tensor({1}, true);
        
        float sum = 0.0f;
        for (float val : x->data) {
            sum += val;
        }
        result->data[0] = sum;
        
        // åˆ›å»ºè®¡ç®—èŠ‚ç‚¹
        ComputeNode node(
            {x}, result,
            [x, result]() {
                // åå‘ä¼ æ’­ï¼šdx = grad_output (å¹¿æ’­åˆ°æ‰€æœ‰å…ƒç´ )
                float grad = result->grad[0];
                for (size_t i = 0; i < x->grad.size(); i++) {
                    x->grad[i] += grad;
                }
            }
        );
        
        graph.add_node(node);
        return result;
    }
    
    static void run_example() {
        std::cout << "C++æ‰‹åŠ¨åå‘ä¼ æ’­ç¤ºä¾‹" << std::endl;
        
        // åˆ›å»ºè¾“å…¥å¼ é‡
        Tensor x({3}, true);
        Tensor y({3}, true);
        
        x.data = {1.0f, 2.0f, 3.0f};
        y.data = {4.0f, 5.0f, 6.0f};
        
        // åˆ›å»ºè®¡ç®—å›¾
        ComputationGraph graph;
        
        // å‰å‘ä¼ æ’­
        Tensor* z = elementwise_multiply(&x, &y, graph);  // z = x * y
        Tensor* w = sum_all(z, graph);                     // w = sum(z)
        
        // è®¾ç½®æŸå¤±
        graph.set_loss(w);
        
        // åå‘ä¼ æ’­
        graph.backward();
        
        // æ‰“å°ç»“æœ
        std::cout << "x = [" << x.data[0] << ", " << x.data[1] << ", " << x.data[2] << "]" << std::endl;
        std::cout << "y = [" << y.data[0] << ", " << y.data[1] << ", " << y.data[2] << "]" << std::endl;
        std::cout << "z = x * y = [" << z->data[0] << ", " << z->data[1] << ", " << z->data[2] << "]" << std::endl;
        std::cout << "w = sum(z) = " << w->data[0] << std::endl;
        std::cout << std::endl;
        std::cout << "æ¢¯åº¦:" << std::endl;
        std::cout << "âˆ‚w/âˆ‚x = [" << x.grad[0] << ", " << x.grad[1] << ", " << x.grad[2] << "] (åº”ä¸º y)" << std::endl;
        std::cout << "âˆ‚w/âˆ‚y = [" << y.grad[0] << ", " << y.grad[1] << ", " << y.grad[2] << "] (åº”ä¸º x)" << std::endl;
        
        // æ¸…ç†
        delete z;
        delete w;
    }
};
```

## 6. å®Œæ•´æ˜ å°„è¡¨

### 6.1 PyTorch â†” C++ ç±»/å‡½æ•°æ˜ å°„è¡¨

| PyTorchç»„ä»¶ | C++å¯¹åº”ç±» | åŠŸèƒ½æè¿° | å…³é”®æ–¹æ³•/å‡½æ•°æ˜ å°„ |
|------------|----------|----------|-----------------|
| `torch.Tensor` | `Tensor` | å¤šç»´æ•°ç»„å­˜å‚¨ | `data`, `shape`, `requires_grad` |
| `torch.nn.Module` | `Layer` | ç¥ç»ç½‘ç»œå±‚åŸºç±» | `forward()`, `parameters()` |
| `torch.nn.Linear` | `LinearLayer` | çº¿æ€§å˜æ¢å±‚ | `forward()`, `backward()` |
| `torch.nn.LayerNorm` | `LayerNorm` | å±‚å½’ä¸€åŒ– | `forward()`, è®¡ç®—mean/var |
| `torch.nn.Dropout` | å†…ç½®åœ¨å±‚ä¸­ | éšæœºå¤±æ´» | `apply_dropout()` |
| `torch.nn.MultiheadAttention` | `MultiHeadAttention` | å¤šå¤´æ³¨æ„åŠ› | `forward()`, è®¡ç®—QKV |
| `torch.nn.TransformerEncoderLayer` | `TransformerBlock` | Transformerç¼–ç å±‚ | `forward()`, æ®‹å·®è¿æ¥ |
| `torch.nn.CrossEntropyLoss` | `CrossEntropyLoss` | äº¤å‰ç†µæŸå¤± | `compute()`, softmax |
| `torch.optim.AdamW` | `AdamOptimizer` | AdamWä¼˜åŒ–å™¨ | `step()`, `zero_grad()` |
| `torch.optim.lr_scheduler` | `LearningRateScheduler` | å­¦ä¹ ç‡è°ƒåº¦ | `get_lr()`, å„ç§è°ƒåº¦ç­–ç•¥ |
| `torch.utils.data.DataLoader` | `DataPipeline` | æ•°æ®åŠ è½½ç®¡é“ | `get_batch()`, æ•°æ®é¢„å¤„ç† |
| `torch.autograd` | `ComputationGraph` | è‡ªåŠ¨å¾®åˆ† | `backward()`, è®¡ç®—å›¾ |
| `torch.nn.utils.clip_grad_norm_` | `GradientClipper` | æ¢¯åº¦è£å‰ª | `clip()`, è®¡ç®—èŒƒæ•° |
| `torch.save()` / `torch.load()` | `save()` / `load()` æ–¹æ³• | æ¨¡å‹ä¿å­˜åŠ è½½ | åºåˆ—åŒ–/ååºåˆ—åŒ– |

### 6.2 è®­ç»ƒå¾ªç¯æ­¥éª¤å¯¹æ¯”è¡¨

```python
# Python/PyTorchè®­ç»ƒæ­¥éª¤
training_steps_pytorch = {
    "1_åˆå§‹åŒ–": """
        # æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°
        model = GPTModel(config).to(device)
        optimizer = AdamW(model.parameters(), lr=lr)
        criterion = CrossEntropyLoss()
        scheduler = get_cosine_schedule_with_warmup(...)
    """,
    
    "2_æ•°æ®åŠ è½½": """
        # åˆ›å»ºDataLoader
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # è¿­ä»£æ‰¹æ¬¡
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
    """,
    
    "3_å‰å‘ä¼ æ’­": """
        # è°ƒç”¨æ¨¡å‹
        outputs = model(input_ids=input_ids, 
                       attention_mask=attention_mask, 
                       labels=labels)
        loss = outputs.loss
        logits = outputs.logits
    """,
    
    "4_åå‘ä¼ æ’­": """
        # PyTorchè‡ªåŠ¨å¾®åˆ†
        loss.backward()  # è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    """,
    
    "5_ä¼˜åŒ–å™¨æ›´æ–°": """
        # æ›´æ–°å‚æ•°
        optimizer.step()
        optimizer.zero_grad()
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
    """,
    
    "6_æ—¥å¿—è®°å½•": """
        # ä½¿ç”¨tensorboardæˆ–æ‰“å°
        writer.add_scalar('Loss/train', loss.item(), global_step)
        print(f"Step {global_step}: Loss = {loss.item():.4f}")
    """
}
```

```cpp
// C++è®­ç»ƒæ­¥éª¤å¯¹åº”
training_steps_cpp = {
    "1_åˆå§‹åŒ–": """
        // åˆ›å»ºæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°
        GPTModel model(config);
        AdamOptimizer optimizer(learning_rate, beta1, beta2, epsilon);
        CrossEntropyLoss criterion;
        LearningRateScheduler scheduler(COSINE_ANNEALING, learning_rate);
    """,
    
    "2_æ•°æ®åŠ è½½": """
        // åˆ›å»ºæ•°æ®ç®¡é“
        DataPipeline pipeline(data_dir, tokenizer);
        
        // è·å–æ‰¹æ¬¡
        while (true) {
            try {
                TrainingBatch batch = pipeline.get_batch();
                // ä½¿ç”¨batch.input_ids, batch.attention_mask, batch.labels
            } catch (...) { break; }
        }
    """,
    
    "3_å‰å‘ä¼ æ’­": """
        // æ‰‹åŠ¨è°ƒç”¨æ¨¡å‹
        Tensor logits = model.forward(batch.input_ids, batch.attention_mask);
        float loss = criterion.compute(logits, batch.labels);
    """,
    
    "4_åå‘ä¼ æ’­": """
        // æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆéœ€è¦ä¸ºæ¯ä¸ªå±‚å®ç°backwardï¼‰
        model.backward(logits, batch.labels);
        
        // æ¢¯åº¦è£å‰ª
        GradientClipper clipper(1.0f);
        clipper.clip(model.get_parameters());
    """,
    
    "5_ä¼˜åŒ–å™¨æ›´æ–°": """
        // æ›´æ–°å‚æ•°
        optimizer.step();
        optimizer.zero_grad();
        
        // æ›´æ–°å­¦ä¹ ç‡
        float lr = scheduler.get_lr();
        optimizer.set_learning_rate(lr);
    """,
    
    "6_æ—¥å¿—è®°å½•": """
        // æ‰‹åŠ¨è®°å½•æ—¥å¿—
        std::cout << "Step " << step << ": Loss = " << loss << std::endl;
        
        // å†™å…¥æ–‡ä»¶
        std::ofstream log_file("training.log", std::ios::app);
        log_file << step << "," << loss << "," << lr << std::endl;
    """
}
```

### 6.3 å…³é”®å‡½æ•°æ˜ å°„è¯¦æƒ…

```python
# è¯¦ç»†å‡½æ•°æ˜ å°„å¯¹ç…§è¡¨
class FunctionMapping:
    """PyTorchå‡½æ•°ä¸C++å®ç°çš„è¯¦ç»†æ˜ å°„"""
    
    @staticmethod
    def tensor_operations_mapping():
        """å¼ é‡æ“ä½œæ˜ å°„"""
        return {
            "åˆ›å»ºå’Œåˆå§‹åŒ–": {
                "torch.zeros": "TensorOperations::zeros()",
                "torch.ones": "TensorOperations::ones()",
                "torch.randn": "TensorOperations::randn()",
                "torch.tensor": "Tensoræ„é€ å‡½æ•°",
                "torch.arange": "éœ€è¦æ‰‹åŠ¨å®ç°rangeåŠŸèƒ½"
            },
            
            "å½¢çŠ¶æ“ä½œ": {
                "tensor.view()": "TensorOperations::reshape()",
                "tensor.reshape()": "TensorOperations::reshape()",
                "tensor.transpose()": "TensorOperations::transpose()",
                "tensor.permute()": "éœ€è¦å®ç°å¤šç»´åº¦è½¬ç½®",
                "tensor.flatten()": "reshapeä¸º1D"
            },
            
            "æ•°å­¦è¿ç®—": {
                "torch.matmul()": "TensorOperations::matmul()",
                "tensor @ tensor": "Tensor::matmul()æ–¹æ³•",
                "torch.add()": "TensorOperations::add()",
                "torch.mul()": "TensorOperations::elementwise_mul()",
                "torch.sum()": "TensorOperations::sum_all()æˆ–sum_along_dim()",
                "torch.mean()": "TensorOperations::mean()",
                "torch.exp()": "std::exp()å¾ªç¯",
                "torch.log()": "std::log()å¾ªç¯",
                "torch.sqrt()": "std::sqrt()å¾ªç¯"
            },
            
            "æ¿€æ´»å‡½æ•°": {
                "torch.nn.functional.relu": "Activation::relu()",
                "torch.nn.functional.gelu": "Activation::gelu()",
                "torch.nn.functional.softmax": "æ‰‹åŠ¨å®ç°ï¼Œæ²¿ç»´åº¦softmax",
                "torch.sigmoid": "1/(1+exp(-x))"
            }
        }
    
    @staticmethod
    def neural_network_mapping():
        """ç¥ç»ç½‘ç»œå±‚æ˜ å°„"""
        return {
            "åŸºç¡€å±‚": {
                "nn.Linear": "LinearLayerç±»",
                "nn.Embedding": "EmbeddingLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.Dropout": "DropoutLayerç±»æˆ–åœ¨å„å±‚ä¸­å®ç°",
                "nn.LayerNorm": "LayerNormç±»"
            },
            
            "å·ç§¯å±‚": {
                "nn.Conv1d": "Conv1DLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.Conv2d": "Conv2DLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.MaxPool1d": "MaxPoolLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰"
            },
            
            "å¾ªç¯ç¥ç»ç½‘ç»œ": {
                "nn.LSTM": "LSTMLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.GRU": "GRULayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.RNN": "RNNLayerç±»ï¼ˆéœ€è¦å®ç°ï¼‰"
            },
            
            "Transformer": {
                "nn.MultiheadAttention": "MultiHeadAttentionç±»",
                "nn.TransformerEncoder": "TransformerEncoderç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.TransformerDecoder": "TransformerDecoderç±»ï¼ˆéœ€è¦å®ç°ï¼‰"
            },
            
            "æŸå¤±å‡½æ•°": {
                "nn.CrossEntropyLoss": "CrossEntropyLossç±»",
                "nn.MSELoss": "MSELossç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "nn.BCELoss": "BCELossç±»ï¼ˆéœ€è¦å®ç°ï¼‰"
            }
        }
    
    @staticmethod
    def optimizer_mapping():
        """ä¼˜åŒ–å™¨æ˜ å°„"""
        return {
            "åŸºç¡€ä¼˜åŒ–å™¨": {
                "torch.optim.SGD": "SGDç±»",
                "torch.optim.Adam": "Adamç±»ï¼ˆéœ€è¦å®ç°ï¼‰",
                "torch.optim.AdamW": "AdamWç±»",
                "torch.optim.RMSprop": "RMSpropç±»ï¼ˆéœ€è¦å®ç°ï¼‰"
            },
            
            "å­¦ä¹ ç‡è°ƒåº¦å™¨": {
                "torch.optim.lr_scheduler.LambdaLR": "éœ€è¦å®ç°lambdaå‡½æ•°æ”¯æŒ",
                "torch.optim.lr_scheduler.StepLR": "StepDecaySchedulerç±»",
                "torch.optim.lr_scheduler.CosineAnnealingLR": "CosineAnnealingSchedulerç±»",
                "torch.optim.lr_scheduler.ReduceLROnPlateau": "éœ€è¦å®ç°ç›‘æ§æŒ‡æ ‡"
            }
        }
```

## 7. å®é™…ç¤ºä¾‹ï¼šTransformerå±‚å‰å‘ä¼ æ’­å¯¹æ¯”

```python
# PyTorch Transformerå±‚å‰å‘ä¼ æ’­
def pytorch_transformer_forward():
    """PyTorchä¸­Transformerå±‚çš„å‰å‘ä¼ æ’­"""
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    batch_size = 2
    seq_len = 10
    d_model = 512
    num_heads = 8
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    attention_mask = torch.ones(batch_size, seq_len, seq_len)
    
    # PyTorch Transformerå±‚
    transformer_layer = nn.TransformerEncoderLayer(
        d_model=d_model,
        nhead=num_heads,
        dim_feedforward=2048,
        dropout=0.1,
        activation='gelu',
        batch_first=True
    )
    
    # å‰å‘ä¼ æ’­ï¼ˆPyTorchè‡ªåŠ¨å¤„ç†ï¼‰
    output = transformer_layer(x, src_mask=attention_mask)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æŸ¥çœ‹å†…éƒ¨ç»„ä»¶
    print(f"\nTransformerå±‚ç»„ä»¶:")
    print(f"- è‡ªæ³¨æ„åŠ›: {type(transformer_layer.self_attn).__name__}")
    print(f"- å‰é¦ˆç½‘ç»œ: {type(transformer_layer.linear1).__name__} -> {type(transformer_layer.linear2).__name__}")
    print(f"- å±‚å½’ä¸€åŒ–1: {type(transformer_layer.norm1).__name__}")
    print(f"- å±‚å½’ä¸€åŒ–2: {type(transformer_layer.norm2).__name__}")
    
    return output
```

```cpp
// C++ Transformerå±‚å‰å‘ä¼ æ’­
Tensor cpp_transformer_forward() {
    std::cout << "C++ä¸­Transformerå±‚çš„å‰å‘ä¼ æ’­" << std::endl;
    
    size_t batch_size = 2;
    size_t seq_len = 10;
    size_t d_model = 512;
    size_t num_heads = 8;
    
    // åˆ›å»ºè¾“å…¥å¼ é‡
    Tensor x({batch_size, seq_len, d_model});
    Tensor attention_mask({batch_size, seq_len, seq_len});
    
    // å¡«å……éšæœºæ•°æ®
    std::random_device rd;
    std::mt19937 gen(rd());
    std::normal_distribution<float> dist(0.0f, 1.0f);
    
    for (float& val : x.data) {
        val = dist(gen);
    }
    
    for (float& val : attention_mask.data) {
        val = 1.0f;  // å…¨éƒ¨ä¸º1ï¼Œæ— æ©ç 
    }
    
    // åˆ›å»ºTransformerå±‚ç»„ä»¶
    MultiHeadAttention attention(d_model, num_heads, 0.1f);
    LayerNorm norm1(d_model);
    FeedForward ff(d_model, 2048);
    LayerNorm norm2(d_model);
    
    std::cout << "\nTransformerå±‚ç»„ä»¶å·²åˆ›å»º:" << std::endl;
    std::cout << "- å¤šå¤´æ³¨æ„åŠ›: " << attention.get_name() << std::endl;
    std::cout << "- å‰é¦ˆç½‘ç»œ: 2ä¸ªçº¿æ€§å±‚ + GELU" << std::endl;
    std::cout << "- å±‚å½’ä¸€åŒ–: 2ä¸ªå®ä¾‹" << std::endl;
    
    // å‰å‘ä¼ æ’­æ­¥éª¤
    std::cout << "\nå¼€å§‹å‰å‘ä¼ æ’­..." << std::endl;
    
    // 1. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    std::cout << "1. è‡ªæ³¨æ„åŠ›..." << std::endl;
    Tensor attn_output = attention.forward(x, attention_mask);
    
    std::cout << "2. æ®‹å·®è¿æ¥..." << std::endl;
    Tensor x1 = add_tensors(x, attn_output);  // éœ€è¦å®ç°add_tensors
    
    std::cout << "3. å±‚å½’ä¸€åŒ–..." << std::endl;
    x1 = norm1.forward(x1);
    
    // 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    std::cout << "4. å‰é¦ˆç½‘ç»œ..." << std::endl;
    Tensor ff_output = ff.forward(x1);
    
    std::cout << "5. æ®‹å·®è¿æ¥..." << std::endl;
    Tensor output = add_tensors(x1, ff_output);
    
    std::cout << "6. å±‚å½’ä¸€åŒ–..." << std::endl;
    output = norm2.forward(output);
    
    std::cout << "\nå‰å‘ä¼ æ’­å®Œæˆ!" << std::endl;
    std::cout << "è¾“å…¥å½¢çŠ¶: [" << batch_size << ", " << seq_len << ", " << d_model << "]" << std::endl;
    std::cout << "è¾“å‡ºå½¢çŠ¶: [" << output.shape[0] << ", " << output.shape[1] << ", " << output.shape[2] << "]" << std::endl;
    
    return output;
}
```

## 8. å­¦ä¹ å»ºè®®ï¼šå¦‚ä½•é€šè¿‡æ˜ å°„è¡¨æ·±åº¦å­¦ä¹ 

### å­¦ä¹ è·¯å¾„å»ºè®®ï¼š

1. **ç¬¬1é˜¶æ®µï¼šç†è§£é«˜å±‚API**
   ```python
   # å…ˆå­¦ä¼šä½¿ç”¨PyTorch
   model = GPT2Model.from_pretrained("gpt2")
   outputs = model(input_ids, attention_mask=attention_mask)
   loss = outputs.loss
   loss.backward()
   ```

2. **ç¬¬2é˜¶æ®µï¼šæŸ¥çœ‹åº•å±‚å®ç°**
   ```cpp
   // ç„¶åæŸ¥çœ‹å¯¹åº”çš„C++å®ç°
   Tensor output = model.forward(input_ids, attention_mask);
   float loss = compute_loss(output, targets);
   model.backward();  // æ‰‹åŠ¨åå‘ä¼ æ’­
   ```

3. **ç¬¬3é˜¶æ®µï¼šé€å±‚æ˜ å°„**
   - ä»æœ€ç®€å•çš„`torch.Tensor` â†” `Tensor`å¼€å§‹
   - ç„¶å`nn.Linear` â†” `LinearLayer`
   - æœ€å`nn.Transformer` â†” `TransformerBlock`

4. **ç¬¬4é˜¶æ®µï¼šæ‰‹åŠ¨å®ç°**
   - å°è¯•ç”¨Pythonå®ç°ç®€åŒ–ç‰ˆçš„C++ç±»
   - éªŒè¯ä¸PyTorchç»“æœçš„ä¸€è‡´æ€§

### è°ƒè¯•å’ŒéªŒè¯æŠ€å·§ï¼š

```python
class DebugHelper:
    """è°ƒè¯•å’ŒéªŒè¯è¾…åŠ©ç±»"""
    
    @staticmethod
    def compare_pytorch_cpp():
        """å¯¹æ¯”PyTorchå’ŒC++å®ç°çš„ç»“æœ"""
        
        # 1. ä½¿ç”¨ç›¸åŒéšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # 2. åˆ›å»ºç›¸åŒè¾“å…¥
        batch_size, seq_len, d_model = 2, 5, 16
        x_np = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        
        # PyTorchè¾“å…¥
        x_torch = torch.from_numpy(x_np)
        
        # C++è¾“å…¥ï¼ˆæ¨¡æ‹Ÿï¼‰
        x_cpp = Tensor({batch_size, seq_len, d_model})
        x_cpp.data = x_np.flatten().tolist()
        
        # 3. è¿è¡ŒPyTorchå±‚
        torch_layer = torch.nn.Linear(d_model, d_model, bias=False)
        with torch.no_grad():
            torch_layer.weight.copy_(torch.eye(d_model))  # å•ä½çŸ©é˜µ
        output_torch = torch_layer(x_torch)
        
        # 4. è¿è¡ŒC++å±‚
        cpp_layer = LinearLayer(d_model, d_model, false)
        # è®¾ç½®æƒé‡ä¸ºå•ä½çŸ©é˜µ
        for i in range(d_model):
            for j in range(d_model):
                cpp_layer.weight[{i, j}] = 1.0 if i == j else 0.0
        output_cpp = cpp_layer.forward(x_cpp)
        
        # 5. æ¯”è¾ƒç»“æœ
        torch_result = output_torch.numpy().flatten()
        cpp_result = output_cpp.data
        
        print("PyTorchç»“æœ:", torch_result[:10])
        print("C++ç»“æœ:", cpp_result[:10])
        print("å·®å¼‚:", np.abs(torch_result - cpp_result).max())
        
        return torch_result, cpp_result
```

è¿™ä¸ªè¯¦ç»†çš„æ˜ å°„è¡¨å¸®åŠ©ä½ ç†è§£PyTorché«˜çº§APIèƒŒåçš„åº•å±‚åŸç†ã€‚å½“ä½ åœ¨PyTorchä¸­è°ƒç”¨ä¸€ä¸ªå‡½æ•°æ—¶ï¼Œä½ å¯ä»¥æƒ³è±¡å®ƒåº•å±‚æ˜¯å¦‚ä½•ç”¨C++å®ç°çš„ã€‚è¿™ç§ç†è§£å¯¹äºè°ƒè¯•ã€æ€§èƒ½ä¼˜åŒ–å’Œå®ç°è‡ªå®šä¹‰æ“ä½œéå¸¸é‡è¦ã€‚

è®°ä½ï¼š**PyTorch = Pythonæ¥å£ + C++å†…æ ¸ + CUDAåŠ é€Ÿ**ã€‚ç†è§£è¿™ä¸ªå±‚æ¬¡ç»“æ„èƒ½è®©ä½ æˆä¸ºæ›´æ·±å…¥çš„æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆã€‚
