# å®Œæ•´LLMè®­ç»ƒç®¡é“ï¼šC++ä¼ªä»£ç å®ç°

æˆ‘å°†æä¾›ä¸€ä¸ªä»åº•å±‚åˆ°é¡¶å±‚çš„å®Œæ•´LLMè®­ç»ƒç®¡é“å®ç°ï¼Œä½¿ç”¨C++é£æ ¼ä¼ªä»£ç å±•ç¤ºæ‰€æœ‰æ•°æ®ç»“æ„å’Œç®—æ³•ã€‚è¿™é€‚åˆä½ åœ¨è„‘ä¸­æˆ–çº¸ä¸Šæ¨¡æ‹Ÿæ•´ä¸ªæµç¨‹ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„æ¦‚è§ˆ

```
llm_training_pipeline/
â”œâ”€â”€ 01_data_pipeline/          # æ•°æ®å¤„ç†ç®¡é“
â”œâ”€â”€ 02_pretraining/           # é¢„è®­ç»ƒé˜¶æ®µ
â”œâ”€â”€ 03_sft/                   # æœ‰ç›‘ç£å¾®è°ƒ
â”œâ”€â”€ 04_reward_modeling/       # å¥–åŠ±å»ºæ¨¡
â”œâ”€â”€ 05_rlhf/                  # RLHFé˜¶æ®µ
â”œâ”€â”€ 06_alignment/             # å¯¹é½è°ƒä¼˜
â”œâ”€â”€ 07_evaluation/            # è¯„ä¼°
â”œâ”€â”€ 08_deployment/            # éƒ¨ç½²
â””â”€â”€ utils/                    # å·¥å…·å‡½æ•°
```

## 1. åŸºç¡€æ•°æ®ç»“æ„å’Œç±»å‹å®šä¹‰

```cpp
// base_types.hpp
#ifndef BASE_TYPES_HPP
#define BASE_TYPES_HPP

#include <vector>
#include <string>
#include <map>
#include <memory>
#include <cmath>
#include <random>
#include <functional>

// ==================== åŸºç¡€ç±»å‹å®šä¹‰ ====================
// Basic Type Definitions

// æ ‡é‡ç±»å‹
typedef float scalar_t;
typedef int32_t token_id_t;
typedef uint32_t batch_size_t;
typedef uint32_t seq_len_t;
typedef uint32_t vocab_size_t;
typedef uint32_t hidden_dim_t;
typedef uint64_t parameter_count_t;

// å¼ é‡åŸºç±»
class Tensor {
public:
    std::vector<size_t> shape;      // ç»´åº¦å½¢çŠ¶
    std::vector<scalar_t> data;     // æ•°æ®å­˜å‚¨
    bool requires_grad;             // æ˜¯å¦éœ€è¦æ¢¯åº¦
    std::vector<scalar_t> grad;     // æ¢¯åº¦å­˜å‚¨
    
    Tensor() : requires_grad(false) {}
    
    Tensor(const std::vector<size_t>& s, bool rg = false) 
        : shape(s), requires_grad(rg) {
        size_t total = 1;
        for (size_t dim : shape) total *= dim;
        data.resize(total, 0.0f);
        if (requires_grad) grad.resize(total, 0.0f);
    }
    
    // è®¿é—®å…ƒç´ 
    scalar_t& operator[](const std::vector<size_t>& indices) {
        size_t idx = 0;
        size_t stride = 1;
        for (int i = shape.size() - 1; i >= 0; i--) {
            idx += indices[i] * stride;
            stride *= shape[i];
        }
        return data[idx];
    }
    
    size_t numel() const {
        size_t total = 1;
        for (size_t dim : shape) total *= dim;
        return total;
    }
};

// ä¼˜åŒ–å™¨çŠ¶æ€
struct OptimizerState {
    scalar_t learning_rate;
    scalar_t beta1;      // Adam beta1
    scalar_t beta2;      // Adam beta2
    scalar_t epsilon;    // Adam epsilon
    int64_t step;        // å½“å‰æ­¥æ•°
    
    // ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©
    std::vector<scalar_t> m;  // ä¸€é˜¶çŸ©
    std::vector<scalar_t> v;  // äºŒé˜¶çŸ©
    
    OptimizerState(scalar_t lr = 1e-3, scalar_t b1 = 0.9, 
                  scalar_t b2 = 0.999, scalar_t eps = 1e-8)
        : learning_rate(lr), beta1(b1), beta2(b2), 
          epsilon(eps), step(0) {}
};

// æ¨¡å‹é…ç½®
struct ModelConfig {
    vocab_size_t vocab_size = 50257;     // è¯æ±‡è¡¨å¤§å°
    hidden_dim_t hidden_size = 768;      // éšè—å±‚ç»´åº¦
    uint32_t num_layers = 12;            // å±‚æ•°
    uint32_t num_heads = 12;             // æ³¨æ„åŠ›å¤´æ•°
    uint32_t max_seq_len = 1024;         // æœ€å¤§åºåˆ—é•¿åº¦
    uint32_t ffn_dim = 3072;             // FFNç»´åº¦
    scalar_t dropout_rate = 0.1f;        // Dropoutç‡
    
    // è®¡ç®—æ€»å‚æ•°é‡
    parameter_count_t total_params() const {
        // ç®€åŒ–è®¡ç®—
        parameter_count_t params = 0;
        
        // è¯åµŒå…¥
        params += vocab_size * hidden_size;
        
        // Transformerå±‚
        for (uint32_t i = 0; i < num_layers; i++) {
            // è‡ªæ³¨æ„åŠ›: Q,K,VæŠ•å½± + è¾“å‡ºæŠ•å½±
            params += 4 * hidden_size * hidden_size;
            
            // å‰é¦ˆç½‘ç»œ
            params += 2 * hidden_size * ffn_dim;
            
            // å±‚å½’ä¸€åŒ–ï¼ˆå¯å¿½ç•¥ï¼Œå‚æ•°é‡å¾ˆå°‘ï¼‰
        }
        
        // è¯­è¨€æ¨¡å‹å¤´
        params += hidden_size * vocab_size;
        
        return params;
    }
};

// è®­ç»ƒæ‰¹æ¬¡
struct TrainingBatch {
    Tensor input_ids;      // [batch_size, seq_len]
    Tensor attention_mask; // [batch_size, seq_len]
    Tensor labels;         // [batch_size, seq_len] æˆ– [batch_size]
    Tensor position_ids;   // [batch_size, seq_len]
    
    // å¯¹äºå¥–åŠ±å»ºæ¨¡
    Tensor chosen_ids;     // è¢«é€‰ä¸­çš„å›ç­”
    Tensor rejected_ids;   // è¢«æ‹’ç»çš„å›ç­”
    scalar_t chosen_score; // äººå·¥è¯„åˆ†
    scalar_t rejected_score;
};

// æŸå¤±å‡½æ•°ç»“æœ
struct LossResult {
    scalar_t loss_value;
    Tensor gradients;  // å¯é€‰ï¼Œç”¨äºæ‰‹åŠ¨åå‘ä¼ æ’­
    std::map<std::string, scalar_t> metrics;  // é¢å¤–æŒ‡æ ‡
};

#endif // BASE_TYPES_HPP
```

## 2. æ•°æ®å¤„ç†ç®¡é“

```cpp
// data_pipeline.hpp
#ifndef DATA_PIPELINE_HPP
#define DATA_PIPELINE_HPP

#include "base_types.hpp"
#include <fstream>
#include <queue>
#include <thread>
#include <mutex>
#include <condition_variable>

// ==================== æ•°æ®å¤„ç†æµæ°´çº¿ ====================
// Data Processing Pipeline

class DataPipeline {
private:
    std::queue<TrainingBatch> data_queue;
    std::mutex queue_mutex;
    std::condition_variable data_ready;
    bool stop_flag = false;
    
    // æ•°æ®æº
    std::vector<std::string> data_files;
    size_t current_file_idx = 0;
    
    // åˆ†è¯å™¨
    class Tokenizer* tokenizer;
    
    // é¢„å¤„ç†é…ç½®
    struct {
        size_t max_seq_len = 1024;
        bool use_causal_mask = true;
        bool shuffle = true;
        size_t buffer_size = 10000;  // é¢„å–ç¼“å†²åŒºå¤§å°
    } config;
    
public:
    DataPipeline(const std::string& data_dir, 
                Tokenizer* tok, 
                size_t batch_size = 32) {
        tokenizer = tok;
        load_data_files(data_dir);
        
        // å¯åŠ¨æ•°æ®åŠ è½½çº¿ç¨‹
        std::thread loader(&DataPipeline::data_loader_thread, this);
        loader.detach();
    }
    
    // è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
    TrainingBatch get_batch() {
        std::unique_lock<std::mutex> lock(queue_mutex);
        
        // ç­‰å¾…æ•°æ®å°±ç»ª
        data_ready.wait(lock, [this]() { 
            return !data_queue.empty() || stop_flag; 
        });
        
        if (stop_flag && data_queue.empty()) {
            throw std::runtime_error("æ•°æ®ç®¡é“å·²åœæ­¢");
        }
        
        TrainingBatch batch = data_queue.front();
        data_queue.pop();
        
        return batch;
    }
    
    // æ•°æ®åŠ è½½çº¿ç¨‹
    void data_loader_thread() {
        std::vector<std::string> buffer;
        
        while (!stop_flag) {
            // å¡«å……ç¼“å†²åŒº
            while (buffer.size() < config.buffer_size && 
                   current_file_idx < data_files.size()) {
                std::string file_path = data_files[current_file_idx];
                load_file_to_buffer(file_path, buffer);
                current_file_idx = (current_file_idx + 1) % data_files.size();
            }
            
            // æ‰“ä¹±æ•°æ®
            if (config.shuffle) {
                std::random_shuffle(buffer.begin(), buffer.end());
            }
            
            // åˆ›å»ºæ‰¹æ¬¡
            for (size_t i = 0; i + config.buffer_size <= buffer.size(); i += config.buffer_size) {
                std::vector<std::string> batch_texts(
                    buffer.begin() + i, 
                    buffer.begin() + i + config.buffer_size
                );
                
                TrainingBatch batch = create_training_batch(batch_texts);
                
                {
                    std::lock_guard<std::mutex> lock(queue_mutex);
                    data_queue.push(batch);
                }
                
                data_ready.notify_one();
            }
            
            // æ¸…ç©ºå·²å¤„ç†çš„æ•°æ®
            buffer.clear();
        }
    }
    
private:
    void load_data_files(const std::string& data_dir) {
        // é€’å½’æ‰«æç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
        // è¿™é‡Œç®€åŒ–ä¸ºç¡¬ç¼–ç æ–‡ä»¶åˆ—è¡¨
        data_files = {
            data_dir + "/corpus_1.txt",
            data_dir + "/corpus_2.txt",
            // ...
        };
    }
    
    void load_file_to_buffer(const std::string& file_path, 
                           std::vector<std::string>& buffer) {
        std::ifstream file(file_path);
        std::string line;
        
        while (std::getline(file, line)) {
            if (!line.empty()) {
                buffer.push_back(line);
            }
        }
    }
    
    TrainingBatch create_training_batch(const std::vector<std::string>& texts) {
        TrainingBatch batch;
        
        size_t batch_size = texts.size();
        size_t seq_len = config.max_seq_len;
        
        // åˆå§‹åŒ–å¼ é‡
        batch.input_ids = Tensor({batch_size, seq_len});
        batch.attention_mask = Tensor({batch_size, seq_len});
        batch.labels = Tensor({batch_size, seq_len});
        batch.position_ids = Tensor({batch_size, seq_len});
        
        // å¡«å……æ•°æ®
        for (size_t b = 0; b < batch_size; b++) {
            std::vector<token_id_t> tokens = tokenizer->encode(texts[b]);
            
            // æˆªæ–­æˆ–å¡«å……åˆ°seq_len
            if (tokens.size() > seq_len) {
                tokens.resize(seq_len);
            } else if (tokens.size() < seq_len) {
                // å¡«å……<pad> tokenï¼ˆå‡è®¾IDä¸º0ï¼‰
                tokens.resize(seq_len, 0);
            }
            
            for (size_t s = 0; s < seq_len; s++) {
                batch.input_ids[{b, s}] = tokens[s];
                batch.attention_mask[{b, s}] = (tokens[s] != 0) ? 1.0f : 0.0f;
                
                // å¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œæ ‡ç­¾æ˜¯ä¸‹ä¸€ä¸ªtoken
                if (s < seq_len - 1) {
                    batch.labels[{b, s}] = tokens[s + 1];
                } else {
                    batch.labels[{b, s}] = -100;  // å¿½ç•¥
                }
                
                batch.position_ids[{b, s}] = s;
            }
        }
        
        return batch;
    }
};

// ==================== åˆ†è¯å™¨å®ç° ====================
class Tokenizer {
private:
    std::map<std::string, token_id_t> token_to_id;
    std::map<token_id_t, std::string> id_to_token;
    token_id_t vocab_size = 0;
    
    // BPEåˆå¹¶è§„åˆ™
    std::map<std::pair<std::string, std::string>, token_id_t> merges;
    
public:
    Tokenizer(vocab_size_t size = 50257) : vocab_size(size) {
        initialize_base_vocab();
    }
    
    void initialize_base_vocab() {
        // åŸºç¡€ASCIIå­—ç¬¦
        for (int i = 0; i < 256; i++) {
            std::string token(1, static_cast<char>(i));
            token_to_id[token] = i;
            id_to_token[i] = token;
        }
        
        // ç‰¹æ®Štoken
        token_to_id["<pad>"] = 256;
        token_to_id["<eos>"] = 257;
        token_to_id["<unk>"] = 258;
        
        id_to_token[256] = "<pad>";
        id_to_token[257] = "<eos>";
        id_to_token[258] = "<unk>";
    }
    
    std::vector<token_id_t> encode(const std::string& text) {
        // ç®€åŒ–çš„BPEç¼–ç 
        std::vector<std::string> tokens;
        
        // æŒ‰å­—ç¬¦åˆ†å‰²
        for (char c : text) {
            tokens.push_back(std::string(1, c));
        }
        
        // åº”ç”¨BPEåˆå¹¶
        bool changed = true;
        while (changed) {
            changed = false;
            
            // æ‰¾åˆ°æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
            std::map<std::pair<std::string, std::string>, int> pair_counts;
            for (size_t i = 0; i < tokens.size() - 1; i++) {
                auto pair = std::make_pair(tokens[i], tokens[i+1]);
                pair_counts[pair]++;
            }
            
            if (pair_counts.empty()) break;
            
            // æ£€æŸ¥æ˜¯å¦æœ‰åˆå¹¶è§„åˆ™
            for (const auto& [pair, count] : pair_counts) {
                if (merges.find(pair) != merges.end()) {
                    // åˆå¹¶è¿™å¯¹å­—ç¬¦
                    std::vector<std::string> new_tokens;
                    for (size_t i = 0; i < tokens.size(); i++) {
                        if (i < tokens.size() - 1 && 
                            tokens[i] == pair.first && 
                            tokens[i+1] == pair.second) {
                            new_tokens.push_back(pair.first + pair.second);
                            i++;  // è·³è¿‡ä¸‹ä¸€ä¸ª
                        } else {
                            new_tokens.push_back(tokens[i]);
                        }
                    }
                    tokens = new_tokens;
                    changed = true;
                    break;
                }
            }
        }
        
        // è½¬æ¢ä¸ºID
        std::vector<token_id_t> ids;
        for (const auto& token : tokens) {
            if (token_to_id.find(token) != token_to_id.end()) {
                ids.push_back(token_to_id[token]);
            } else {
                ids.push_back(token_to_id["<unk>"]);
            }
        }
        
        return ids;
    }
    
    std::string decode(const std::vector<token_id_t>& ids) {
        std::string text;
        for (token_id_t id : ids) {
            if (id_to_token.find(id) != id_to_token.end()) {
                text += id_to_token[id];
            } else {
                text += "<unk>";
            }
        }
        return text;
    }
};

#endif // DATA_PIPELINE_HPP
```

## 3. é¢„è®­ç»ƒé˜¶æ®µï¼ˆå®Œæ•´å®ç°ï¼‰

```cpp
// pretraining.hpp
#ifndef PRETRAINING_HPP
#define PRETRAINING_HPP

#include "base_types.hpp"
#include <cmath>
#include <vector>
#include <functional>

// ==================== æ¿€æ´»å‡½æ•° ====================
namespace Activation {
    inline scalar_t relu(scalar_t x) {
        return std::max((scalar_t)0.0, x);
    }
    
    inline scalar_t gelu(scalar_t x) {
        // GELUè¿‘ä¼¼å®ç°
        return 0.5 * x * (1 + std::tanh(
            std::sqrt(2 / M_PI) * (x + 0.044715 * x * x * x)
        ));
    }
    
    inline scalar_t softplus(scalar_t x) {
        return std::log(1 + std::exp(x));
    }
}

// ==================== å±‚å½’ä¸€åŒ– ====================
class LayerNorm {
private:
    Tensor gamma;  // ç¼©æ”¾å‚æ•° [hidden_size]
    Tensor beta;   // å¹³ç§»å‚æ•° [hidden_size]
    scalar_t eps;
    bool affine;   // æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ å‚æ•°
    
public:
    LayerNorm(hidden_dim_t hidden_size, scalar_t epsilon = 1e-5, bool aff = true) 
        : eps(epsilon), affine(aff) {
        gamma = Tensor({hidden_size}, true);
        beta = Tensor({hidden_size}, true);
        
        // åˆå§‹åŒ–
        for (size_t i = 0; i < hidden_size; i++) {
            gamma.data[i] = 1.0f;
            beta.data[i] = 0.0f;
        }
    }
    
    Tensor forward(const Tensor& x) {
        // x shape: [batch_size, seq_len, hidden_size]
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        size_t hidden_size = x.shape[2];
        
        Tensor output(x.shape);
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                // è®¡ç®—å‡å€¼å’Œæ–¹å·®
                scalar_t mean = 0.0f;
                scalar_t variance = 0.0f;
                
                for (size_t h = 0; h < hidden_size; h++) {
                    mean += x[{b, s, h}];
                }
                mean /= hidden_size;
                
                for (size_t h = 0; h < hidden_size; h++) {
                    scalar_t diff = x[{b, s, h}] - mean;
                    variance += diff * diff;
                }
                variance /= hidden_size;
                
                // å½’ä¸€åŒ–
                scalar_t std = std::sqrt(variance + eps);
                
                for (size_t h = 0; h < hidden_size; h++) {
                    scalar_t normalized = (x[{b, s, h}] - mean) / std;
                    
                    if (affine) {
                        output[{b, s, h}] = gamma.data[h] * normalized + beta.data[h];
                    } else {
                        output[{b, s, h}] = normalized;
                    }
                }
            }
        }
        
        return output;
    }
    
    // åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰
    void backward(const Tensor& grad_output, const Tensor& x) {
        // è®¡ç®—gammaå’Œbetaçš„æ¢¯åº¦
        // å®é™…å®ç°éœ€è¦å®Œæ•´çš„åå‘ä¼ æ’­
    }
};

// ==================== å‰é¦ˆç½‘ç»œ ====================
class FeedForward {
private:
    Tensor weight1;  // [hidden_size, ffn_dim]
    Tensor bias1;    // [ffn_dim]
    Tensor weight2;  // [ffn_dim, hidden_size]
    Tensor bias2;    // [hidden_size]
    
    hidden_dim_t hidden_size;
    hidden_dim_t ffn_dim;
    
public:
    FeedForward(hidden_dim_t h_size, hidden_dim_t f_dim) 
        : hidden_size(h_size), ffn_dim(f_dim) {
        
        // åˆå§‹åŒ–æƒé‡
        weight1 = Tensor({hidden_size, ffn_dim}, true);
        bias1 = Tensor({ffn_dim}, true);
        weight2 = Tensor({ffn_dim, hidden_size}, true);
        bias2 = Tensor({hidden_size}, true);
        
        initialize_weights();
    }
    
    void initialize_weights() {
        // Xavier/Heåˆå§‹åŒ–
        scalar_t std1 = std::sqrt(2.0f / (hidden_size + ffn_dim));
        scalar_t std2 = std::sqrt(2.0f / (ffn_dim + hidden_size));
        
        std::normal_distribution<scalar_t> dist1(0.0f, std1);
        std::normal_distribution<scalar_t> dist2(0.0f, std2);
        std::random_device rd;
        std::mt19937 gen(rd());
        
        for (size_t i = 0; i < weight1.numel(); i++) {
            weight1.data[i] = dist1(gen);
        }
        for (size_t i = 0; i < bias1.numel(); i++) {
            bias1.data[i] = 0.0f;
        }
        for (size_t i = 0; i < weight2.numel(); i++) {
            weight2.data[i] = dist2(gen);
        }
        for (size_t i = 0; i < bias2.numel(); i++) {
            bias2.data[i] = 0.0f;
        }
    }
    
    Tensor forward(const Tensor& x) {
        // x shape: [batch_size, seq_len, hidden_size]
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        
        // ç¬¬ä¸€å±‚: x * W1 + b1
        Tensor hidden(Tensor({batch_size, seq_len, ffn_dim}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t f = 0; f < ffn_dim; f++) {
                    scalar_t sum = bias1.data[f];
                    
                    for (size_t h = 0; h < hidden_size; h++) {
                        sum += x[{b, s, h}] * weight1[{h, f}];
                    }
                    
                    hidden[{b, s, f}] = Activation::gelu(sum);
                }
            }
        }
        
        // ç¬¬äºŒå±‚: hidden * W2 + b2
        Tensor output(Tensor({batch_size, seq_len, hidden_size}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t h = 0; h < hidden_size; h++) {
                    scalar_t sum = bias2.data[h];
                    
                    for (size_t f = 0; f < ffn_dim; f++) {
                        sum += hidden[{b, s, f}] * weight2[{f, h}];
                    }
                    
                    output[{b, s, h}] = sum;
                }
            }
        }
        
        return output;
    }
};

// ==================== å¤šå¤´æ³¨æ„åŠ› ====================
class MultiHeadAttention {
private:
    Tensor W_q;  // [hidden_size, hidden_size]
    Tensor W_k;  // [hidden_size, hidden_size]
    Tensor W_v;  // [hidden_size, hidden_size]
    Tensor W_o;  // [hidden_size, hidden_size]
    
    hidden_dim_t hidden_size;
    uint32_t num_heads;
    hidden_dim_t head_dim;
    scalar_t dropout_rate;
    
public:
    MultiHeadAttention(hidden_dim_t h_size, uint32_t n_heads, scalar_t dropout = 0.1f)
        : hidden_size(h_size), num_heads(n_heads), dropout_rate(dropout) {
        
        head_dim = hidden_size / num_heads;
        
        // åˆå§‹åŒ–æƒé‡
        W_q = Tensor({hidden_size, hidden_size}, true);
        W_k = Tensor({hidden_size, hidden_size}, true);
        W_v = Tensor({hidden_size, hidden_size}, true);
        W_o = Tensor({hidden_size, hidden_size}, true);
        
        initialize_weights();
    }
    
    void initialize_weights() {
        scalar_t std = std::sqrt(2.0f / (hidden_size * 2));
        std::normal_distribution<scalar_t> dist(0.0f, std);
        std::random_device rd;
        std::mt19937 gen(rd());
        
        // åˆå§‹åŒ–æ‰€æœ‰æƒé‡
        auto init_tensor = [&](Tensor& t) {
            for (size_t i = 0; i < t.numel(); i++) {
                t.data[i] = dist(gen);
            }
        };
        
        init_tensor(W_q);
        init_tensor(W_k);
        init_tensor(W_v);
        init_tensor(W_o);
    }
    
    Tensor forward(const Tensor& x, const Tensor& attention_mask) {
        // x shape: [batch_size, seq_len, hidden_size]
        // mask shape: [batch_size, seq_len, seq_len] æˆ– [seq_len, seq_len]
        
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        
        // 1. çº¿æ€§æŠ•å½±å¾—åˆ°Q,K,V
        Tensor Q = linear_projection(x, W_q);  // [batch, seq, hidden]
        Tensor K = linear_projection(x, W_k);
        Tensor V = linear_projection(x, W_v);
        
        // 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Tensor Q_heads = reshape_to_heads(Q);  // [batch, heads, seq, head_dim]
        Tensor K_heads = reshape_to_heads(K);
        Tensor V_heads = reshape_to_heads(V);
        
        // 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        Tensor attention_scores = compute_attention_scores(Q_heads, K_heads);
        
        // 4. åº”ç”¨æ©ç 
        if (attention_mask.shape.size() > 0) {
            apply_attention_mask(attention_scores, attention_mask);
        }
        
        // 5. Softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        Tensor attention_weights = softmax_attention(attention_scores);
        
        // 6. åº”ç”¨Dropoutï¼ˆè®­ç»ƒæ—¶ï¼‰
        // è¿™é‡Œçœç•¥äº†dropoutå®ç°
        
        // 7. æ³¨æ„åŠ›åŠ æƒ
        Tensor attention_output = apply_attention(attention_weights, V_heads);
        
        // 8. é‡å¡‘å›åŸå§‹å½¢çŠ¶
        Tensor output = reshape_from_heads(attention_output);
        
        // 9. è¾“å‡ºæŠ•å½±
        output = linear_projection(output, W_o);
        
        return output;
    }
    
private:
    Tensor linear_projection(const Tensor& x, const Tensor& W) {
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        
        Tensor result(Tensor({batch_size, seq_len, hidden_size}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t h = 0; h < hidden_size; h++) {
                    scalar_t sum = 0.0f;
                    
                    for (size_t i = 0; i < hidden_size; i++) {
                        sum += x[{b, s, i}] * W[{i, h}];
                    }
                    
                    result[{b, s, h}] = sum;
                }
            }
        }
        
        return result;
    }
    
    Tensor reshape_to_heads(const Tensor& x) {
        size_t batch_size = x.shape[0];
        size_t seq_len = x.shape[1];
        
        Tensor heads(Tensor({batch_size, num_heads, seq_len, head_dim}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t s = 0; s < seq_len; s++) {
                    for (size_t d = 0; d < head_dim; d++) {
                        size_t src_idx = h * head_dim + d;
                        heads[{b, h, s, d}] = x[{b, s, src_idx}];
                    }
                }
            }
        }
        
        return heads;
    }
    
    Tensor compute_attention_scores(const Tensor& Q, const Tensor& K) {
        size_t batch_size = Q.shape[0];
        size_t num_heads = Q.shape[1];
        size_t seq_len_q = Q.shape[2];
        size_t seq_len_k = K.shape[2];
        
        Tensor scores(Tensor({batch_size, num_heads, seq_len_q, seq_len_k}));
        
        scalar_t scale_factor = 1.0f / std::sqrt(static_cast<scalar_t>(head_dim));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t j = 0; j < seq_len_k; j++) {
                        scalar_t score = 0.0f;
                        
                        for (size_t d = 0; d < head_dim; d++) {
                            score += Q[{b, h, i, d}] * K[{b, h, j, d}];
                        }
                        
                        scores[{b, h, i, j}] = score * scale_factor;
                    }
                }
            }
        }
        
        return scores;
    }
    
    void apply_attention_mask(Tensor& scores, const Tensor& mask) {
        size_t batch_size = scores.shape[0];
        size_t num_heads = scores.shape[1];
        size_t seq_len_q = scores.shape[2];
        size_t seq_len_k = scores.shape[3];
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t j = 0; j < seq_len_k; j++) {
                        if (mask.shape.size() == 2) {
                            // äºŒç»´æ©ç 
                            if (mask[{i, j}] == 0.0f) {
                                scores[{b, h, i, j}] = -1e9f;
                            }
                        } else if (mask.shape.size() == 3) {
                            // ä¸‰ç»´æ©ç 
                            if (mask[{b, i, j}] == 0.0f) {
                                scores[{b, h, i, j}] = -1e9f;
                            }
                        }
                    }
                }
            }
        }
    }
    
    Tensor softmax_attention(const Tensor& scores) {
        size_t batch_size = scores.shape[0];
        size_t num_heads = scores.shape[1];
        size_t seq_len_q = scores.shape[2];
        size_t seq_len_k = scores.shape[3];
        
        Tensor weights(Tensor({batch_size, num_heads, seq_len_q, seq_len_k}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    // æ‰¾åˆ°æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
                    scalar_t max_val = -std::numeric_limits<scalar_t>::max();
                    for (size_t j = 0; j < seq_len_k; j++) {
                        max_val = std::max(max_val, scores[{b, h, i, j}]);
                    }
                    
                    // è®¡ç®—æŒ‡æ•°å’Œ
                    scalar_t sum_exp = 0.0f;
                    for (size_t j = 0; j < seq_len_k; j++) {
                        sum_exp += std::exp(scores[{b, h, i, j}] - max_val);
                    }
                    
                    // è®¡ç®—softmax
                    for (size_t j = 0; j < seq_len_k; j++) {
                        weights[{b, h, i, j}] = 
                            std::exp(scores[{b, h, i, j}] - max_val) / sum_exp;
                    }
                }
            }
        }
        
        return weights;
    }
    
    Tensor apply_attention(const Tensor& weights, const Tensor& V) {
        size_t batch_size = weights.shape[0];
        size_t num_heads = weights.shape[1];
        size_t seq_len_q = weights.shape[2];
        size_t seq_len_k = weights.shape[3];
        size_t head_dim = V.shape[3];
        
        Tensor output(Tensor({batch_size, num_heads, seq_len_q, head_dim}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < num_heads; h++) {
                for (size_t i = 0; i < seq_len_q; i++) {
                    for (size_t d = 0; d < head_dim; d++) {
                        scalar_t sum = 0.0f;
                        
                        for (size_t j = 0; j < seq_len_k; j++) {
                            sum += weights[{b, h, i, j}] * V[{b, h, j, d}];
                        }
                        
                        output[{b, h, i, d}] = sum;
                    }
                }
            }
        }
        
        return output;
    }
    
    Tensor reshape_from_heads(const Tensor& x) {
        size_t batch_size = x.shape[0];
        size_t num_heads = x.shape[1];
        size_t seq_len = x.shape[2];
        size_t head_dim = x.shape[3];
        
        Tensor output(Tensor({batch_size, seq_len, hidden_size}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t h = 0; h < num_heads; h++) {
                    for (size_t d = 0; d < head_dim; d++) {
                        size_t dst_idx = h * head_dim + d;
                        output[{b, s, dst_idx}] = x[{b, h, s, d}];
                    }
                }
            }
        }
        
        return output;
    }
};

// ==================== Transformerå— ====================
class TransformerBlock {
private:
    MultiHeadAttention attention;
    LayerNorm norm1;
    FeedForward ff;
    LayerNorm norm2;
    scalar_t dropout_rate;
    
public:
    TransformerBlock(const ModelConfig& config)
        : attention(config.hidden_size, config.num_heads, config.dropout_rate),
          norm1(config.hidden_size),
          ff(config.hidden_size, config.ffn_dim),
          norm2(config.hidden_size),
          dropout_rate(config.dropout_rate) {}
    
    Tensor forward(const Tensor& x, const Tensor& attention_mask) {
        // è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        Tensor attn_output = attention.forward(x, attention_mask);
        Tensor x1 = add_residual(x, attn_output);
        x1 = norm1.forward(x1);
        
        // å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        Tensor ff_output = ff.forward(x1);
        Tensor output = add_residual(x1, ff_output);
        output = norm2.forward(output);
        
        return output;
    }
    
private:
    Tensor add_residual(const Tensor& x, const Tensor& residual) {
        // ç®€å•çš„é€å…ƒç´ ç›¸åŠ 
        Tensor result(x.shape);
        
        for (size_t i = 0; i < x.numel(); i++) {
            result.data[i] = x.data[i] + residual.data[i];
        }
        
        return result;
    }
};

// ==================== GPTæ¨¡å‹ ====================
class GPTModel {
private:
    ModelConfig config;
    
    // åµŒå…¥å±‚
    Tensor token_embedding;  // [vocab_size, hidden_size]
    Tensor position_embedding;  // [max_seq_len, hidden_size]
    
    // Transformerå±‚
    std::vector<TransformerBlock> layers;
    
    // è¾“å‡ºå±‚
    LayerNorm final_norm;
    Tensor lm_head;  // [hidden_size, vocab_size]
    
public:
    GPTModel(const ModelConfig& cfg) : config(cfg), final_norm(cfg.hidden_size) {
        // åˆå§‹åŒ–åµŒå…¥å±‚
        token_embedding = Tensor({config.vocab_size, config.hidden_size}, true);
        position_embedding = Tensor({config.max_seq_len, config.hidden_size}, true);
        
        // åˆå§‹åŒ–Transformerå±‚
        for (uint32_t i = 0; i < config.num_layers; i++) {
            layers.emplace_back(config);
        }
        
        // åˆå§‹åŒ–è¯­è¨€æ¨¡å‹å¤´
        lm_head = Tensor({config.hidden_size, config.vocab_size}, true);
        
        initialize_weights();
    }
    
    void initialize_weights() {
        // åˆå§‹åŒ–æ‰€æœ‰å‚æ•°
        scalar_t std = 0.02f;  // GPT-2ä½¿ç”¨çš„æ ‡å‡†å·®
        
        std::normal_distribution<scalar_t> dist(0.0f, std);
        std::random_device rd;
        std::mt19937 gen(rd());
        
        // åˆå§‹åŒ–è¯åµŒå…¥
        for (size_t i = 0; i < token_embedding.numel(); i++) {
            token_embedding.data[i] = dist(gen);
        }
        
        // åˆå§‹åŒ–ä½ç½®åµŒå…¥ï¼ˆä½¿ç”¨æ­£å¼¦ä½™å¼¦ï¼‰
        for (size_t pos = 0; pos < config.max_seq_len; pos++) {
            for (size_t i = 0; i < config.hidden_size; i++) {
                if (i % 2 == 0) {
                    position_embedding[{pos, i}] = 
                        std::sin(pos / std::pow(10000.0f, i / config.hidden_size));
                } else {
                    position_embedding[{pos, i}] = 
                        std::cos(pos / std::pow(10000.0f, (i-1) / config.hidden_size));
                }
            }
        }
        
        // åˆå§‹åŒ–lm_headï¼ˆä¸è¯åµŒå…¥å…±äº«æƒé‡ï¼Œè¿™æ˜¯å¸¸è§åšæ³•ï¼‰
        // è¿™é‡Œç®€åŒ–ä¸ºéšæœºåˆå§‹åŒ–
        for (size_t i = 0; i < lm_head.numel(); i++) {
            lm_head.data[i] = dist(gen);
        }
    }
    
    Tensor forward(const Tensor& input_ids, const Tensor& attention_mask) {
        size_t batch_size = input_ids.shape[0];
        size_t seq_len = input_ids.shape[1];
        
        // 1. è¯åµŒå…¥ + ä½ç½®åµŒå…¥
        Tensor embeddings(Tensor({batch_size, seq_len, config.hidden_size}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                token_id_t token_id = static_cast<token_id_t>(input_ids[{b, s}]);
                
                for (size_t h = 0; h < config.hidden_size; h++) {
                    embeddings[{b, s, h}] = 
                        token_embedding[{token_id, h}] + 
                        position_embedding[{s, h}];
                }
            }
        }
        
        // 2. é€šè¿‡Transformerå±‚
        Tensor hidden_states = embeddings;
        
        for (auto& layer : layers) {
            hidden_states = layer.forward(hidden_states, attention_mask);
        }
        
        // 3. æœ€ç»ˆå±‚å½’ä¸€åŒ–
        hidden_states = final_norm.forward(hidden_states);
        
        // 4. è¯­è¨€æ¨¡å‹å¤´
        Tensor logits(Tensor({batch_size, seq_len, config.vocab_size}));
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                for (size_t v = 0; v < config.vocab_size; v++) {
                    scalar_t sum = 0.0f;
                    
                    for (size_t h = 0; h < config.hidden_size; h++) {
                        sum += hidden_states[{b, s, h}] * lm_head[{h, v}];
                    }
                    
                    logits[{b, s, v}] = sum;
                }
            }
        }
        
        return logits;
    }
};

#endif // PRETRAINING_HPP
```

## 4. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°

```cpp
// optimizer.hpp
#ifndef OPTIMIZER_HPP
#define OPTIMIZER_HPP

#include "base_types.hpp"
#include <vector>
#include <cmath>
#include <map>

// ==================== æŸå¤±å‡½æ•° ====================
class LossFunction {
public:
    virtual LossResult compute(const Tensor& predictions, 
                              const Tensor& targets) = 0;
    virtual ~LossFunction() = default;
};

class CrossEntropyLoss : public LossFunction {
private:
    scalar_t label_smoothing;
    
public:
    CrossEntropyLoss(scalar_t smoothing = 0.0f) : label_smoothing(smoothing) {}
    
    LossResult compute(const Tensor& predictions, const Tensor& targets) override {
        // predictions: [batch_size, seq_len, vocab_size]
        // targets: [batch_size, seq_len]
        
        size_t batch_size = predictions.shape[0];
        size_t seq_len = predictions.shape[1];
        size_t vocab_size = predictions.shape[2];
        
        LossResult result;
        scalar_t total_loss = 0.0f;
        size_t total_tokens = 0;
        
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                token_id_t target_id = static_cast<token_id_t>(targets[{b, s}]);
                
                if (target_id == static_cast<token_id_t>(-100)) {
                    continue;  // å¿½ç•¥æ ‡è®°
                }
                
                // æ‰¾åˆ°é¢„æµ‹çš„æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
                scalar_t max_logit = -std::numeric_limits<scalar_t>::max();
                for (size_t v = 0; v < vocab_size; v++) {
                    max_logit = std::max(max_logit, predictions[{b, s, v}]);
                }
                
                // è®¡ç®—log sum exp
                scalar_t log_sum_exp = 0.0f;
                for (size_t v = 0; v < vocab_size; v++) {
                    log_sum_exp += std::exp(predictions[{b, s, v}] - max_logit);
                }
                log_sum_exp = max_logit + std::log(log_sum_exp);
                
                // è®¡ç®—äº¤å‰ç†µæŸå¤±
                scalar_t target_logit = predictions[{b, s, target_id}];
                scalar_t loss = log_sum_exp - target_logit;
                
                // æ ‡ç­¾å¹³æ»‘
                if (label_smoothing > 0.0f) {
                    scalar_t smooth_loss = 0.0f;
                    for (size_t v = 0; v < vocab_size; v++) {
                        if (v == target_id) {
                            smooth_loss += (1.0f - label_smoothing) * 
                                         (log_sum_exp - predictions[{b, s, v}]);
                        } else {
                            smooth_loss += (label_smoothing / (vocab_size - 1)) * 
                                         (log_sum_exp - predictions[{b, s, v}]);
                        }
                    }
                    loss = smooth_loss;
                }
                
                total_loss += loss;
                total_tokens++;
            }
        }
        
        result.loss_value = total_loss / total_tokens;
        result.metrics["perplexity"] = std::exp(result.loss_value);
        
        return result;
    }
};

// ==================== ä¼˜åŒ–å™¨ ====================
class Optimizer {
protected:
    scalar_t learning_rate;
    std::vector<Tensor*> parameters;
    std::vector<OptimizerState> states;
    
public:
    Optimizer(scalar_t lr = 1e-3) : learning_rate(lr) {}
    
    virtual void add_parameters(Tensor* param) {
        parameters.push_back(param);
        states.emplace_back(learning_rate);
    }
    
    virtual void step() = 0;
    virtual void zero_grad() {
        for (auto param : parameters) {
            if (param->requires_grad) {
                std::fill(param->grad.begin(), param->grad.end(), 0.0f);
            }
        }
    }
    
    virtual ~Optimizer() = default;
};

class SGD : public Optimizer {
private:
    scalar_t momentum;
    std::vector<std::vector<scalar_t>> velocities;
    
public:
    SGD(scalar_t lr = 1e-3, scalar_t mom = 0.9f) 
        : Optimizer(lr), momentum(mom) {}
    
    void add_parameters(Tensor* param) override {
        Optimizer::add_parameters(param);
        velocities.emplace_back(param->grad.size(), 0.0f);
    }
    
    void step() override {
        for (size_t i = 0; i < parameters.size(); i++) {
            Tensor* param = parameters[i];
            
            if (!param->requires_grad) continue;
            
            for (size_t j = 0; j < param->grad.size(); j++) {
                // åŠ¨é‡æ›´æ–°
                velocities[i][j] = momentum * velocities[i][j] + 
                                  learning_rate * param->grad[j];
                param->data[j] -= velocities[i][j];
            }
        }
    }
};

class AdamW : public Optimizer {
private:
    scalar_t beta1;
    scalar_t beta2;
    scalar_t epsilon;
    scalar_t weight_decay;
    
public:
    AdamW(scalar_t lr = 1e-3, scalar_t b1 = 0.9f, scalar_t b2 = 0.999f,
         scalar_t eps = 1e-8f, scalar_t wd = 0.01f)
        : Optimizer(lr), beta1(b1), beta2(b2), epsilon(eps), weight_decay(wd) {}
    
    void step() override {
        for (size_t i = 0; i < parameters.size(); i++) {
            Tensor* param = parameters[i];
            OptimizerState& state = states[i];
            
            if (!param->requires_grad) continue;
            
            state.step++;
            
            // åˆå§‹åŒ–çŸ©ä¼°è®¡
            if (state.m.empty()) {
                state.m.resize(param->grad.size(), 0.0f);
                state.v.resize(param->grad.size(), 0.0f);
            }
            
            // åå·®æ ¡æ­£ç³»æ•°
            scalar_t m_correction = 1.0f - std::pow(beta1, state.step);
            scalar_t v_correction = 1.0f - std::pow(beta2, state.step);
            
            for (size_t j = 0; j < param->grad.size(); j++) {
                scalar_t grad = param->grad[j];
                
                // æ›´æ–°ä¸€é˜¶çŸ©
                state.m[j] = beta1 * state.m[j] + (1.0f - beta1) * grad;
                
                // æ›´æ–°äºŒé˜¶çŸ©
                state.v[j] = beta2 * state.v[j] + (1.0f - beta2) * grad * grad;
                
                // åå·®æ ¡æ­£
                scalar_t m_hat = state.m[j] / m_correction;
                scalar_t v_hat = state.v[j] / v_correction;
                
                // AdamWæ›´æ–°è§„åˆ™
                scalar_t update = state.learning_rate * m_hat / 
                                 (std::sqrt(v_hat) + epsilon);
                
                // æƒé‡è¡°å‡
                update += state.learning_rate * weight_decay * param->data[j];
                
                param->data[j] -= update;
            }
        }
    }
};

// ==================== æ¢¯åº¦è£å‰ª ====================
class GradientClipper {
private:
    scalar_t max_norm;
    
public:
    GradientClipper(scalar_t max_n = 1.0f) : max_norm(max_n) {}
    
    void clip(std::vector<Tensor*>& parameters) {
        // è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
        scalar_t total_norm_sq = 0.0f;
        
        for (auto param : parameters) {
            if (!param->requires_grad) continue;
            
            for (scalar_t grad : param->grad) {
                total_norm_sq += grad * grad;
            }
        }
        
        scalar_t total_norm = std::sqrt(total_norm_sq);
        
        // å¦‚æœè¶…è¿‡æœ€å¤§èŒƒæ•°ï¼Œè¿›è¡Œç¼©æ”¾
        if (total_norm > max_norm) {
            scalar_t scale = max_norm / (total_norm + 1e-6f);
            
            for (auto param : parameters) {
                if (!param->requires_grad) continue;
                
                for (scalar_t& grad : param->grad) {
                    grad *= scale;
                }
            }
        }
    }
};

// ==================== å­¦ä¹ ç‡è°ƒåº¦å™¨ ====================
class LearningRateScheduler {
public:
    enum ScheduleType {
        CONSTANT,
        LINEAR_WARMUP,
        COSINE_DECAY,
        STEP_DECAY
    };
    
private:
    ScheduleType type;
    scalar_t initial_lr;
    scalar_t current_lr;
    scalar_t min_lr;
    size_t warmup_steps;
    size_t total_steps;
    size_t current_step;
    
public:
    LearningRateScheduler(ScheduleType t = LINEAR_WARMUP, 
                         scalar_t lr = 1e-3,
                         size_t warmup = 1000,
                         size_t total = 100000,
                         scalar_t min = 1e-6f)
        : type(t), initial_lr(lr), current_lr(lr), min_lr(min),
          warmup_steps(warmup), total_steps(total), current_step(0) {}
    
    scalar_t get_lr() {
        current_step++;
        
        switch (type) {
            case CONSTANT:
                return initial_lr;
                
            case LINEAR_WARMUP: {
                if (current_step <= warmup_steps) {
                    // çº¿æ€§é¢„çƒ­
                    return initial_lr * (current_step / (scalar_t)warmup_steps);
                } else {
                    // ä½™å¼¦è¡°å‡
                    scalar_t progress = (current_step - warmup_steps) / 
                                      (scalar_t)(total_steps - warmup_steps);
                    progress = std::min(progress, 1.0f);
                    
                    scalar_t cosine_decay = 0.5f * 
                        (1.0f + std::cos(M_PI * progress));
                    
                    current_lr = min_lr + 
                        (initial_lr - min_lr) * cosine_decay;
                    return current_lr;
                }
            }
                
            case COSINE_DECAY: {
                scalar_t progress = current_step / (scalar_t)total_steps;
                progress = std::min(progress, 1.0f);
                
                scalar_t cosine_decay = 0.5f * 
                    (1.0f + std::cos(M_PI * progress));
                
                current_lr = min_lr + 
                    (initial_lr - min_lr) * cosine_decay;
                return current_lr;
            }
                
            case STEP_DECAY: {
                // æ¯10000æ­¥è¡°å‡ä¸ºåŸæ¥çš„0.9å€
                size_t decay_steps = 10000;
                size_t decay_count = current_step / decay_steps;
                
                current_lr = initial_lr * std::pow(0.9f, decay_count);
                current_lr = std::max(current_lr, min_lr);
                return current_lr;
            }
        }
        
        return initial_lr;
    }
    
    void update_optimizer(Optimizer& optimizer) {
        scalar_t lr = get_lr();
        // æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        // è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ä¼˜åŒ–å™¨å®ç°æ¥è®¾ç½®
    }
};

#endif // OPTIMIZER_HPP
```

## 5. é¢„è®­ç»ƒä¸»å¾ªç¯

```cpp
// train_pipeline.hpp
#ifndef TRAIN_PIPELINE_HPP
#define TRAIN_PIPELINE_HPP

#include "pretraining.hpp"
#include "optimizer.hpp"
#include "data_pipeline.hpp"
#include <chrono>
#include <fstream>
#include <iomanip>

// ==================== è®­ç»ƒç›‘æ§å™¨ ====================
class TrainingMonitor {
private:
    struct Checkpoint {
        size_t step;
        scalar_t loss;
        scalar_t learning_rate;
        std::chrono::system_clock::time_point timestamp;
        std::string file_path;
    };
    
    std::vector<Checkpoint> checkpoints;
    std::string log_file;
    std::ofstream log_stream;
    
    // è®­ç»ƒç»Ÿè®¡
    scalar_t running_loss = 0.0f;
    size_t running_steps = 0;
    std::chrono::steady_clock::time_point start_time;
    
public:
    TrainingMonitor(const std::string& log_path = "training_log.txt") 
        : log_file(log_path) {
        log_stream.open(log_file);
        start_time = std::chrono::steady_clock::now();
    }
    
    ~TrainingMonitor() {
        if (log_stream.is_open()) {
            log_stream.close();
        }
    }
    
    void log_step(size_t step, scalar_t loss, scalar_t lr, 
                 const std::map<std::string, scalar_t>& metrics = {}) {
        auto now = std::chrono::steady_clock::now();
        auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
            now - start_time).count();
        
        // æ›´æ–°è¿è¡Œç»Ÿè®¡
        running_loss += loss;
        running_steps++;
        
        // æ¯100æ­¥æ‰“å°ä¸€æ¬¡
        if (step % 100 == 0) {
            scalar_t avg_loss = running_loss / running_steps;
            
            std::cout << std::setw(8) << step << " | "
                      << std::setw(10) << std::fixed << std::setprecision(4) 
                      << loss << " | "
                      << std::setw(10) << avg_loss << " | "
                      << std::setw(12) << std::scientific << lr << " | "
                      << std::setw(8) << elapsed << "s"
                      << std::endl;
            
            // é‡ç½®è¿è¡Œç»Ÿè®¡
            running_loss = 0.0f;
            running_steps = 0;
        }
        
        // å†™å…¥æ—¥å¿—æ–‡ä»¶
        log_stream << step << ", " << loss << ", " << lr << ", " << elapsed;
        for (const auto& [key, value] : metrics) {
            log_stream << ", " << value;
        }
        log_stream << std::endl;
    }
    
    void save_checkpoint(size_t step, scalar_t loss, scalar_t lr, 
                        const std::string& model_path) {
        Checkpoint checkpoint;
        checkpoint.step = step;
        checkpoint.loss = loss;
        checkpoint.learning_rate = lr;
        checkpoint.timestamp = std::chrono::system_clock::now();
        checkpoint.file_path = model_path;
        
        checkpoints.push_back(checkpoint);
        
        // ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æ–‡ä»¶
        std::ofstream checkpoint_file("checkpoints/checkpoint_" + 
                                     std::to_string(step) + ".json");
        
        checkpoint_file << "{"
                       << "\"step\": " << step << ", "
                       << "\"loss\": " << loss << ", "
                       << "\"lr\": " << lr << ", "
                       << "\"timestamp\": \"" 
                       << std::chrono::system_clock::to_time_t(checkpoint.timestamp)
                       << "\", "
                       << "\"file_path\": \"" << model_path << "\""
                       << "}" << std::endl;
        
        checkpoint_file.close();
    }
};

// ==================== é¢„è®­ç»ƒä¸»å¾ªç¯ ====================
class PreTrainingPipeline {
private:
    ModelConfig config;
    GPTModel model;
    DataPipeline data_pipeline;
    AdamW optimizer;
    CrossEntropyLoss loss_function;
    GradientClipper gradient_clipper;
    LearningRateScheduler lr_scheduler;
    TrainingMonitor monitor;
    
    // è®­ç»ƒçŠ¶æ€
    size_t current_step = 0;
    size_t total_steps;
    size_t save_every;
    size_t eval_every;
    
public:
    PreTrainingPipeline(const ModelConfig& cfg, 
                       const std::string& data_dir,
                       size_t total_s = 100000,
                       size_t save_interval = 1000,
                       size_t eval_interval = 500)
        : config(cfg),
          model(cfg),
          data_pipeline(data_dir, new Tokenizer(cfg.vocab_size)),
          optimizer(1e-4, 0.9, 0.999, 1e-8, 0.01),
          loss_function(0.1f),  // 10%æ ‡ç­¾å¹³æ»‘
          gradient_clipper(1.0f),
          lr_scheduler(LearningRateScheduler::LINEAR_WARMUP, 
                      1e-4, 1000, total_s, 1e-6),
          monitor("pretraining_log.txt"),
          total_steps(total_s),
          save_every(save_interval),
          eval_every(eval_interval) {
        
        // æ³¨å†Œæ¨¡å‹å‚æ•°åˆ°ä¼˜åŒ–å™¨
        register_model_parameters();
    }
    
    void register_model_parameters() {
        // è¿™é‡Œéœ€è¦æ³¨å†Œæ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        // ç”±äºæˆ‘ä»¬ç®€åŒ–äº†æ¨¡å‹å®ç°ï¼Œè¿™é‡Œçœç•¥å…·ä½“å®ç°
        // åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦éå†æ¨¡å‹çš„æ‰€æœ‰å±‚ï¼Œæ”¶é›†æ‰€æœ‰requires_grad=trueçš„å¼ é‡
    }
    
    void train() {
        std::cout << "å¼€å§‹é¢„è®­ç»ƒ..." << std::endl;
        std::cout << "æ¨¡å‹å‚æ•°é‡: " << config.total_params() << std::endl;
        std::cout << "æ€»è®­ç»ƒæ­¥æ•°: " << total_steps << std::endl;
        std::cout << "=" << 80 << std::endl;
        std::cout << std::setw(8) << "Step" << " | "
                  << std::setw(10) << "Loss" << " | "
                  << std::setw(10) << "Avg Loss" << " | "
                  << std::setw(12) << "LR" << " | "
                  << std::setw(8) << "Time" << std::endl;
        std::cout << "=" << 80 << std::endl;
        
        while (current_step < total_steps) {
            train_step();
            current_step++;
            
            // å®šæœŸè¯„ä¼°
            if (current_step % eval_every == 0) {
                evaluate();
            }
            
            // å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (current_step % save_every == 0) {
                save_checkpoint();
            }
        }
        
        std::cout << "é¢„è®­ç»ƒå®Œæˆ!" << std::endl;
    }
    
private:
    void train_step() {
        // 1. è·å–è®­ç»ƒæ‰¹æ¬¡
        TrainingBatch batch = data_pipeline.get_batch();
        
        // 2. å‰å‘ä¼ æ’­
        Tensor logits = model.forward(batch.input_ids, batch.attention_mask);
        
        // 3. è®¡ç®—æŸå¤±
        LossResult loss_result = loss_function.compute(logits, batch.labels);
        scalar_t loss = loss_result.loss_value;
        
        // 4. åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦è®¡ç®—æ¢¯åº¦
        compute_gradients(logits, batch.labels);
        
        // 5. æ¢¯åº¦è£å‰ª
        // gradient_clipper.clip(model_parameters);
        
        // 6. æ›´æ–°å­¦ä¹ ç‡
        scalar_t lr = lr_scheduler.get_lr();
        // optimizer.set_learning_rate(lr);
        
        // 7. ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.zero_grad();
        optimizer.step();
        
        // 8. è®°å½•æ—¥å¿—
        std::map<std::string, scalar_t> metrics;
        metrics["perplexity"] = loss_result.metrics["perplexity"];
        
        monitor.log_step(current_step, loss, lr, metrics);
    }
    
    void compute_gradients(const Tensor& logits, const Tensor& targets) {
        // ç®€åŒ–çš„æ¢¯åº¦è®¡ç®—ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
        // å®é™…å®ç°éœ€è¦å®Œæ•´çš„åå‘ä¼ æ’­
        // è¿™é‡Œæˆ‘ä»¬å‡è®¾å·²ç»è®¡ç®—å¥½äº†æ¢¯åº¦
    }
    
    void evaluate() {
        // è¯„ä¼°æ¨¡å‹æ€§èƒ½
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥åœ¨éªŒè¯é›†ä¸Šè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
        
        std::cout << "è¯„ä¼°æ­¥æ•° " << current_step << "..." << std::endl;
        
        // ç¤ºä¾‹ï¼šè®¡ç®—éªŒè¯æŸå¤±
        scalar_t valid_loss = 0.0f;
        size_t valid_steps = 10;  // åªè¯„ä¼°å°‘é‡æ‰¹æ¬¡
        
        for (size_t i = 0; i < valid_steps; i++) {
            TrainingBatch batch = data_pipeline.get_batch();
            Tensor logits = model.forward(batch.input_ids, batch.attention_mask);
            LossResult loss_result = loss_function.compute(logits, batch.labels);
            valid_loss += loss_result.loss_value;
        }
        
        valid_loss /= valid_steps;
        
        std::cout << "éªŒè¯æŸå¤±: " << valid_loss 
                  << ", å›°æƒ‘åº¦: " << std::exp(valid_loss) << std::endl;
    }
    
    void save_checkpoint() {
        std::string checkpoint_path = "checkpoints/step_" + 
                                     std::to_string(current_step) + ".bin";
        
        // ä¿å­˜æ¨¡å‹çŠ¶æ€
        save_model(checkpoint_path);
        
        // è®°å½•æ£€æŸ¥ç‚¹
        monitor.save_checkpoint(current_step, 0.0f,  // æŸå¤±éœ€è¦ä»ç›‘æ§å™¨è·å–
                               lr_scheduler.get_lr(), checkpoint_path);
        
        std::cout << "ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: " << checkpoint_path << std::endl;
    }
    
    void save_model(const std::string& path) {
        // ä¿å­˜æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦åºåˆ—åŒ–æ‰€æœ‰å‚æ•°
        
        std::ofstream file(path, std::ios::binary);
        if (!file) {
            std::cerr << "æ— æ³•æ‰“å¼€æ–‡ä»¶ä¿å­˜æ¨¡å‹: " << path << std::endl;
            return;
        }
        
        // ä¿å­˜æ¨¡å‹é…ç½®
        file.write(reinterpret_cast<const char*>(&config.vocab_size), 
                  sizeof(config.vocab_size));
        file.write(reinterpret_cast<const char*>(&config.hidden_size), 
                  sizeof(config.hidden_size));
        // ... ä¿å­˜å…¶ä»–é…ç½®
        
        // ä¿å­˜æ¨¡å‹å‚æ•°
        // è¿™é‡Œéœ€è¦ä¿å­˜æ‰€æœ‰å±‚çš„æƒé‡å’Œåç½®
        
        file.close();
    }
};

#endif // TRAIN_PIPELINE_HPP
```

## 6. SFTè®­ç»ƒé˜¶æ®µ

```cpp
// sft_training.hpp
#ifndef SFT_TRAINING_HPP
#define SFT_TRAINING_HPP

#include "pretraining.hpp"
#include "optimizer.hpp"

// ==================== SFTæ•°æ®é›† ====================
class SFTDataset {
private:
    struct InstructionExample {
        std::string instruction;
        std::string input;
        std::string output;
        std::string system_prompt;
    };
    
    std::vector<InstructionExample> examples;
    Tokenizer* tokenizer;
    
public:
    SFTDataset(Tokenizer* tok) : tokenizer(tok) {
        // åŠ è½½æˆ–ç”ŸæˆSFTæ•°æ®
        load_examples();
    }
    
    void load_examples() {
        // ç¤ºä¾‹æ•°æ®
        examples = {
            {"è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "", 
             "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ...",
             "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
             
            {"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", "",
             "æ˜¥é£æ‹‚é¢èŠ±é¦™æº¢ï¼Œä¸‡ç‰©å¤è‹ç”Ÿæœºå‹ƒ...",
             "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„AI"},
             
            {"å¦‚ä½•ç…®æ„å¤§åˆ©é¢ï¼Ÿ", "",
             "é¦–å…ˆï¼ŒæŠŠæ°´çƒ§å¼€ï¼ŒåŠ å…¥ç›...",
             "ä½ æ˜¯ä¸€ä¸ªå¨è‰ºåŠ©æ‰‹"},
             
            {"è§£é‡Šé‡å­è®¡ç®—", "",
             "é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œè®¡ç®—çš„ä¸€ç§æ–¹æ³•...",
             "ä½ æ˜¯ä¸€ä¸ªç§‘å­¦åŠ©æ‰‹"}
        };
    }
    
    TrainingBatch get_batch(size_t batch_size) {
        TrainingBatch batch;
        
        // éšæœºé€‰æ‹©æ ·æœ¬
        std::vector<size_t> indices(batch_size);
        for (size_t i = 0; i < batch_size; i++) {
            indices[i] = rand() % examples.size();
        }
        
        // æ„å»ºæ‰¹æ¬¡
        size_t max_seq_len = 512;
        
        batch.input_ids = Tensor({batch_size, max_seq_len});
        batch.attention_mask = Tensor({batch_size, max_seq_len});
        batch.labels = Tensor({batch_size, max_seq_len});
        
        for (size_t i = 0; i < batch_size; i++) {
            const InstructionExample& example = examples[indices[i]];
            
            // æ ¼å¼åŒ–æç¤º
            std::string prompt = format_prompt(example);
            
            // ç¼–ç æç¤ºå’Œå›å¤
            std::vector<token_id_t> prompt_tokens = tokenizer->encode(prompt);
            std::vector<token_id_t> output_tokens = tokenizer->encode(example.output);
            
            // åˆå¹¶è¾“å…¥å’Œè¾“å‡º
            std::vector<token_id_t> all_tokens = prompt_tokens;
            all_tokens.insert(all_tokens.end(), 
                            output_tokens.begin(), 
                            output_tokens.end());
            
            // æ·»åŠ EOS token
            all_tokens.push_back(tokenizer->get_eos_id());
            
            // æˆªæ–­æˆ–å¡«å……
            if (all_tokens.size() > max_seq_len) {
                all_tokens.resize(max_seq_len);
            } else if (all_tokens.size() < max_seq_len) {
                all_tokens.resize(max_seq_len, tokenizer->get_pad_id());
            }
            
            // å¡«å……åˆ°æ‰¹æ¬¡
            for (size_t j = 0; j < max_seq_len; j++) {
                batch.input_ids[{i, j}] = all_tokens[j];
                batch.attention_mask[{i, j}] = (all_tokens[j] != 
                                              tokenizer->get_pad_id()) ? 1.0f : 0.0f;
                
                // å¯¹äºSFTï¼Œæˆ‘ä»¬åªè®¡ç®—è¾“å‡ºéƒ¨åˆ†çš„æŸå¤±
                // æ ‡ç­¾ä¸­ï¼Œè¾“å…¥éƒ¨åˆ†è®¾ç½®ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
                if (j < prompt_tokens.size()) {
                    batch.labels[{i, j}] = -100;
                } else if (j < all_tokens.size() - 1) {
                    // è¾“å‡ºéƒ¨åˆ†çš„æ ‡ç­¾æ˜¯ä¸‹ä¸€ä¸ªtoken
                    batch.labels[{i, j}] = all_tokens[j + 1];
                } else {
                    batch.labels[{i, j}] = -100;
                }
            }
        }
        
        return batch;
    }
    
private:
    std::string format_prompt(const InstructionExample& example) {
        // ä½¿ç”¨ChatMLæ ¼å¼
        std::string prompt;
        
        if (!example.system_prompt.empty()) {
            prompt += "<|im_start|>system\n";
            prompt += example.system_prompt + "\n";
            prompt += "<|im_end|>\n";
        }
        
        prompt += "<|im_start|>user\n";
        prompt += example.instruction;
        if (!example.input.empty()) {
            prompt += "\n" + example.input;
        }
        prompt += "\n<|im_end|>\n";
        prompt += "<|im_start|>assistant\n";
        
        return prompt;
    }
};

// ==================== SFTè®­ç»ƒå™¨ ====================
class SFTTrainer {
private:
    GPTModel& model;
    SFTDataset dataset;
    AdamW optimizer;
    CrossEntropyLoss loss_function;
    size_t num_epochs;
    
public:
    SFTTrainer(GPTModel& m, Tokenizer* tokenizer, 
               size_t epochs = 3, scalar_t lr = 2e-5)
        : model(m), dataset(tokenizer), 
          optimizer(lr, 0.9, 0.999, 1e-8, 0.01),
          num_epochs(epochs) {}
    
    void train() {
        std::cout << "å¼€å§‹SFTè®­ç»ƒ..." << std::endl;
        std::cout << "è®­ç»ƒè½®æ•°: " << num_epochs << std::endl;
        
        size_t batch_size = 8;
        size_t total_steps = (dataset.size() / batch_size) * num_epochs;
        size_t current_step = 0;
        
        for (size_t epoch = 0; epoch < num_epochs; epoch++) {
            std::cout << "Epoch " << (epoch + 1) << "/" << num_epochs << std::endl;
            
            // æ‰“ä¹±æ•°æ®
            dataset.shuffle();
            
            size_t num_batches = dataset.size() / batch_size;
            
            for (size_t batch_idx = 0; batch_idx < num_batches; batch_idx++) {
                TrainingBatch batch = dataset.get_batch(batch_size);
                
                // å‰å‘ä¼ æ’­
                Tensor logits = model.forward(batch.input_ids, batch.attention_mask);
                
                // è®¡ç®—æŸå¤±ï¼ˆåªè®¡ç®—assistantéƒ¨åˆ†çš„æŸå¤±ï¼‰
                LossResult loss_result = loss_function.compute(logits, batch.labels);
                
                // åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad();
                // compute_gradients(...);  // è®¡ç®—æ¢¯åº¦
                optimizer.step();
                
                current_step++;
                
                // æ‰“å°è¿›åº¦
                if (current_step % 100 == 0) {
                    std::cout << "Step " << current_step << "/" << total_steps
                              << ", Loss: " << loss_result.loss_value
                              << ", Perplexity: " << loss_result.metrics["perplexity"]
                              << std::endl;
                }
            }
        }
        
        std::cout << "SFTè®­ç»ƒå®Œæˆ!" << std::endl;
    }
};

#endif // SFT_TRAINING_HPP
```

## 7. å¥–åŠ±å»ºæ¨¡å’ŒRLHF

```cpp
// rlhf_training.hpp
#ifndef RLHF_TRAINING_HPP
#define RLHF_TRAINING_HPP

#include "sft_training.hpp"

// ==================== å¥–åŠ±æ¨¡å‹ ====================
class RewardModel {
private:
    GPTModel& base_model;  // SFTåçš„æ¨¡å‹
    Tensor reward_head;    // [hidden_size, 1]
    
public:
    RewardModel(GPTModel& model) : base_model(model) {
        // åˆå§‹åŒ–å¥–åŠ±å¤´
        reward_head = Tensor({model.config.hidden_size, 1}, true);
        
        // éšæœºåˆå§‹åŒ–
        std::normal_distribution<scalar_t> dist(0.0f, 0.02f);
        std::random_device rd;
        std::mt19937 gen(rd());
        
        for (size_t i = 0; i < reward_head.numel(); i++) {
            reward_head.data[i] = dist(gen);
        }
    }
    
    scalar_t forward(const Tensor& input_ids, const Tensor& attention_mask) {
        // è·å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        Tensor logits = base_model.forward(input_ids, attention_mask);
        
        // å‡è®¾logitsçš„shapeæ˜¯[batch, seq, hidden]
        size_t batch_size = logits.shape[0];
        size_t seq_len = logits.shape[1];
        
        // å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        Tensor last_hidden({batch_size, logits.shape[2]});
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t h = 0; h < logits.shape[2]; h++) {
                last_hidden[{b, h}] = logits[{b, seq_len - 1, h}];
            }
        }
        
        // é€šè¿‡å¥–åŠ±å¤´
        scalar_t reward = 0.0f;
        for (size_t h = 0; h < logits.shape[2]; h++) {
            reward += last_hidden[{0, h}] * reward_head[{h, 0}];
        }
        
        return reward;
    }
};

// ==================== å¥–åŠ±æ¨¡å‹è®­ç»ƒ ====================
class RewardModelTrainer {
private:
    RewardModel& reward_model;
    AdamW optimizer;
    
    // åå¥½æ•°æ®é›†
    struct PreferenceExample {
        std::string prompt;
        std::string chosen_response;    // è¢«é€‰ä¸­çš„å›ç­”
        std::string rejected_response;  // è¢«æ‹’ç»çš„å›ç­”
        scalar_t chosen_score;          // äººå·¥è¯„åˆ†
        scalar_t rejected_score;
    };
    
    std::vector<PreferenceExample> dataset;
    Tokenizer* tokenizer;
    
public:
    RewardModelTrainer(RewardModel& rm, Tokenizer* tok)
        : reward_model(rm), optimizer(1e-5, 0.9, 0.999, 1e-8, 0.01), 
          tokenizer(tok) {
        load_preference_data();
    }
    
    void load_preference_data() {
        // ç¤ºä¾‹åå¥½æ•°æ®
        dataset = {
            {"è§£é‡Šé‡å­è®¡ç®—", 
             "é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œè®¡ç®—çš„æ–¹æ³•...",
             "é‡å­è®¡ç®—å°±æ˜¯å¾ˆå¿«çš„è®¡ç®—",
             0.9f, 0.2f},
             
            {"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
             "æ˜¥é£æ‹‚é¢èŠ±é¦™æº¢ï¼Œä¸‡ç‰©å¤è‹ç”Ÿæœºå‹ƒ...",
             "æ˜¥å¤©æ¥äº†ï¼ŒèŠ±å¼€äº†",
             0.8f, 0.3f}
        };
    }
    
    scalar_t compute_preference_loss(scalar_t chosen_reward, 
                                    scalar_t rejected_reward) {
        // æˆå¯¹æ’åæŸå¤±
        // ç›®æ ‡ï¼šchosen_reward > rejected_reward
        
        // Bradley-Terryæ¨¡å‹æŸå¤±
        scalar_t loss = -std::log(1.0f / (1.0f + 
                          std::exp(rejected_reward - chosen_reward)));
        
        return loss;
    }
    
    void train_step() {
        // éšæœºé€‰æ‹©ä¸€ä¸ªåå¥½æ ·æœ¬
        size_t idx = rand() % dataset.size();
        const PreferenceExample& example = dataset[idx];
        
        // ç¼–ç chosenå“åº”
        std::string chosen_input = example.prompt + "\n" + example.chosen_response;
        std::vector<token_id_t> chosen_tokens = tokenizer->encode(chosen_input);
        
        // ç¼–ç rejectedå“åº”
        std::string rejected_input = example.prompt + "\n" + example.rejected_response;
        std::vector<token_id_t> rejected_tokens = tokenizer->encode(rejected_input);
        
        // åˆ›å»ºå¼ é‡ï¼ˆç®€åŒ–ï¼‰
        Tensor chosen_ids({1, (size_t)chosen_tokens.size()});
        Tensor chosen_mask({1, (size_t)chosen_tokens.size()});
        
        Tensor rejected_ids({1, (size_t)rejected_tokens.size()});
        Tensor rejected_mask({1, (size_t)rejected_tokens.size()});
        
        // å¡«å……æ•°æ®
        for (size_t i = 0; i < chosen_tokens.size(); i++) {
            chosen_ids[{0, i}] = chosen_tokens[i];
            chosen_mask[{0, i}] = 1.0f;
        }
        
        for (size_t i = 0; i < rejected_tokens.size(); i++) {
            rejected_ids[{0, i}] = rejected_tokens[i];
            rejected_mask[{0, i}] = 1.0f;
        }
        
        // å‰å‘ä¼ æ’­
        scalar_t chosen_reward = reward_model.forward(chosen_ids, chosen_mask);
        scalar_t rejected_reward = reward_model.forward(rejected_ids, rejected_mask);
        
        // è®¡ç®—æŸå¤±
        scalar_t loss = compute_preference_loss(chosen_reward, rejected_reward);
        
        // åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad();
        // compute_gradients(...);
        optimizer.step();
        
        std::cout << "Reward Model Loss: " << loss 
                  << ", Chosen: " << chosen_reward 
                  << ", Rejected: " << rejected_reward << std::endl;
    }
};

// ==================== PPOç®—æ³• ====================
class PPO {
private:
    GPTModel& policy_model;   // è¦ä¼˜åŒ–çš„æ¨¡å‹
    GPTModel& reference_model; // å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸ä¸policy_modelåˆå§‹ç›¸åŒï¼‰
    RewardModel& reward_model;
    AdamW optimizer;
    
    // PPOè¶…å‚æ•°
    scalar_t clip_epsilon = 0.2f;
    scalar_t kl_coef = 0.01f;
    scalar_t gamma = 0.99f;    // æŠ˜æ‰£å› å­
    scalar_t lambda = 0.95f;   // GAEå‚æ•°
    
public:
    PPO(GPTModel& policy, GPTModel& ref, RewardModel& rm)
        : policy_model(policy), reference_model(ref), reward_model(rm),
          optimizer(1e-6, 0.9, 0.999, 1e-8, 0.01) {}
    
    struct RolloutBuffer {
        std::vector<Tensor> states;      // è¾“å…¥çŠ¶æ€
        std::vector<Tensor> actions;     // ç”Ÿæˆçš„tokens
        std::vector<scalar_t> rewards;   // å¥–åŠ±
        std::vector<scalar_t> values;    // ä»·å€¼ä¼°è®¡
        std::vector<scalar_t> logprobs;  // å¯¹æ•°æ¦‚ç‡
        std::vector<bool> dones;         // æ˜¯å¦ç»“æŸ
    };
    
    RolloutBuffer collect_rollouts(const Tensor& initial_prompt, 
                                  size_t max_steps = 100) {
        RolloutBuffer buffer;
        
        Tensor current_state = initial_prompt;
        
        for (size_t step = 0; step < max_steps; step++) {
            // ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
            Tensor action_dist = policy_model.forward(current_state);
            
            // é‡‡æ ·åŠ¨ä½œ
            Tensor action = sample_action(action_dist);
            
            // è®¡ç®—å¯¹æ•°æ¦‚ç‡
            scalar_t logprob = compute_log_prob(action_dist, action);
            
            // è·å–ä»·å€¼ä¼°è®¡ï¼ˆç®€åŒ–ï¼‰
            scalar_t value = estimate_value(current_state);
            
            // æ‰§è¡ŒåŠ¨ä½œï¼ˆå°†tokenæ·»åŠ åˆ°åºåˆ—ï¼‰
            Tensor next_state = append_token(current_state, action);
            
            // è®¡ç®—å¥–åŠ±
            scalar_t reward = compute_reward(next_state);
            
            // æ£€æŸ¥æ˜¯å¦ç»“æŸ
            bool done = (action[{0, 0}] == tokenizer->get_eos_id()) || 
                       (step == max_steps - 1);
            
            // å­˜å‚¨åˆ°ç¼“å†²åŒº
            buffer.states.push_back(current_state);
            buffer.actions.push_back(action);
            buffer.rewards.push_back(reward);
            buffer.values.push_back(value);
            buffer.logprobs.push_back(logprob);
            buffer.dones.push_back(done);
            
            if (done) break;
            
            current_state = next_state;
        }
        
        return buffer;
    }
    
    void update_policy(const RolloutBuffer& buffer) {
        // è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        std::vector<scalar_t> advantages = compute_advantages(buffer);
        
        // è®¡ç®—å›æŠ¥
        std::vector<scalar_t> returns = compute_returns(buffer);
        
        // PPOæ›´æ–°
        for (size_t i = 0; i < buffer.states.size(); i++) {
            // è·å–æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”
            scalar_t ratio = compute_probability_ratio(buffer, i);
            
            // è£å‰ªçš„ç›®æ ‡å‡½æ•°
            scalar_t surr1 = ratio * advantages[i];
            scalar_t surr2 = std::clamp(ratio, 1.0f - clip_epsilon, 
                                       1.0f + clip_epsilon) * advantages[i];
            
            scalar_t policy_loss = -std::min(surr1, surr2);
            
            // KLæ•£åº¦æƒ©ç½š
            scalar_t kl_penalty = compute_kl_divergence(buffer, i);
            
            // ä»·å€¼æŸå¤±
            scalar_t value_loss = compute_value_loss(buffer, i, returns[i]);
            
            // æ€»æŸå¤±
            scalar_t total_loss = policy_loss + kl_coef * kl_penalty + 
                                0.5f * value_loss;
            
            // åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad();
            // compute_gradients(total_loss);
            optimizer.step();
        }
    }
    
private:
    Tensor sample_action(const Tensor& distribution) {
        // ä»åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªtoken
        // ç®€åŒ–å®ç°ï¼šå–æ¦‚ç‡æœ€å¤§çš„token
        size_t vocab_size = distribution.shape[2];
        
        scalar_t max_prob = -std::numeric_limits<scalar_t>::max();
        token_id_t best_token = 0;
        
        for (size_t v = 0; v < vocab_size; v++) {
            if (distribution[{0, 0, v}] > max_prob) {
                max_prob = distribution[{0, 0, v}];
                best_token = v;
            }
        }
        
        Tensor action({1, 1});
        action[{0, 0}] = best_token;
        return action;
    }
    
    scalar_t compute_log_prob(const Tensor& distribution, const Tensor& action) {
        token_id_t token = static_cast<token_id_t>(action[{0, 0}]);
        return std::log(distribution[{0, 0, token}] + 1e-10f);
    }
    
    scalar_t estimate_value(const Tensor& state) {
        // ç®€åŒ–ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºä½œä¸ºä»·å€¼ä¼°è®¡
        Tensor dummy_mask(state.shape);
        std::fill(dummy_mask.data.begin(), dummy_mask.data.end(), 1.0f);
        
        return reward_model.forward(state, dummy_mask);
    }
    
    scalar_t compute_reward(const Tensor& state) {
        // ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å³æ—¶å¥–åŠ±
        Tensor dummy_mask(state.shape);
        std::fill(dummy_mask.data.begin(), dummy_mask.data.end(), 1.0f);
        
        scalar_t reward = reward_model.forward(state, dummy_mask);
        
        // KLæƒ©ç½šï¼ˆé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼‰
        scalar_t kl_penalty = compute_kl_penalty(state);
        
        return reward - kl_coef * kl_penalty;
    }
    
    scalar_t compute_kl_penalty(const Tensor& state) {
        // è®¡ç®—å½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦
        // ç®€åŒ–å®ç°
        return 0.1f;
    }
    
    std::vector<scalar_t> compute_advantages(const RolloutBuffer& buffer) {
        // ä½¿ç”¨GAEï¼ˆGeneralized Advantage Estimationï¼‰è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        size_t n = buffer.rewards.size();
        std::vector<scalar_t> advantages(n, 0.0f);
        
        scalar_t last_advantage = 0.0f;
        for (int t = n - 1; t >= 0; t--) {
            scalar_t delta = buffer.rewards[t] + 
                           gamma * (t < n - 1 ? buffer.values[t + 1] : 0.0f) - 
                           buffer.values[t];
            
            advantages[t] = delta + gamma * lambda * 
                          (t < n - 1 ? (1.0f - buffer.dones[t]) : 0.0f) * 
                          last_advantage;
            
            last_advantage = advantages[t];
        }
        
        return advantages;
    }
    
    std::vector<scalar_t> compute_returns(const RolloutBuffer& buffer) {
        // è®¡ç®—æŠ˜æ‰£å›æŠ¥
        size_t n = buffer.rewards.size();
        std::vector<scalar_t> returns(n, 0.0f);
        
        scalar_t last_return = 0.0f;
        for (int t = n - 1; t >= 0; t--) {
            returns[t] = buffer.rewards[t] + gamma * 
                        (t < n - 1 ? (1.0f - buffer.dones[t]) : 0.0f) * 
                        last_return;
            last_return = returns[t];
        }
        
        return returns;
    }
    
    scalar_t compute_probability_ratio(const RolloutBuffer& buffer, size_t idx) {
        // è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”
        // åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„æ¦‚ç‡
        return 1.0f;
    }
    
    scalar_t compute_kl_divergence(const RolloutBuffer& buffer, size_t idx) {
        // è®¡ç®—KLæ•£åº¦
        return 0.0f;
    }
    
    scalar_t compute_value_loss(const RolloutBuffer& buffer, 
                               size_t idx, scalar_t target_return) {
        scalar_t value_pred = buffer.values[idx];
        scalar_t error = target_return - value_pred;
        return 0.5f * error * error;
    }
    
    Tensor append_token(const Tensor& state, const Tensor& token) {
        // å°†tokenæ·»åŠ åˆ°åºåˆ—æœ«å°¾
        size_t batch_size = state.shape[0];
        size_t seq_len = state.shape[1];
        
        Tensor new_state({batch_size, seq_len + 1});
        
        // å¤åˆ¶åŸæœ‰åºåˆ—
        for (size_t b = 0; b < batch_size; b++) {
            for (size_t s = 0; s < seq_len; s++) {
                new_state[{b, s}] = state[{b, s}];
            }
            // æ·»åŠ æ–°token
            new_state[{b, seq_len}] = token[{b, 0}];
        }
        
        return new_state;
    }
};

#endif // RLHF_TRAINING_HPP
```

## 8. å®Œæ•´çš„è®­ç»ƒç®¡é“æ•´åˆ

```cpp
// complete_pipeline.cpp
#include <iostream>
#include <memory>
#include <vector>

#include "pretraining.hpp"
#include "train_pipeline.hpp"
#include "sft_training.hpp"
#include "rlhf_training.hpp"

class CompleteLLMTrainingPipeline {
private:
    // é…ç½®
    struct PipelineConfig {
        ModelConfig model_config;
        
        struct {
            size_t total_steps = 100000;
            std::string data_dir = "./data/pretrain";
        } pretrain;
        
        struct {
            size_t epochs = 3;
            std::string sft_data = "./data/sft.json";
        } sft;
        
        struct {
            size_t preference_samples = 10000;
            std::string preference_data = "./data/preferences.json";
        } reward_model;
        
        struct {
            size_t ppo_epochs = 5;
            size_t rollout_steps = 100;
        } rlhf;
    };
    
    PipelineConfig config;
    
    // ç»„ä»¶
    std::unique_ptr<GPTModel> model;
    std::unique_ptr<Tokenizer> tokenizer;
    std::unique_ptr<DataPipeline> data_pipeline;
    std::unique_ptr<TrainingMonitor> monitor;
    
    // è®­ç»ƒçŠ¶æ€
    enum PipelineStage {
        STAGE_PRETRAIN,
        STAGE_SFT,
        STAGE_REWARD_MODEL,
        STAGE_RLHF,
        STAGE_DONE
    };
    
    PipelineStage current_stage = STAGE_PRETRAIN;
    
public:
    CompleteLLMTrainingPipeline(const PipelineConfig& cfg) : config(cfg) {
        initialize();
    }
    
    void initialize() {
        std::cout << "åˆå§‹åŒ–LLMè®­ç»ƒç®¡é“..." << std::endl;
        
        // 1. åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = std::make_unique<Tokenizer>(config.model_config.vocab_size);
        
        // 2. åˆå§‹åŒ–æ¨¡å‹
        model = std::make_unique<GPTModel>(config.model_config);
        
        std::cout << "æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œæ€»å‚æ•°é‡: " 
                  << config.model_config.total_params() << std::endl;
    }
    
    void run_pipeline() {
        std::cout << "\n" << std::string(80, '=') << std::endl;
        std::cout << "å¼€å§‹å®Œæ•´LLMè®­ç»ƒç®¡é“" << std::endl;
        std::cout << std::string(80, '=') << "\n" << std::endl;
        
        // é˜¶æ®µ1: é¢„è®­ç»ƒ
        run_pretraining();
        
        // é˜¶æ®µ2: æœ‰ç›‘ç£å¾®è°ƒ
        run_sft();
        
        // é˜¶æ®µ3: å¥–åŠ±å»ºæ¨¡
        run_reward_model_training();
        
        // é˜¶æ®µ4: RLHF
        run_rlhf();
        
        // é˜¶æ®µ5: æœ€ç»ˆè¯„ä¼°å’Œä¿å­˜
        finalize();
        
        std::cout << "\n" << std::string(80, '=') << std::endl;
        std::cout << "LLMè®­ç»ƒç®¡é“å®Œæˆ!" << std::endl;
        std::cout << std::string(80, '=') << std::endl;
    }
    
private:
    void run_pretraining() {
        std::cout << "\né˜¶æ®µ1: é¢„è®­ç»ƒ (Pre-training)" << std::endl;
        std::cout << std::string(50, '-') << std::endl;
        
        // åˆ›å»ºæ•°æ®ç®¡é“
        data_pipeline = std::make_unique<DataPipeline>(
            config.pretrain.data_dir, tokenizer.get());
        
        // åˆ›å»ºè®­ç»ƒå™¨
        PreTrainingPipeline pretrainer(
            config.model_config,
            config.pretrain.data_dir,
            config.pretrain.total_steps
        );
        
        // å¼€å§‹è®­ç»ƒ
        pretrainer.train();
        
        std::cout << "é¢„è®­ç»ƒå®Œæˆ!" << std::endl;
        current_stage = STAGE_SFT;
    }
    
    void run_sft() {
        std::cout << "\né˜¶æ®µ2: æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning)" << std::endl;
        std::cout << std::string(50, '-') << std::endl;
        
        // åˆ›å»ºSFTè®­ç»ƒå™¨
        SFTTrainer sft_trainer(
            *model,
            tokenizer.get(),
            config.sft.epochs
        );
        
        // å¼€å§‹SFTè®­ç»ƒ
        sft_trainer.train();
        
        std::cout << "SFTè®­ç»ƒå®Œæˆ!" << std::endl;
        current_stage = STAGE_REWARD_MODEL;
    }
    
    void run_reward_model_training() {
        std::cout << "\né˜¶æ®µ3: å¥–åŠ±æ¨¡å‹è®­ç»ƒ (Reward Modeling)" << std::endl;
        std::cout << std::string(50, '-') << std::endl;
        
        // åˆ›å»ºå¥–åŠ±æ¨¡å‹ï¼ˆåŸºäºSFTåçš„æ¨¡å‹ï¼‰
        RewardModel reward_model(*model);
        
        // åˆ›å»ºå¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨
        RewardModelTrainer rm_trainer(reward_model, tokenizer.get());
        
        // è®­ç»ƒå¥–åŠ±æ¨¡å‹
        std::cout << "è®­ç»ƒå¥–åŠ±æ¨¡å‹..." << std::endl;
        for (size_t step = 0; step < 10000; step++) {
            rm_trainer.train_step();
            
            if (step % 1000 == 0) {
                std::cout << "å¥–åŠ±æ¨¡å‹è®­ç»ƒæ­¥æ•°: " << step << "/10000" << std::endl;
            }
        }
        
        std::cout << "å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆ!" << std::endl;
        current_stage = STAGE_RLHF;
    }
    
    void run_rlhf() {
        std::cout << "\né˜¶æ®µ4: RLHF (Reinforcement Learning from Human Feedback)" << std::endl;
        std::cout << std::string(50, '-') << std::endl;
        
        // åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆå¤åˆ¶å½“å‰æ¨¡å‹ï¼‰
        GPTModel reference_model = *model;  // éœ€è¦æ·±æ‹·è´
        
        // åˆ›å»ºå¥–åŠ±æ¨¡å‹
        RewardModel reward_model(*model);
        
        // åˆ›å»ºPPOè®­ç»ƒå™¨
        PPO ppo_trainer(*model, reference_model, reward_model);
        
        std::cout << "å¼€å§‹PPOè®­ç»ƒ..." << std::endl;
        
        // è®­ç»ƒå¾ªç¯
        for (size_t epoch = 0; epoch < config.rlhf.ppo_epochs; epoch++) {
            std::cout << "PPO Epoch " << (epoch + 1) << "/" 
                      << config.rlhf.ppo_epochs << std::endl;
            
            // æ”¶é›†ç»éªŒ
            Tensor initial_prompt({1, 10});  // ç¤ºä¾‹æç¤º
            // å¡«å……æç¤º...
            
            PPO::RolloutBuffer buffer = ppo_trainer.collect_rollouts(
                initial_prompt, config.rlhf.rollout_steps);
            
            // æ›´æ–°ç­–ç•¥
            ppo_trainer.update_policy(buffer);
            
            // è¯„ä¼°å½“å‰ç­–ç•¥
            evaluate_policy(epoch);
        }
        
        std::cout << "RLHFè®­ç»ƒå®Œæˆ!" << std::endl;
        current_stage = STAGE_DONE;
    }
    
    void evaluate_policy(size_t epoch) {
        // è¯„ä¼°å½“å‰ç­–ç•¥çš„æ€§èƒ½
        std::cout << "è¯„ä¼°ç­–ç•¥..." << std::endl;
        
        // ç¤ºä¾‹ï¼šç”Ÿæˆä¸€äº›æ–‡æœ¬å¹¶è¯„ä¼°è´¨é‡
        std::vector<std::string> test_prompts = {
            "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            "å†™ä¸€ä¸ªå…³äºå‹è°Šçš„çŸ­æ•…äº‹",
            "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹",
            "æè¿°å·´é»çš„é£æ™¯"
        };
        
        for (const auto& prompt : test_prompts) {
            std::vector<token_id_t> tokens = tokenizer->encode(prompt);
            Tensor input_tensor({1, (size_t)tokens.size()});
            
            for (size_t i = 0; i < tokens.size(); i++) {
                input_tensor[{0, i}] = tokens[i];
            }
            
            // ç”Ÿæˆæ–‡æœ¬
            Tensor generated = generate_text(input_tensor, 50);
            
            // è§£ç å¹¶æ˜¾ç¤º
            std::vector<token_id_t> output_tokens(generated.shape[1]);
            for (size_t i = 0; i < output_tokens.size(); i++) {
                output_tokens[i] = static_cast<token_id_t>(generated[{0, i}]);
            }
            
            std::string generated_text = tokenizer->decode(output_tokens);
            
            std::cout << "Prompt: " << prompt << std::endl;
            std::cout << "Generated: " << generated_text.substr(0, 100) 
                      << "..." << std::endl;
            std::cout << std::endl;
        }
    }
    
    Tensor generate_text(const Tensor& input, size_t max_length) {
        // ç®€åŒ–çš„æ–‡æœ¬ç”Ÿæˆ
        Tensor output = input;
        
        for (size_t i = 0; i < max_length; i++) {
            // å‰å‘ä¼ æ’­
            Tensor attention_mask(output.shape);
            std::fill(attention_mask.data.begin(), attention_mask.data.end(), 1.0f);
            
            Tensor logits = model->forward(output, attention_mask);
            
            // å–æœ€åä¸€ä¸ªtokençš„é¢„æµ‹
            size_t last_idx = output.shape[1] - 1;
            
            // æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„token
            size_t vocab_size = logits.shape[2];
            scalar_t max_prob = -std::numeric_limits<scalar_t>::max();
            token_id_t next_token = 0;
            
            for (size_t v = 0; v < vocab_size; v++) {
                if (logits[{0, last_idx, v}] > max_prob) {
                    max_prob = logits[{0, last_idx, v}];
                    next_token = v;
                }
            }
            
            // æ·»åŠ æ–°tokenåˆ°è¾“å‡º
            Tensor new_output({output.shape[0], output.shape[1] + 1});
            
            // å¤åˆ¶åŸæœ‰tokens
            for (size_t s = 0; s < output.shape[1]; s++) {
                new_output[{0, s}] = output[{0, s}];
            }
            
            // æ·»åŠ æ–°token
            new_output[{0, output.shape[1]}] = next_token;
            
            output = new_output;
            
            // å¦‚æœç”ŸæˆEOS tokenï¼Œåœæ­¢
            if (next_token == tokenizer->get_eos_id()) {
                break;
            }
        }
        
        return output;
    }
    
    void finalize() {
        std::cout << "\né˜¶æ®µ5: æœ€ç»ˆå¤„ç†å’Œä¿å­˜" << std::endl;
        std::cout << std::string(50, '-') << std::endl;
        
        // ä¿å­˜æœ€ç»ˆæ¨¡å‹
        save_final_model();
        
        // ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š
        generate_evaluation_report();
        
        std::cout << "ç®¡é“å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜!" << std::endl;
    }
    
    void save_final_model() {
        std::string model_path = "./final_model/model.bin";
        std::string config_path = "./final_model/config.json";
        
        std::cout << "ä¿å­˜æ¨¡å‹åˆ°: " << model_path << std::endl;
        
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦åºåˆ—åŒ–æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        // ä¿å­˜é…ç½®
        std::ofstream config_file(config_path);
        config_file << "{\n"
                   << "  \"vocab_size\": " << config.model_config.vocab_size << ",\n"
                   << "  \"hidden_size\": " << config.model_config.hidden_size << ",\n"
                   << "  \"num_layers\": " << config.model_config.num_layers << ",\n"
                   << "  \"num_heads\": " << config.model_config.num_heads << ",\n"
                   << "  \"max_seq_len\": " << config.model_config.max_seq_len << ",\n"
                   << "  \"ffn_dim\": " << config.model_config.ffn_dim << "\n"
                   << "}" << std::endl;
        config_file.close();
    }
    
    void generate_evaluation_report() {
        std::ofstream report("./final_model/evaluation_report.txt");
        
        report << "LLMè®­ç»ƒç®¡é“æœ€ç»ˆæŠ¥å‘Š\n";
        report << "====================\n\n";
        
        report << "æ¨¡å‹é…ç½®:\n";
        report << "- è¯æ±‡è¡¨å¤§å°: " << config.model_config.vocab_size << "\n";
        report << "- éšè—å±‚ç»´åº¦: " << config.model_config.hidden_size << "\n";
        report << "- å±‚æ•°: " << config.model_config.num_layers << "\n";
        report << "- æ³¨æ„åŠ›å¤´æ•°: " << config.model_config.num_heads << "\n";
        report << "- æ€»å‚æ•°é‡: " << config.model_config.total_params() << "\n\n";
        
        report << "è®­ç»ƒæµç¨‹:\n";
        report << "- é¢„è®­ç»ƒæ­¥æ•°: " << config.pretrain.total_steps << "\n";
        report << "- SFTè½®æ•°: " << config.sft.epochs << "\n";
        report << "- RLHFè½®æ•°: " << config.rlhf.ppo_epochs << "\n\n";
        
        report << "ç”Ÿæˆç¤ºä¾‹:\n";
        
        // ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ–‡æœ¬
        std::vector<std::string> prompts = {
            "äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯",
            "æœºå™¨å­¦ä¹ å¯ä»¥å¸®åŠ©æˆ‘ä»¬",
            "å†™ä¸€é¦–çŸ­è¯—:"
        };
        
        for (const auto& prompt : prompts) {
            report << "Prompt: " << prompt << "\n";
            
            // ç¼–ç æç¤º
            std::vector<token_id_t> tokens = tokenizer->encode(prompt);
            Tensor input({1, (size_t)tokens.size()});
            
            for (size_t i = 0; i < tokens.size(); i++) {
                input[{0, i}] = tokens[i];
            }
            
            // ç”Ÿæˆæ–‡æœ¬
            Tensor generated = generate_text(input, 50);
            
            // è§£ç 
            std::vector<token_id_t> output_tokens(generated.shape[1]);
            for (size_t i = 0; i < output_tokens.size(); i++) {
                output_tokens[i] = static_cast<token_id_t>(generated[{0, i}]);
            }
            
            std::string generated_text = tokenizer->decode(output_tokens);
            report << "Generated: " << generated_text << "\n\n";
        }
        
        report.close();
    }
};

int main() {
    std::cout << "LLMè®­ç»ƒç®¡é“æ¨¡æ‹Ÿå™¨" << std::endl;
    std::cout << "=================\n" << std::endl;
    
    // é…ç½®å°æ¨¡å‹ç”¨äºæ¼”ç¤º
    CompleteLLMTrainingPipeline::PipelineConfig config;
    
    config.model_config.vocab_size = 50257;
    config.model_config.hidden_size = 768;
    config.model_config.num_layers = 12;
    config.model_config.num_heads = 12;
    config.model_config.max_seq_len = 1024;
    config.model_config.ffn_dim = 3072;
    
    config.pretrain.total_steps = 1000;  // å°è§„æ¨¡æ¼”ç¤º
    config.sft.epochs = 1;
    config.rlhf.ppo_epochs = 2;
    
    // åˆ›å»ºå¹¶è¿è¡Œç®¡é“
    CompleteLLMTrainingPipeline pipeline(config);
    
    try {
        pipeline.run_pipeline();
    } catch (const std::exception& e) {
        std::cerr << "é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}
```

## 9. è®­ç»ƒæµç¨‹å¯è§†åŒ–

```
å®Œæ•´LLMè®­ç»ƒç®¡é“æµç¨‹å›¾
================================

åˆå§‹åŒ–
  â”‚
  â”œâ”€â–¶ æ•°æ®é¢„å¤„ç†
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ æ–‡æœ¬æ¸…æ´—
  â”‚     â”œâ”€â–¶ åˆ†è¯
  â”‚     â””â”€â–¶ åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡
  â”‚
  â”œâ”€â–¶ é¢„è®­ç»ƒé˜¶æ®µ (100,000æ­¥)
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ å‰å‘ä¼ æ’­: token â†’ åµŒå…¥ â†’ Transformer â†’ logits
  â”‚     â”œâ”€â–¶ æŸå¤±è®¡ç®—: äº¤å‰ç†µæŸå¤± (è¯­è¨€å»ºæ¨¡)
  â”‚     â”œâ”€â–¶ åå‘ä¼ æ’­: è®¡ç®—æ¢¯åº¦
  â”‚     â”œâ”€â–¶ ä¼˜åŒ–å™¨æ­¥éª¤: AdamWæ›´æ–°æƒé‡
  â”‚     â””â”€â–¶ å®šæœŸè¯„ä¼°å’Œä¿å­˜æ£€æŸ¥ç‚¹
  â”‚
  â”œâ”€â–¶ æœ‰ç›‘ç£å¾®è°ƒ (3è½®)
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ åŠ è½½æŒ‡ä»¤-å›å¤å¯¹æ•°æ®
  â”‚     â”œâ”€â–¶ åªè®¡ç®—assistantéƒ¨åˆ†çš„æŸå¤±
  â”‚     â”œâ”€â–¶ å¾®è°ƒå…¨éƒ¨å‚æ•°
  â”‚     â””â”€â–¶ ä¿å­˜SFTæ¨¡å‹
  â”‚
  â”œâ”€â–¶ å¥–åŠ±å»ºæ¨¡
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ æ”¶é›†åå¥½æ•°æ® (chosen vs rejected)
  â”‚     â”œâ”€â–¶ åœ¨SFTæ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ å¥–åŠ±å¤´
  â”‚     â”œâ”€â–¶ è®­ç»ƒå¥–åŠ±æ¨¡å‹: æˆå¯¹æ’åæŸå¤±
  â”‚     â””â”€â–¶ ä¿å­˜å¥–åŠ±æ¨¡å‹
  â”‚
  â”œâ”€â–¶ RLHFè®­ç»ƒ (5è½®PPO)
  â”‚     â”‚
  â”‚     â”œâ”€â–¶ æ”¶é›†ç»éªŒ: å½“å‰ç­–ç•¥ç”Ÿæˆå›å¤
  â”‚     â”œâ”€â–¶ è®¡ç®—å¥–åŠ±: å¥–åŠ±æ¨¡å‹ + KLæƒ©ç½š
  â”‚     â”œâ”€â–¶ PPOæ›´æ–°: ç­–ç•¥æ¢¯åº¦ + ä»·å€¼å‡½æ•°æ›´æ–°
  â”‚     â””â”€â–¶ å®šæœŸè¯„ä¼°ç­–ç•¥æ€§èƒ½
  â”‚
  â””â”€â–¶ æœ€ç»ˆå¤„ç†å’Œéƒ¨ç½²
        â”‚
        â”œâ”€â–¶ æ¨¡å‹é‡åŒ– (å¯é€‰)
        â”œâ”€â–¶ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        â””â”€â–¶ å¯¼å‡ºä¸ºå¯éƒ¨ç½²æ ¼å¼
```

## 10. å…³é”®ç®—æ³•æ€»ç»“

### é¢„è®­ç»ƒå…³é”®ç®—æ³•ï¼š
1. **è‡ªå›å½’è¯­è¨€å»ºæ¨¡**: $P(w_t|w_{<t})$
2. **Transformerå‰å‘ä¼ æ’­**: $\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
3. **AdamWä¼˜åŒ–å™¨**: $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$, $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$, $\hat{m}_t = m_t/(1-\beta_1^t)$, $\hat{v}_t = v_t/(1-\beta_2^t)$, $\theta_t = \theta_{t-1} - \eta(\hat{m}_t/(\sqrt{\hat{v}_t}+\epsilon) + \lambda\theta_{t-1})$
4. **æ¢¯åº¦è£å‰ª**: $g \leftarrow g \times \min(1, \frac{\text{max\_norm}}{\|g\|_2})$

### SFTå…³é”®ç®—æ³•ï¼š
1. **æŒ‡ä»¤å¾®è°ƒ**: æœ€å°åŒ– $-\sum \log P(\text{response}|\text{instruction})$
2. **åªè®¡ç®—assistantéƒ¨åˆ†æŸå¤±**: æ©ç æ‰userå’Œsysteméƒ¨åˆ†çš„æŸå¤±

### å¥–åŠ±å»ºæ¨¡å…³é”®ç®—æ³•ï¼š
1. **Bradley-Terryæ¨¡å‹**: $P(y_w \succ y_l|x) = \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))}$
2. **æˆå¯¹æ’åæŸå¤±**: $\mathcal{L} = -\mathbb{E}_{(x,y_w,y_l)}[\log\sigma(r(x,y_w) - r(x,y_l))]$

### RLHFå…³é”®ç®—æ³•ï¼š
1. **PPOç›®æ ‡å‡½æ•°**: $L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$
2. **GAEä¼˜åŠ¿ä¼°è®¡**: $\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + \cdots + (\gamma\lambda)^{T-t+1}\delta_{T-1}$
3. **KLæƒ©ç½š**: $\mathcal{L}^{KL} = \beta \cdot \text{KL}[\pi_\theta||\pi_{\text{ref}}]$

## å­¦ä¹ å»ºè®®ï¼š

1. **çº¸ä¸Šæ¨¡æ‹Ÿ**: æ‹¿ä¸€å¼ çº¸ï¼Œç”»å‡ºå¼ é‡å½¢çŠ¶çš„æµåŠ¨
   - è¾“å…¥: [batch=8, seq=512]
   - åµŒå…¥å: [8, 512, 768]
   - æ³¨æ„åŠ›å: å½¢çŠ¶ä¸å˜
   - è¾“å‡ºlogits: [8, 512, 50257]

2. **é€æ­¥å®ç°**:
   - ç¬¬1å¤©: å®ç°Tensorç±»å’ŒåŸºç¡€è¿ç®—
   - ç¬¬2-3å¤©: å®ç°Transformerå±‚
   - ç¬¬4å¤©: å®ç°ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
   - ç¬¬5å¤©: æ•´åˆæˆå®Œæ•´è®­ç»ƒå¾ªç¯

3. **è°ƒè¯•æŠ€å·§**:
   - ä»å°æ¨¡å‹å¼€å§‹ (hidden_size=64)
   - ä½¿ç”¨å›ºå®šæ•°æ®éªŒè¯å‰å‘ä¼ æ’­
   - æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åˆç† (ä¸æ˜¯NaNæˆ–è¿‡å¤§)
   - é€æ­¥å¢åŠ å¤æ‚åº¦

è¿™ä¸ªå®Œæ•´çš„C++ä¼ªä»£ç å®ç°å±•ç¤ºäº†LLMè®­ç»ƒç®¡é“çš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ã€‚è™½ç„¶ä¸ºäº†å¯è¯»æ€§è¿›è¡Œäº†ç®€åŒ–ï¼Œä½†å®ƒåŒ…å«äº†ç†è§£æ•´ä¸ªæµç¨‹æ‰€éœ€çš„æ‰€æœ‰å…³é”®æ¦‚å¿µå’Œç®—æ³•ã€‚ä½ å¯ä»¥ç”¨å®ƒä½œä¸ºè“å›¾ï¼Œåœ¨å®é™…å®ç°æ—¶å¡«å……ç»†èŠ‚ã€‚
