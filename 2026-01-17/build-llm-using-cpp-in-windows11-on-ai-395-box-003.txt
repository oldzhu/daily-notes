Sounds good — we’ll pause here.

For today, you can keep using the baseline core train_gpt.exe (it still uses nn::self_attention_1h, not MHA).

Quick baseline commands to practice (Debug build):

Train (small smoke run): .[train_gpt.exe](http://_vscodecontentref_/1) --data [the-verdict.txt](http://_vscodecontentref_/2) --steps 50 --batch 4 --seq 64 --dmodel 64 --layers 1 --lr 0.0003 --seed 1
Save checkpoint: .[train_gpt.exe](http://_vscodecontentref_/3) --data [the-verdict.txt](http://_vscodecontentref_/4) --steps 200 --batch 4 --seq 64 --dmodel 64 --layers 1 --save .\data\ckpt_tiny
Generate from checkpoint: .[train_gpt.exe](http://_vscodecontentref_/5) --load .\data\ckpt_tiny --steps 0 --prompt "Hello: " --gen 200 --temp 0.8 --topk 40 --ascii-only 1 --escape-bytes 1
Tomorrow, when you’re ready, we can decide whether to add --heads to the CLI (still defaulting to baseline) or keep MHA as a separate executable.
